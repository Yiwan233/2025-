# -*- coding: utf-8 -*-
"""
visualization_integration.py
æ³•åŒ»DNAåˆ†æå¯è§†åŒ–é›†æˆæ¨¡å—

é›†æˆQ1ã€Q2ã€Q3ã€Q4æ‰€æœ‰æ–¹æ³•çš„å¯è§†åŒ–ç»“æœ
æä¾›ç»Ÿä¸€çš„å›¾è¡¨ç”Ÿæˆå’Œç»“æœå±•ç¤ºåŠŸèƒ½
"""

import os
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional, Any
import logging
import warnings
from pathlib import Path
import re
from collections import defaultdict

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œå¿½ç•¥è­¦å‘Š
plt.rcParams['font.sans-serif'] = ["Arial Unicode MS", "SimHei", "Microsoft YaHei"]
plt.rcParams['axes.unicode_minus'] = False
warnings.filterwarnings('ignore')

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class VisualizationIntegrator:
    """å¯è§†åŒ–é›†æˆå™¨ - ç»Ÿä¸€æ‰€æœ‰æ–¹æ³•çš„ç»“æœå±•ç¤º"""
    
    def __init__(self):
        self.color_palette = {
            'Q1_NoC': '#FF6B6B',      # çº¢è‰² - NoCè¯†åˆ«
            'Q2_Ratio': '#4ECDC4',    # é’è‰² - æ··åˆæ¯”ä¾‹
            'Q3_Genotype': '#45B7D1', # è“è‰² - åŸºå› å‹æ¨æ–­
            'Q4_Denoise': '#96CEB4'   # ç»¿è‰² - é™å™ª
        }
        self.method_names = {
            'Q1': 'NoCè¯†åˆ« (RFECV+éšæœºæ£®æ—)',
            'Q2': 'æ··åˆæ¯”ä¾‹æ¨æ–­ (MGM-RF)',
            'Q3': 'åŸºå› å‹æ¨æ–­ (å¢å¼ºMCMC)',
            'Q4': 'é™å™ªå¤„ç† (UPG-M)'
        }
        logger.info("å¯è§†åŒ–é›†æˆå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def load_results_from_directory(self, results_dir: str, method_name: str) -> Dict:
        """ä»ç»“æœç›®å½•åŠ è½½åˆ†æç»“æœ"""
        results = {}
        
        if not os.path.exists(results_dir):
            logger.warning(f"{method_name}ç»“æœç›®å½•ä¸å­˜åœ¨: {results_dir}")
            return results
        
        try:
            # æœç´¢JSONç»“æœæ–‡ä»¶
            for file_path in Path(results_dir).rglob("*.json"):
                if any(keyword in file_path.name.lower() 
                      for keyword in ['result', 'summary', 'analysis']):
                    
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                        
                        # æå–æ ·æœ¬ID
                        sample_id = self._extract_sample_id(file_path.name, data)
                        if sample_id:
                            results[sample_id] = {
                                'data': data,
                                'file_path': str(file_path),
                                'method': method_name
                            }
                    
                    except Exception as e:
                        logger.warning(f"åŠ è½½æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            
            logger.info(f"ä»{method_name}åŠ è½½äº†{len(results)}ä¸ªæ ·æœ¬ç»“æœ")
            
        except Exception as e:
            logger.error(f"åŠ è½½{method_name}ç»“æœç›®å½•å¤±è´¥: {e}")
        
        return results
    
    def _extract_sample_id(self, filename: str, data: Dict) -> Optional[str]:
        """ä»æ–‡ä»¶åæˆ–æ•°æ®ä¸­æå–æ ·æœ¬ID"""
        # æ–¹æ³•1ï¼šä»æ•°æ®ä¸­ç›´æ¥è·å–
        for key in ['sample_file', 'sample_id', 'Sample_File', 'Sample_ID']:
            if key in data and data[key]:
                return str(data[key])
        
        # æ–¹æ³•2ï¼šä»æ–‡ä»¶åè§£æ
        # åŒ¹é…æ¨¡å¼å¦‚: sample_xxx_result.json, xxx_analysis.json
        patterns = [
            r'sample_(.+?)_result',
            r'(.+?)_analysis',
            r'(.+?)_result',
            r'sample_(.+?)\.json',
            r'([^_/\\]+)_[^_]*\.json'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, filename)
            if match:
                return match.group(1)
        
        return None
    
    def create_unified_comparison_plots(self, all_results: Dict, output_dir: str) -> int:
        """åˆ›å»ºç»Ÿä¸€çš„æ¯”è¾ƒå›¾è¡¨"""
        os.makedirs(output_dir, exist_ok=True)
        plot_count = 0
        
        # 1. æ–¹æ³•è¦†ç›–ç‡å¯¹æ¯”
        plot_count += self._plot_method_coverage(all_results, output_dir)
        
        # 2. NoCé¢„æµ‹ä¸€è‡´æ€§åˆ†æ
        plot_count += self._plot_noc_consistency(all_results, output_dir)
        
        # 3. æ··åˆæ¯”ä¾‹é¢„æµ‹å¯¹æ¯”
        plot_count += self._plot_mixture_ratio_comparison(all_results, output_dir)
        
        # 4. å¤„ç†æ—¶é—´å¯¹æ¯”
        plot_count += self._plot_processing_time_comparison(all_results, output_dir)
        
        # 5. æ•´ä½“æ€§èƒ½é›·è¾¾å›¾
        plot_count += self._plot_performance_radar(all_results, output_dir)
        
        # 6. æ ·æœ¬å¤æ‚åº¦åˆ†æ
        plot_count += self._plot_sample_complexity_analysis(all_results, output_dir)
        
        return plot_count
    
    def _plot_method_coverage(self, all_results: Dict, output_dir: str) -> int:
        """ç»˜åˆ¶å„æ–¹æ³•çš„æ ·æœ¬è¦†ç›–ç‡"""
        try:
            methods = list(all_results.keys())
            coverage_data = []
            
            # ç»Ÿè®¡æ¯ä¸ªæ–¹æ³•å¤„ç†çš„æ ·æœ¬æ•°
            all_samples = set()
            for method_results in all_results.values():
                all_samples.update(method_results.keys())
            
            for method in methods:
                method_samples = set(all_results[method].keys())
                coverage_rate = len(method_samples) / len(all_samples) if all_samples else 0
                coverage_data.append({
                    'Method': self.method_names.get(method, method),
                    'Coverage_Rate': coverage_rate,
                    'Sample_Count': len(method_samples),
                    'Total_Samples': len(all_samples)
                })
            
            df_coverage = pd.DataFrame(coverage_data)
            
            # åˆ›å»ºå›¾è¡¨
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
            
            # è¦†ç›–ç‡æ¡å½¢å›¾
            bars = ax1.bar(df_coverage['Method'], df_coverage['Coverage_Rate'], 
                          color=[self.color_palette.get(f'{method}_NoC', '#666666') 
                                for method in methods])
            ax1.set_ylabel('æ ·æœ¬è¦†ç›–ç‡')
            ax1.set_title('å„æ–¹æ³•æ ·æœ¬è¦†ç›–ç‡å¯¹æ¯”')
            ax1.set_ylim(0, 1.1)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, rate in zip(bars, df_coverage['Coverage_Rate']):
                height = bar.get_height()
                ax1.text(bar.get_x() + bar.get_width()/2., height + 0.02,
                        f'{rate:.1%}', ha='center', va='bottom')
            
            # æ ·æœ¬æ•°é‡å¯¹æ¯”
            ax2.bar(df_coverage['Method'], df_coverage['Sample_Count'],
                   color=[self.color_palette.get(f'{method}_NoC', '#666666') 
                         for method in methods])
            ax2.set_ylabel('å¤„ç†æ ·æœ¬æ•°')
            ax2.set_title('å„æ–¹æ³•å¤„ç†æ ·æœ¬æ•°å¯¹æ¯”')
            
            plt.xticks(rotation=45)
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, 'method_coverage_comparison.png'), 
                       dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info("æ–¹æ³•è¦†ç›–ç‡å¯¹æ¯”å›¾å·²ç”Ÿæˆ")
            return 1
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ–¹æ³•è¦†ç›–ç‡å›¾å¤±è´¥: {e}")
            return 0
    
    def _plot_noc_consistency(self, all_results: Dict, output_dir: str) -> int:
        """ç»˜åˆ¶NoCé¢„æµ‹ä¸€è‡´æ€§åˆ†æ"""
        try:
            # æ”¶é›†NoCé¢„æµ‹ç»“æœ
            noc_predictions = defaultdict(dict)
            
            for method, method_results in all_results.items():
                for sample_id, result in method_results.items():
                    data = result['data']
                    
                    # æå–NoCé¢„æµ‹
                    noc = None
                    if method == 'Q1':
                        noc = data.get('è´¡çŒ®è€…äººæ•°') or data.get('predicted_noc')
                    else:
                        noc = data.get('predicted_noc') or data.get('noc_prediction')
                    
                    if noc is not None:
                        noc_predictions[sample_id][method] = int(noc)
            
            if not noc_predictions:
                logger.warning("æ²¡æœ‰æ‰¾åˆ°NoCé¢„æµ‹æ•°æ®")
                return 0
            
            # åˆ›å»ºNoCä¸€è‡´æ€§çŸ©é˜µ
            df_noc = pd.DataFrame(noc_predictions).T.fillna(0).astype(int)
            
            # ç»˜åˆ¶NoCåˆ†å¸ƒçƒ­åŠ›å›¾
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
            
            # NoCé¢„æµ‹åˆ†å¸ƒ
            noc_counts = {}
            for method in df_noc.columns:
                if method in df_noc.columns:
                    noc_counts[self.method_names.get(method, method)] = df_noc[method].value_counts().sort_index()
            
            noc_df = pd.DataFrame(noc_counts).fillna(0)
            sns.heatmap(noc_df, annot=True, fmt='g', cmap='Blues', ax=ax1)
            ax1.set_title('å„æ–¹æ³•NoCé¢„æµ‹åˆ†å¸ƒçƒ­åŠ›å›¾')
            ax1.set_xlabel('åˆ†ææ–¹æ³•')
            ax1.set_ylabel('é¢„æµ‹NoCå€¼')
            
            # NoCä¸€è‡´æ€§åˆ†æ
            if len(df_noc.columns) >= 2:
                # è®¡ç®—æ–¹æ³•é—´çš„ä¸€è‡´æ€§
                methods = list(df_noc.columns)
                consistency_matrix = np.zeros((len(methods), len(methods)))
                
                for i, method1 in enumerate(methods):
                    for j, method2 in enumerate(methods):
                        if i != j:
                            # è®¡ç®—ä¸¤ä¸ªæ–¹æ³•é¢„æµ‹ç»“æœçš„ä¸€è‡´æ€§
                            common_samples = df_noc[(df_noc[method1] > 0) & (df_noc[method2] > 0)]
                            if len(common_samples) > 0:
                                consistency = (common_samples[method1] == common_samples[method2]).mean()
                                consistency_matrix[i, j] = consistency
                        else:
                            consistency_matrix[i, j] = 1.0
                
                method_labels = [self.method_names.get(m, m) for m in methods]
                sns.heatmap(consistency_matrix, 
                           xticklabels=method_labels,
                           yticklabels=method_labels,
                           annot=True, fmt='.3f', cmap='RdYlGn', ax=ax2)
                ax2.set_title('æ–¹æ³•é—´NoCé¢„æµ‹ä¸€è‡´æ€§')
            
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, 'noc_consistency_analysis.png'), 
                       dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info("NoCä¸€è‡´æ€§åˆ†æå›¾å·²ç”Ÿæˆ")
            return 1
            
        except Exception as e:
            logger.error(f"ç”ŸæˆNoCä¸€è‡´æ€§å›¾å¤±è´¥: {e}")
            return 0
    
    def _plot_mixture_ratio_comparison(self, all_results: Dict, output_dir: str) -> int:
        """ç»˜åˆ¶æ··åˆæ¯”ä¾‹é¢„æµ‹å¯¹æ¯”"""
        try:
            mixture_data = defaultdict(dict)
            
            # æ”¶é›†æ··åˆæ¯”ä¾‹æ•°æ®
            for method, method_results in all_results.items():
                for sample_id, result in method_results.items():
                    data = result['data']
                    
                    # æå–æ··åˆæ¯”ä¾‹
                    ratios = None
                    if method == 'Q2':
                        if 'posterior_summary' in data:
                            summary = data['posterior_summary']
                            ratios = []
                            i = 1
                            while f'Mx_{i}' in summary:
                                ratios.append(summary[f'Mx_{i}'].get('mean', 0))
                                i += 1
                    elif method == 'Q3':
                        # Q3ä¸­å¯èƒ½ä¹Ÿæœ‰æ··åˆæ¯”ä¾‹ä¿¡æ¯
                        if 'mixture_ratios' in data:
                            ratios = data['mixture_ratios']
                    
                    if ratios:
                        mixture_data[sample_id][method] = ratios
            
            if not mixture_data:
                logger.warning("æ²¡æœ‰æ‰¾åˆ°æ··åˆæ¯”ä¾‹æ•°æ®")
                return 0
            
            # ç»˜åˆ¶æ··åˆæ¯”ä¾‹å¯¹æ¯”
            n_samples = min(6, len(mixture_data))  # é™åˆ¶æ˜¾ç¤ºæ ·æœ¬æ•°
            if n_samples == 0:
                return 0
            
            sample_ids = list(mixture_data.keys())[:n_samples]
            
            fig, axes = plt.subplots(2, 3, figsize=(18, 12))
            axes = axes.flatten()
            
            for idx, sample_id in enumerate(sample_ids):
                ax = axes[idx]
                sample_ratios = mixture_data[sample_id]
                
                methods = list(sample_ratios.keys())
                x_pos = np.arange(len(methods))
                
                # è·å–æœ€å¤§ç»„åˆ†æ•°
                max_components = max(len(ratios) for ratios in sample_ratios.values())
                
                # ç»˜åˆ¶æ¯ä¸ªç»„åˆ†
                bottom = np.zeros(len(methods))
                for comp_idx in range(max_components):
                    values = []
                    for method in methods:
                        ratios = sample_ratios[method]
                        value = ratios[comp_idx] if comp_idx < len(ratios) else 0
                        values.append(value)
                    
                    ax.bar(x_pos, values, bottom=bottom, 
                          label=f'ç»„åˆ†{comp_idx+1}', alpha=0.7)
                    bottom += values
                
                ax.set_title(f'æ ·æœ¬ {sample_id}\næ··åˆæ¯”ä¾‹å¯¹æ¯”')
                ax.set_xticks(x_pos)
                ax.set_xticklabels([self.method_names.get(m, m) for m in methods], 
                                  rotation=45)
                ax.set_ylabel('æ··åˆæ¯”ä¾‹')
                ax.legend()
                ax.set_ylim(0, 1.1)
            
            # éšè—å¤šä½™çš„å­å›¾
            for idx in range(n_samples, len(axes)):
                axes[idx].set_visible(False)
            
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, 'mixture_ratio_comparison.png'), 
                       dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info("æ··åˆæ¯”ä¾‹å¯¹æ¯”å›¾å·²ç”Ÿæˆ")
            return 1
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ··åˆæ¯”ä¾‹å¯¹æ¯”å›¾å¤±è´¥: {e}")
            return 0
    
    def _plot_processing_time_comparison(self, all_results: Dict, output_dir: str) -> int:
        """ç»˜åˆ¶å¤„ç†æ—¶é—´å¯¹æ¯”"""
        try:
            time_data = []
            
            for method, method_results in all_results.items():
                times = []
                for sample_id, result in method_results.items():
                    data = result['data']
                    
                    # æå–è®¡ç®—æ—¶é—´
                    comp_time = data.get('computation_time')
                    if comp_time is not None:
                        times.append(float(comp_time))
                
                if times:
                    time_data.append({
                        'Method': self.method_names.get(method, method),
                        'Mean_Time': np.mean(times),
                        'Std_Time': np.std(times),
                        'Median_Time': np.median(times),
                        'Max_Time': np.max(times),
                        'Min_Time': np.min(times),
                        'Sample_Count': len(times)
                    })
            
            if not time_data:
                logger.warning("æ²¡æœ‰æ‰¾åˆ°å¤„ç†æ—¶é—´æ•°æ®")
                return 0
            
            df_time = pd.DataFrame(time_data)
            
            # åˆ›å»ºå¤„ç†æ—¶é—´å¯¹æ¯”å›¾
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
            
            # å¹³å‡å¤„ç†æ—¶é—´
            bars = ax1.bar(df_time['Method'], df_time['Mean_Time'], 
                          yerr=df_time['Std_Time'], capsize=5,
                          color=[self.color_palette.get(f'{method}_NoC', '#666666') 
                                for method in ['Q1', 'Q2', 'Q3', 'Q4']])
            ax1.set_ylabel('å¤„ç†æ—¶é—´ (ç§’)')
            ax1.set_title('å„æ–¹æ³•å¹³å‡å¤„ç†æ—¶é—´å¯¹æ¯”')
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, time_val in zip(bars, df_time['Mean_Time']):
                height = bar.get_height()
                ax1.text(bar.get_x() + bar.get_width()/2., height,
                        f'{time_val:.1f}s', ha='center', va='bottom')
            
            # å¤„ç†æ—¶é—´åˆ†å¸ƒç®±çº¿å›¾
            time_distributions = []
            method_labels = []
            
            for method, method_results in all_results.items():
                times = []
                for result in method_results.values():
                    comp_time = result['data'].get('computation_time')
                    if comp_time is not None:
                        times.append(float(comp_time))
                
                if times:
                    time_distributions.append(times)
                    method_labels.append(self.method_names.get(method, method))
            
            if time_distributions:
                ax2.boxplot(time_distributions, labels=method_labels)
                ax2.set_ylabel('å¤„ç†æ—¶é—´ (ç§’)')
                ax2.set_title('å¤„ç†æ—¶é—´åˆ†å¸ƒç®±çº¿å›¾')
                ax2.tick_params(axis='x', rotation=45)
            
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, 'processing_time_comparison.png'), 
                       dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info("å¤„ç†æ—¶é—´å¯¹æ¯”å›¾å·²ç”Ÿæˆ")
            return 1
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå¤„ç†æ—¶é—´å¯¹æ¯”å›¾å¤±è´¥: {e}")
            return 0
    
    def _plot_performance_radar(self, all_results: Dict, output_dir: str) -> int:
        """ç»˜åˆ¶æ•´ä½“æ€§èƒ½é›·è¾¾å›¾"""
        try:
            # è®¡ç®—å„æ–¹æ³•çš„æ€§èƒ½æŒ‡æ ‡
            performance_metrics = {}
            
            for method, method_results in all_results.items():
                if not method_results:
                    continue
                
                metrics = {
                    'success_rate': 0,
                    'average_confidence': 0,
                    'processing_speed': 0,
                    'result_quality': 0,
                    'convergence_rate': 0
                }
                
                successful_samples = 0
                confidences = []
                times = []
                convergences = []
                
                for result in method_results.values():
                    data = result['data']
                    
                    # æˆåŠŸç‡
                    if data.get('success', True):
                        successful_samples += 1
                    
                    # ç½®ä¿¡åº¦
                    confidence = data.get('noc_confidence') or data.get('confidence')
                    if confidence is not None:
                        confidences.append(float(confidence))
                    
                    # å¤„ç†æ—¶é—´
                    comp_time = data.get('computation_time')
                    if comp_time is not None:
                        times.append(float(comp_time))
                    
                    # æ”¶æ•›æ€§
                    converged = data.get('converged')
                    if converged is not None:
                        convergences.append(bool(converged))
                
                # è®¡ç®—æ ‡å‡†åŒ–æŒ‡æ ‡ (0-1èŒƒå›´)
                metrics['success_rate'] = successful_samples / len(method_results) if method_results else 0
                metrics['average_confidence'] = np.mean(confidences) if confidences else 0
                metrics['processing_speed'] = 1 / (1 + np.mean(times)) if times else 0  # æ—¶é—´è¶ŠçŸ­ï¼Œé€Ÿåº¦åˆ†æ•°è¶Šé«˜
                metrics['convergence_rate'] = np.mean(convergences) if convergences else 0
                
                # ç»“æœè´¨é‡ï¼ˆåŸºäºå¤šä¸ªå› ç´ çš„ç»¼åˆè¯„åˆ†ï¼‰
                quality_score = (metrics['success_rate'] + metrics['average_confidence'] + 
                               metrics['convergence_rate']) / 3
                metrics['result_quality'] = quality_score
                
                performance_metrics[method] = metrics
            
            if not performance_metrics:
                logger.warning("æ²¡æœ‰æ€§èƒ½æŒ‡æ ‡æ•°æ®")
                return 0
            
            # åˆ›å»ºé›·è¾¾å›¾
            categories = ['æˆåŠŸç‡', 'å¹³å‡ç½®ä¿¡åº¦', 'å¤„ç†é€Ÿåº¦', 'ç»“æœè´¨é‡', 'æ”¶æ•›ç‡']
            
            fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))
            
            # è®¾ç½®è§’åº¦
            angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
            angles += angles[:1]  # é—­åˆå›¾å½¢
            
            colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']
            
            for idx, (method, metrics) in enumerate(performance_metrics.items()):
                values = [
                    metrics['success_rate'],
                    metrics['average_confidence'],
                    metrics['processing_speed'],
                    metrics['result_quality'],
                    metrics['convergence_rate']
                ]
                values += values[:1]  # é—­åˆå›¾å½¢
                
                color = colors[idx % len(colors)]
                ax.plot(angles, values, 'o-', linewidth=2, label=self.method_names.get(method, method), color=color)
                ax.fill(angles, values, alpha=0.25, color=color)
            
            # è®¾ç½®æ ‡ç­¾
            ax.set_xticks(angles[:-1])
            ax.set_xticklabels(categories)
            ax.set_ylim(0, 1)
            ax.set_title('å„æ–¹æ³•æ•´ä½“æ€§èƒ½é›·è¾¾å›¾', size=16, y=1.1)
            ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
            ax.grid(True)
            
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, 'performance_radar_chart.png'), 
                       dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info("æ€§èƒ½é›·è¾¾å›¾å·²ç”Ÿæˆ")
            return 1
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ€§èƒ½é›·è¾¾å›¾å¤±è´¥: {e}")
            return 0
    
    def _plot_sample_complexity_analysis(self, all_results: Dict, output_dir: str) -> int:
        """ç»˜åˆ¶æ ·æœ¬å¤æ‚åº¦åˆ†æ"""
        try:
            complexity_data = []
            
            # ä»Q1ç»“æœä¸­æå–å¤æ‚åº¦ç›¸å…³ç‰¹å¾
            q1_results = all_results.get('Q1', {})
            
            for sample_id, result in q1_results.items():
                data = result['data']
                
                # æå–å¤æ‚åº¦æŒ‡æ ‡
                complexity_indicators = {
                    'sample_id': sample_id,
                    'noc': data.get('è´¡çŒ®è€…äººæ•°', 0),
                    'mac_profile': 0,
                    'total_alleles': 0,
                    'avg_alleles_per_locus': 0,
                    'processing_difficulty': 'Unknown'
                }
                
                # ä»ç‰¹å¾ä¸­æå–
                features = data
                complexity_indicators['mac_profile'] = features.get('mac_profile', 0)
                complexity_indicators['total_alleles'] = features.get('total_distinct_alleles', 0)
                complexity_indicators['avg_alleles_per_locus'] = features.get('avg_alleles_per_locus', 0)
                
                # è®¡ç®—å¤„ç†éš¾åº¦
                difficulty_score = (complexity_indicators['mac_profile'] * 0.4 + 
                                  complexity_indicators['total_alleles'] / 50 * 0.3 +
                                  complexity_indicators['avg_alleles_per_locus'] / 5 * 0.3)
                
                if difficulty_score < 2:
                    complexity_indicators['processing_difficulty'] = 'ç®€å•'
                elif difficulty_score < 4:
                    complexity_indicators['processing_difficulty'] = 'ä¸­ç­‰'
                else:
                    complexity_indicators['processing_difficulty'] = 'å¤æ‚'
                
                complexity_data.append(complexity_indicators)
            
            if not complexity_data:
                logger.warning("æ²¡æœ‰å¤æ‚åº¦åˆ†ææ•°æ®")
                return 0
            
            df_complexity = pd.DataFrame(complexity_data)
            
            # åˆ›å»ºå¤æ‚åº¦åˆ†æå›¾
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
            
            # 1. NoCä¸å¤æ‚åº¦çš„å…³ç³»
            noc_complexity = df_complexity.groupby('noc').agg({
                'mac_profile': 'mean',
                'total_alleles': 'mean',
                'avg_alleles_per_locus': 'mean'
            }).reset_index()
            
            ax1.scatter(noc_complexity['noc'], noc_complexity['mac_profile'], 
                       s=100, alpha=0.7, color=self.color_palette['Q1_NoC'])
            ax1.set_xlabel('è´¡çŒ®è€…äººæ•° (NoC)')
            ax1.set_ylabel('å¹³å‡æœ€å¤§ç­‰ä½åŸºå› æ•°')
            ax1.set_title('NoCä¸å›¾è°±å¤æ‚åº¦å…³ç³»')
            ax1.grid(True, alpha=0.3)
            
            # 2. å¤„ç†éš¾åº¦åˆ†å¸ƒ
            difficulty_counts = df_complexity['processing_difficulty'].value_counts()
            colors = ['#98FB98', '#FFD700', '#FF6347']  # ç»¿ã€é»„ã€çº¢
            ax2.pie(difficulty_counts.values, labels=difficulty_counts.index, 
                   colors=colors, autopct='%1.1f%%', startangle=90)
            ax2.set_title('æ ·æœ¬å¤„ç†éš¾åº¦åˆ†å¸ƒ')
            
            # 3. å¤æ‚åº¦æ•£ç‚¹å›¾
            scatter = ax3.scatter(df_complexity['total_alleles'], 
                                df_complexity['avg_alleles_per_locus'],
                                c=df_complexity['noc'], 
                                cmap='viridis', s=60, alpha=0.7)
            ax3.set_xlabel('æ€»ç­‰ä½åŸºå› æ•°')
            ax3.set_ylabel('å¹³å‡æ¯ä½ç‚¹ç­‰ä½åŸºå› æ•°')
            ax3.set_title('æ ·æœ¬å¤æ‚åº¦äºŒç»´åˆ†å¸ƒ')
            plt.colorbar(scatter, ax=ax3, label='NoC')
            
            # 4. ä¸åŒNoCçš„å¤æ‚åº¦åˆ†å¸ƒ
            noc_values = sorted(df_complexity['noc'].unique())
            for noc in noc_values:
                subset = df_complexity[df_complexity['noc'] == noc]
                ax4.hist(subset['mac_profile'], alpha=0.6, 
                        label=f'{noc}äººæ··åˆ', bins=10)
            
            ax4.set_xlabel('æœ€å¤§ç­‰ä½åŸºå› æ•°')
            ax4.set_ylabel('æ ·æœ¬æ•°é‡')
            ax4.set_title('ä¸åŒNoCçš„å¤æ‚åº¦åˆ†å¸ƒ')
            ax4.legend()
            
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, 'sample_complexity_analysis.png'), 
                       dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info("æ ·æœ¬å¤æ‚åº¦åˆ†æå›¾å·²ç”Ÿæˆ")
            return 1
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ ·æœ¬å¤æ‚åº¦åˆ†æå›¾å¤±è´¥: {e}")
            return 0
    
    def create_summary_report(self, all_results: Dict, output_dir: str) -> str:
        """åˆ›å»ºç»¼åˆåˆ†ææŠ¥å‘Š"""
        try:
            report_lines = []
            report_lines.append("="*80)
            report_lines.append("          æ³•åŒ»DNAåˆ†æç»¼åˆç»“æœæŠ¥å‘Š")
            report_lines.append("="*80)
            
            # ç»Ÿè®¡å„æ–¹æ³•çš„æ ·æœ¬æ•°
            report_lines.append("\nğŸ“Š æ–¹æ³•è¦†ç›–ç‡ç»Ÿè®¡:")
            all_samples = set()
            for method_results in all_results.values():
                all_samples.update(method_results.keys())
            
            for method, method_results in all_results.items():
                sample_count = len(method_results)
                coverage_rate = sample_count / len(all_samples) if all_samples else 0
                method_name = self.method_names.get(method, method)
                report_lines.append(f"   {method_name}: {sample_count}ä¸ªæ ·æœ¬ ({coverage_rate:.1%}è¦†ç›–ç‡)")
            
            # NoCé¢„æµ‹ä¸€è‡´æ€§åˆ†æ
            report_lines.append(f"\nğŸ¯ NoCé¢„æµ‹ä¸€è‡´æ€§åˆ†æ:")
            noc_predictions = defaultdict(dict)
            
            for method, method_results in all_results.items():
                for sample_id, result in method_results.items():
                    data = result['data']
                    noc = None
                    if method == 'Q1':
                        noc = data.get('è´¡çŒ®è€…äººæ•°') or data.get('predicted_noc')
                    else:
                        noc = data.get('predicted_noc') or data.get('noc_prediction')
                    
                    if noc is not None:
                        noc_predictions[sample_id][method] = int(noc)
            
            if noc_predictions:
                # è®¡ç®—å„æ–¹æ³•çš„NoCåˆ†å¸ƒ
                for method in all_results.keys():
                    noc_values = [pred.get(method) for pred in noc_predictions.values() if method in pred]
                    if noc_values:
                        noc_dist = pd.Series(noc_values).value_counts().sort_index()
                        method_name = self.method_names.get(method, method)
                        report_lines.append(f"   {method_name} NoCåˆ†å¸ƒ: {dict(noc_dist)}")
            
            # å¤„ç†æ—¶é—´ç»Ÿè®¡
            report_lines.append(f"\nâ±ï¸  å¤„ç†æ—¶é—´ç»Ÿè®¡:")
            for method, method_results in all_results.items():
                times = []
                for result in method_results.values():
                    comp_time = result['data'].get('computation_time')
                    if comp_time is not None:
                        times.append(float(comp_time))
                
                if times:
                    avg_time = np.mean(times)
                    max_time = np.max(times)
                    min_time = np.min(times)
                    method_name = self.method_names.get(method, method)
                    report_lines.append(f"   {method_name}: å¹³å‡{avg_time:.1f}s (èŒƒå›´: {min_time:.1f}s - {max_time:.1f}s)")
            
            # æˆåŠŸç‡ç»Ÿè®¡
            report_lines.append(f"\nâœ… åˆ†ææˆåŠŸç‡:")
            for method, method_results in all_results.items():
                successful = 0
                total = len(method_results)
                
                for result in method_results.values():
                    data = result['data']
                    if data.get('success', True):  # é»˜è®¤ä¸ºæˆåŠŸ
                        successful += 1
                
                success_rate = successful / total if total > 0 else 0
                method_name = self.method_names.get(method, method)
                report_lines.append(f"   {method_name}: {successful}/{total} ({success_rate:.1%})")
            
            # è´¨é‡è¯„ä¼°
            report_lines.append(f"\nğŸ† è´¨é‡è¯„ä¼°:")
            for method, method_results in all_results.items():
                confidences = []
                convergences = []
                
                for result in method_results.values():
                    data = result['data']
                    
                    # æ”¶é›†ç½®ä¿¡åº¦
                    confidence = data.get('noc_confidence') or data.get('confidence')
                    if confidence is not None:
                        confidences.append(float(confidence))
                    
                    # æ”¶é›†æ”¶æ•›æ€§
                    converged = data.get('converged')
                    if converged is not None:
                        convergences.append(bool(converged))
                
                method_name = self.method_names.get(method, method)
                
                if confidences:
                    avg_confidence = np.mean(confidences)
                    report_lines.append(f"   {method_name} å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.3f}")
                
                if convergences:
                    convergence_rate = np.mean(convergences)
                    report_lines.append(f"   {method_name} æ”¶æ•›ç‡: {convergence_rate:.1%}")
            
            # å…³é”®å‘ç°
            report_lines.append(f"\nğŸ’¡ å…³é”®å‘ç°:")
            
            # æ‰¾å‡ºå¤„ç†æœ€å¿«çš„æ–¹æ³•
            fastest_method = None
            fastest_time = float('inf')
            for method, method_results in all_results.items():
                times = [float(result['data'].get('computation_time', float('inf'))) 
                        for result in method_results.values() 
                        if result['data'].get('computation_time') is not None]
                if times:
                    avg_time = np.mean(times)
                    if avg_time < fastest_time:
                        fastest_time = avg_time
                        fastest_method = method
            
            if fastest_method:
                report_lines.append(f"   â€¢ å¤„ç†é€Ÿåº¦æœ€å¿«: {self.method_names.get(fastest_method, fastest_method)} (å¹³å‡{fastest_time:.1f}ç§’)")
            
            # æ‰¾å‡ºè¦†ç›–ç‡æœ€é«˜çš„æ–¹æ³•
            highest_coverage_method = None
            highest_coverage = 0
            for method, method_results in all_results.items():
                coverage = len(method_results) / len(all_samples) if all_samples else 0
                if coverage > highest_coverage:
                    highest_coverage = coverage
                    highest_coverage_method = method
            
            if highest_coverage_method:
                report_lines.append(f"   â€¢ æ ·æœ¬è¦†ç›–ç‡æœ€é«˜: {self.method_names.get(highest_coverage_method, highest_coverage_method)} ({highest_coverage:.1%})")
            
            # æ¨èçš„åˆ†ææµç¨‹
            report_lines.append(f"\nğŸš€ æ¨èåˆ†ææµç¨‹:")
            report_lines.append(f"   1. ä½¿ç”¨Q1æ–¹æ³•è¿›è¡ŒNoCè¯†åˆ«å’Œç‰¹å¾æå–")
            report_lines.append(f"   2. åŸºäºQ1ç»“æœï¼Œä½¿ç”¨Q2æ–¹æ³•æ¨æ–­æ··åˆæ¯”ä¾‹")
            report_lines.append(f"   3. ç»“åˆQ1å’ŒQ2ç»“æœï¼Œä½¿ç”¨Q3æ–¹æ³•è¿›è¡ŒåŸºå› å‹æ¨æ–­")
            report_lines.append(f"   4. å¯¹äºä½è´¨é‡æ ·æœ¬ï¼Œä½¿ç”¨Q4æ–¹æ³•è¿›è¡Œé™å™ªå¤„ç†")
            
            report_lines.append(f"\nğŸ“ˆ æ”¹è¿›å»ºè®®:")
            
            # åŸºäºç»“æœç»™å‡ºæ”¹è¿›å»ºè®®
            for method, method_results in all_results.items():
                method_name = self.method_names.get(method, method)
                
                # è®¡ç®—è¯¥æ–¹æ³•çš„é—®é¢˜
                issues = []
                
                # æ£€æŸ¥å¤„ç†æ—¶é—´
                times = [float(result['data'].get('computation_time', 0)) 
                        for result in method_results.values() 
                        if result['data'].get('computation_time') is not None]
                if times and np.mean(times) > 60:  # è¶…è¿‡1åˆ†é’Ÿ
                    issues.append("å¤„ç†æ—¶é—´è¾ƒé•¿ï¼Œå»ºè®®ä¼˜åŒ–ç®—æ³•æˆ–å‚æ•°")
                
                # æ£€æŸ¥æˆåŠŸç‡
                successful = sum(1 for result in method_results.values() 
                               if result['data'].get('success', True))
                success_rate = successful / len(method_results) if method_results else 0
                if success_rate < 0.8:
                    issues.append("æˆåŠŸç‡åä½ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®è´¨é‡æˆ–è°ƒæ•´å‚æ•°")
                
                # æ£€æŸ¥æ”¶æ•›æ€§
                convergences = [bool(result['data'].get('converged', True)) 
                              for result in method_results.values() 
                              if 'converged' in result['data']]
                if convergences and np.mean(convergences) < 0.7:
                    issues.append("MCMCæ”¶æ•›ç‡åä½ï¼Œå»ºè®®å¢åŠ è¿­ä»£æ¬¡æ•°æˆ–è°ƒæ•´æè®®åˆ†å¸ƒ")
                
                if issues:
                    report_lines.append(f"   â€¢ {method_name}:")
                    for issue in issues:
                        report_lines.append(f"     - {issue}")
            
            report_lines.append(f"\n" + "="*80)
            report_lines.append(f"æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
            report_lines.append(f"="*80)
            
            # ä¿å­˜æŠ¥å‘Š
            report_content = '\n'.join(report_lines)
            report_path = os.path.join(output_dir, 'comprehensive_analysis_report.txt')
            
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report_content)
            
            logger.info(f"ç»¼åˆåˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
            return report_path
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆç»¼åˆæŠ¥å‘Šå¤±è´¥: {e}")
            return ""

def integrate_all_methods(q1_dir: str = None, q2_dir: str = None, 
                         q3_dir: str = None, q4_dir: str = None,
                         output_dir: str = "./integrated_results") -> Dict:
    """
    é›†æˆæ‰€æœ‰æ–¹æ³•çš„å¯è§†åŒ–ç»“æœ
    
    Args:
        q1_dir: Q1ç»“æœç›®å½•è·¯å¾„
        q2_dir: Q2ç»“æœç›®å½•è·¯å¾„  
        q3_dir: Q3ç»“æœç›®å½•è·¯å¾„
        q4_dir: Q4ç»“æœç›®å½•è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•è·¯å¾„
        
    Returns:
        Dict: åŒ…å«å¤„ç†ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
    """
    integrator = VisualizationIntegrator()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    print("ğŸ” å¼€å§‹åŠ è½½å„æ–¹æ³•çš„åˆ†æç»“æœ...")
    
    # åŠ è½½å„æ–¹æ³•çš„ç»“æœ
    all_results = {}
    
    # å®šä¹‰æ–¹æ³•ç›®å½•æ˜ å°„
    method_dirs = {
        'Q1': q1_dir,
        'Q2': q2_dir, 
        'Q3': q3_dir,
        'Q4': q4_dir
    }
    
    # åŠ è½½ç»“æœ
    for method, dir_path in method_dirs.items():
        if dir_path and os.path.exists(dir_path):
            print(f"   æ­£åœ¨åŠ è½½{method}ç»“æœ...")
            method_results = integrator.load_results_from_directory(dir_path, method)
            if method_results:
                all_results[method] = method_results
                print(f"   âœ… {method}: åŠ è½½äº†{len(method_results)}ä¸ªæ ·æœ¬ç»“æœ")
            else:
                print(f"   âš ï¸  {method}: æœªæ‰¾åˆ°æœ‰æ•ˆç»“æœ")
        else:
            print(f"   âŒ {method}: ç›®å½•ä¸å­˜åœ¨æˆ–æœªæŒ‡å®š - {dir_path}")
    
    if not all_results:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„åˆ†æç»“æœ")
        return {
            'total_samples': 0,
            'total_plots': 0,
            'methods_processed': 0,
            'success': False
        }
    
    print(f"\nğŸ“Š å¼€å§‹ç”Ÿæˆé›†æˆå¯è§†åŒ–å›¾è¡¨...")
    
    # ç”Ÿæˆç»Ÿä¸€æ¯”è¾ƒå›¾è¡¨
    total_plots = integrator.create_unified_comparison_plots(all_results, output_dir)
    
    # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    print("ğŸ“ ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...")
    report_path = integrator.create_summary_report(all_results, output_dir)
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_samples = len(set().union(*[results.keys() for results in all_results.values()]))
    
    result_summary = {
        'total_samples': total_samples,
        'total_plots': total_plots,
        'methods_processed': len(all_results),
        'methods_found': list(all_results.keys()),
        'output_directory': output_dir,
        'report_path': report_path,
        'success': True
    }
    
    print(f"\nâœ… å¯è§†åŒ–é›†æˆå®Œæˆ!")
    print(f"   ğŸ“ˆ ç”Ÿæˆå›¾è¡¨: {total_plots}ä¸ª")
    print(f"   ğŸ“Š å¤„ç†æ ·æœ¬: {total_samples}ä¸ª")
    print(f"   ğŸ”¬ é›†æˆæ–¹æ³•: {', '.join(all_results.keys())}")
    print(f"   ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    if report_path:
        print(f"   ğŸ“„ ç»¼åˆæŠ¥å‘Š: {report_path}")
    
    return result_summary

def create_method_comparison_dashboard(all_results: Dict, output_dir: str) -> str:
    """åˆ›å»ºæ–¹æ³•å¯¹æ¯”ä»ªè¡¨æ¿"""
    try:
        # è¿™é‡Œå¯ä»¥æ‰©å±•ä¸ºäº¤äº’å¼ä»ªè¡¨æ¿
        # ç›®å‰åˆ›å»ºé™æ€çš„HTMLæŠ¥å‘Š
        
        html_content = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <meta charset="UTF-8">
            <title>æ³•åŒ»DNAåˆ†ææ–¹æ³•å¯¹æ¯”ä»ªè¡¨æ¿</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                .header {{ background-color: #f0f0f0; padding: 20px; text-align: center; }}
                .section {{ margin: 20px 0; padding: 15px; border-left: 4px solid #007acc; }}
                .method {{ background-color: #f9f9f9; margin: 10px 0; padding: 10px; }}
                .metric {{ display: inline-block; margin: 5px 10px; }}
                .good {{ color: green; }}
                .warning {{ color: orange; }}
                .error {{ color: red; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>æ³•åŒ»DNAåˆ†ææ–¹æ³•å¯¹æ¯”ä»ªè¡¨æ¿</h1>
                <p>ç”Ÿæˆæ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
            </div>
            
            <div class="section">
                <h2>æ–¹æ³•æ¦‚è§ˆ</h2>
        """
        
        for method, results in all_results.items():
            method_name = VisualizationIntegrator().method_names.get(method, method)
            sample_count = len(results)
            
            html_content += f"""
                <div class="method">
                    <h3>{method_name}</h3>
                    <div class="metric">æ ·æœ¬æ•°: <strong>{sample_count}</strong></div>
                </div>
            """
        
        html_content += """
            </div>
            
            <div class="section">
                <h2>ç”Ÿæˆçš„å›¾è¡¨</h2>
                <ul>
                    <li><a href="method_coverage_comparison.png">æ–¹æ³•è¦†ç›–ç‡å¯¹æ¯”</a></li>
                    <li><a href="noc_consistency_analysis.png">NoCé¢„æµ‹ä¸€è‡´æ€§åˆ†æ</a></li>
                    <li><a href="mixture_ratio_comparison.png">æ··åˆæ¯”ä¾‹é¢„æµ‹å¯¹æ¯”</a></li>
                    <li><a href="processing_time_comparison.png">å¤„ç†æ—¶é—´å¯¹æ¯”</a></li>
                    <li><a href="performance_radar_chart.png">æ•´ä½“æ€§èƒ½é›·è¾¾å›¾</a></li>
                    <li><a href="sample_complexity_analysis.png">æ ·æœ¬å¤æ‚åº¦åˆ†æ</a></li>
                </ul>
            </div>
            
            <div class="section">
                <h2>è¯¦ç»†æŠ¥å‘Š</h2>
                <p><a href="comprehensive_analysis_report.txt">æŸ¥çœ‹å®Œæ•´æ–‡æœ¬æŠ¥å‘Š</a></p>
            </div>
        </body>
        </html>
        """
        
        dashboard_path = os.path.join(output_dir, 'dashboard.html')
        with open(dashboard_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        logger.info(f"ä»ªè¡¨æ¿å·²ç”Ÿæˆ: {dashboard_path}")
        return dashboard_path
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆä»ªè¡¨æ¿å¤±è´¥: {e}")
        return ""

# ä¸»å‡½æ•°ç¤ºä¾‹
if __name__ == "__main__":
    # ç¤ºä¾‹ä½¿ç”¨
    results = integrate_all_methods(
        q1_dir="./q1_results",
        q2_dir="./q2_mgm_rf_results",
        q3_dir="./q3_enhanced_results", 
        q4_dir="./q4_upg_denoising_results",
        output_dir="./integrated_visualization_results"
    )
    
    print(f"é›†æˆå®Œæˆ: {results}")