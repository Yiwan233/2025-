# -*- coding: utf-8 -*-
"""
æ³•åŒ»DNAåˆ†æ - Q2/Q3/Q4æ··åˆæ¯”ä¾‹é¢„æµ‹å‡æ–¹è¯¯å·®(MSE)è®¡ç®—å™¨

ç‰ˆæœ¬: V1.0
æ—¥æœŸ: 2025-06-10
æè¿°: è®¡ç®—Q2ã€Q3ã€Q4æ–¹æ³•çš„æ··åˆæ¯”ä¾‹é¢„æµ‹å‡æ–¹è¯¯å·®
åŠŸèƒ½:
1. ä»æ–‡ä»¶åè§£æçœŸå®æ··åˆæ¯”ä¾‹
2. åŠ è½½å„æ–¹æ³•çš„é¢„æµ‹ç»“æœ
3. è®¡ç®—MSEã€MAEã€RMSEç­‰è¯„ä¼°æŒ‡æ ‡
4. ç”Ÿæˆå¯¹æ¯”åˆ†ææŠ¥å‘Šå’Œå¯è§†åŒ–å›¾è¡¨
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
import json
import re
import warnings
from typing import Dict, List, Tuple, Optional
from pathlib import Path
import logging
from sklearn.metrics import mean_squared_error, mean_absolute_error

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œå¿½ç•¥è­¦å‘Š
plt.rcParams['font.sans-serif'] = ["Arial Unicode MS", "SimHei", "Microsoft YaHei"]
plt.rcParams['axes.unicode_minus'] = False
warnings.filterwarnings('ignore')

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

print("=" * 80)
print("                Q2/Q3/Q4æ··åˆæ¯”ä¾‹é¢„æµ‹å‡æ–¹è¯¯å·®(MSE)è®¡ç®—å™¨")
print("           Mixture Ratio Prediction MSE Calculator")
print("=" * 80)

class MixtureRatioMSECalculator:
    """æ··åˆæ¯”ä¾‹MSEè®¡ç®—å™¨"""
    
    def __init__(self):
        self.true_ratios = {}  # çœŸå®æ··åˆæ¯”ä¾‹
        self.q2_results = {}   # Q2é¢„æµ‹ç»“æœ
        self.q3_results = {}   # Q3é¢„æµ‹ç»“æœ  
        self.q4_results = {}   # Q4é¢„æµ‹ç»“æœ
        self.evaluation_results = {}
        
        # ç»“æœä¿å­˜ç›®å½•
        self.output_dir = './mse_evaluation_results'
        os.makedirs(self.output_dir, exist_ok=True)
        
        logger.info("æ··åˆæ¯”ä¾‹MSEè®¡ç®—å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def parse_mixture_ratio_from_filename(self, filename: str) -> Tuple[List[str], List[float]]:
        """
        ä»æ–‡ä»¶åè§£ææ··åˆæ¯”ä¾‹
        
        ä¾‹å¦‚: "A02_RD14-0003-40_41-1;4-M3S30-0.075IP-Q4.0_001.5sec.fsa"
        è§£æä¸º: è´¡çŒ®è€…IDs=['40', '41'], æ··åˆæ¯”ä¾‹=[0.2, 0.8] (1:4æ ‡å‡†åŒ–å)
        """
        try:
            # åŒ¹é…æ¨¡å¼: -è´¡çŒ®è€…IDs-æ¯”ä¾‹-
            pattern = r'-(\d+(?:_\d+)*)-([^-]*?)-M\d'
            match = re.search(pattern, str(filename))
            
            if not match:
                logger.warning(f"æ— æ³•ä»æ–‡ä»¶åè§£æè´¡çŒ®è€…ä¿¡æ¯: {filename}")
                return None, None
            
            contributor_ids = match.group(1).split('_')
            ratio_part = match.group(2)
            
            logger.debug(f"è§£ææ–‡ä»¶å: {filename}")
            logger.debug(f"  è´¡çŒ®è€…IDs: {contributor_ids}")
            logger.debug(f"  æ¯”ä¾‹éƒ¨åˆ†: '{ratio_part}'")
            
            # è§£ææ¯”ä¾‹éƒ¨åˆ†
            if ';' in ratio_part:
                try:
                    ratio_values = [float(x) for x in ratio_part.split(';')]
                    
                    if len(ratio_values) == len(contributor_ids):
                        # æ ‡å‡†åŒ–ä¸ºæ¦‚ç‡
                        total = sum(ratio_values)
                        if total > 0:
                            true_ratios = [r/total for r in ratio_values]
                            logger.debug(f"  è§£æå¾—åˆ°çš„æ ‡å‡†åŒ–æ¯”ä¾‹: {true_ratios}")
                            return contributor_ids, true_ratios
                        else:
                            logger.warning(f"æ¯”ä¾‹å€¼æ€»å’Œä¸º0: {filename}")
                    else:
                        logger.warning(f"æ¯”ä¾‹æ•°é‡({len(ratio_values)})ä¸è´¡çŒ®è€…æ•°é‡({len(contributor_ids)})ä¸åŒ¹é…")
                except ValueError as e:
                    logger.warning(f"è§£ææ¯”ä¾‹å€¼å¤±è´¥: {e}")
            
            # å¦‚æœè§£æå¤±è´¥ï¼Œå‡è®¾ç­‰æ¯”ä¾‹
            true_ratios = [1.0/len(contributor_ids)] * len(contributor_ids)
            logger.debug(f"  ä½¿ç”¨ç­‰æ¯”ä¾‹å‡è®¾: {true_ratios}")
            return contributor_ids, true_ratios
            
        except Exception as e:
            logger.error(f"è§£ææ–‡ä»¶åæ—¶å‡ºé”™: {e}")
            return None, None
    
    def load_true_ratios_from_data(self, data_path: str):
        """ä»æ•°æ®æ–‡ä»¶åŠ è½½çœŸå®æ··åˆæ¯”ä¾‹"""
        logger.info(f"ä»æ•°æ®æ–‡ä»¶åŠ è½½çœŸå®æ··åˆæ¯”ä¾‹: {data_path}")
        
        try:
            df = pd.read_csv(data_path, encoding='utf-8')
            sample_files = df['Sample File'].unique()
            
            logger.info(f"å‘ç° {len(sample_files)} ä¸ªæ ·æœ¬")
            
            for sample_file in sample_files:
                contributor_ids, true_ratios = self.parse_mixture_ratio_from_filename(sample_file)
                
                if contributor_ids and true_ratios:
                    self.true_ratios[sample_file] = {
                        'contributor_ids': contributor_ids,
                        'ratios': true_ratios,
                        'noc': len(contributor_ids)
                    }
                    logger.info(f"æ ·æœ¬ {sample_file}: {len(contributor_ids)}ä¸ªè´¡çŒ®è€…, æ¯”ä¾‹={true_ratios}")
            
            logger.info(f"æˆåŠŸè§£æ {len(self.true_ratios)} ä¸ªæ ·æœ¬çš„çœŸå®æ··åˆæ¯”ä¾‹")
            
        except Exception as e:
            logger.error(f"åŠ è½½çœŸå®æ¯”ä¾‹å¤±è´¥: {e}")
    
    def load_q2_results(self, q2_results_dir: str):
        """åŠ è½½Q2æ–¹æ³•çš„é¢„æµ‹ç»“æœ"""
        logger.info(f"åŠ è½½Q2é¢„æµ‹ç»“æœ: {q2_results_dir}")
        
        try:
            q2_dir = Path(q2_results_dir)
            if not q2_dir.exists():
                logger.warning(f"Q2ç»“æœç›®å½•ä¸å­˜åœ¨: {q2_results_dir}")
                return
            
            # æŸ¥æ‰¾Q2ç»“æœæ–‡ä»¶
            result_files = list(q2_dir.glob("*_result.json")) + list(q2_dir.glob("*_analysis_result.json"))
            
            logger.info(f"æ‰¾åˆ° {len(result_files)} ä¸ªQ2ç»“æœæ–‡ä»¶")
            
            for result_file in result_files:
                try:
                    with open(result_file, 'r', encoding='utf-8') as f:
                        result_data = json.load(f)
                    
                    sample_file = result_data.get('sample_file', '')
                    if not sample_file:
                        # å°è¯•ä»æ–‡ä»¶åæå–æ ·æœ¬ID
                        sample_file = result_file.stem.replace('_result', '').replace('_analysis_result', '')
                    
                    # æå–æ··åˆæ¯”ä¾‹
                    posterior_summary = result_data.get('posterior_summary', {})
                    if posterior_summary:
                        predicted_ratios = []
                        noc = result_data.get('predicted_noc', 0)
                        
                        for i in range(1, noc + 1):
                            mx_key = f'Mx_{i}'
                            if mx_key in posterior_summary:
                                ratio = posterior_summary[mx_key].get('mean', 0)
                                predicted_ratios.append(ratio)
                        
                        if predicted_ratios:
                            self.q2_results[sample_file] = {
                                'ratios': predicted_ratios,
                                'noc': noc,
                                'confidence': result_data.get('noc_confidence', 0),
                                'mcmc_converged': result_data.get('mcmc_quality', {}).get('converged', False)
                            }
                            logger.debug(f"Q2 - {sample_file}: {predicted_ratios}")
                
                except Exception as e:
                    logger.warning(f"åŠ è½½Q2ç»“æœæ–‡ä»¶å¤±è´¥ {result_file}: {e}")
            
            logger.info(f"æˆåŠŸåŠ è½½ {len(self.q2_results)} ä¸ªQ2é¢„æµ‹ç»“æœ")
            
        except Exception as e:
            logger.error(f"åŠ è½½Q2ç»“æœå¤±è´¥: {e}")
    
    def load_q3_results(self, q3_results_dir: str):
        """åŠ è½½Q3æ–¹æ³•çš„é¢„æµ‹ç»“æœ"""
        logger.info(f"åŠ è½½Q3é¢„æµ‹ç»“æœ: {q3_results_dir}")
        
        try:
            q3_dir = Path(q3_results_dir)
            if not q3_dir.exists():
                logger.warning(f"Q3ç»“æœç›®å½•ä¸å­˜åœ¨: {q3_results_dir}")
                return
            
            # æŸ¥æ‰¾Q3ç»“æœæ–‡ä»¶
            result_files = list(q3_dir.glob("*_result.json")) + list(q3_dir.glob("*_analysis_result.json"))
            
            logger.info(f"æ‰¾åˆ° {len(result_files)} ä¸ªQ3ç»“æœæ–‡ä»¶")
            
            for result_file in result_files:
                try:
                    with open(result_file, 'r', encoding='utf-8') as f:
                        result_data = json.load(f)
                    
                    sample_file = result_data.get('sample_id', '')
                    if not sample_file:
                        sample_file = result_file.stem.replace('_result', '').replace('_analysis_result', '')
                    
                    # Q3çš„ç»“æœå¯èƒ½åŒ…å«åŸºå› å‹ä¿¡æ¯å’Œæ··åˆæ¯”ä¾‹
                    # å¦‚æœQ3ä¹Ÿè¾“å‡ºäº†æ··åˆæ¯”ä¾‹ï¼Œæå–å®ƒä»¬
                    mcmc_results = result_data.get('mcmc_results', {})
                    if mcmc_results and 'mixture_ratio_summary' in mcmc_results:
                        mx_summary = mcmc_results['mixture_ratio_summary']
                        predicted_ratios = []
                        noc = result_data.get('predicted_noc', 0)
                        
                        for i in range(1, noc + 1):
                            mx_key = f'Mx_{i}'
                            if mx_key in mx_summary:
                                ratio = mx_summary[mx_key].get('mean', 0)
                                predicted_ratios.append(ratio)
                        
                        if predicted_ratios:
                            self.q3_results[sample_file] = {
                                'ratios': predicted_ratios,
                                'noc': noc,
                                'confidence': result_data.get('noc_confidence', 0),
                                'mcmc_converged': mcmc_results.get('converged', False)
                            }
                            logger.debug(f"Q3 - {sample_file}: {predicted_ratios}")
                
                except Exception as e:
                    logger.warning(f"åŠ è½½Q3ç»“æœæ–‡ä»¶å¤±è´¥ {result_file}: {e}")
            
            logger.info(f"æˆåŠŸåŠ è½½ {len(self.q3_results)} ä¸ªQ3é¢„æµ‹ç»“æœ")
            
        except Exception as e:
            logger.error(f"åŠ è½½Q3ç»“æœå¤±è´¥: {e}")
    
    def load_q4_results(self, q4_results_dir: str):
        """åŠ è½½Q4æ–¹æ³•çš„é¢„æµ‹ç»“æœ"""
        logger.info(f"åŠ è½½Q4é¢„æµ‹ç»“æœ: {q4_results_dir}")
        
        try:
            q4_dir = Path(q4_results_dir)
            if not q4_dir.exists():
                logger.warning(f"Q4ç»“æœç›®å½•ä¸å­˜åœ¨: {q4_results_dir}")
                return
            
            # æŸ¥æ‰¾Q4ç»“æœæ–‡ä»¶
            sample_dirs = [d for d in q4_dir.iterdir() if d.is_dir() and d.name.startswith('sample_')]
            
            logger.info(f"æ‰¾åˆ° {len(sample_dirs)} ä¸ªQ4æ ·æœ¬ç›®å½•")
            
            for sample_dir in sample_dirs:
                try:
                    # æŸ¥æ‰¾æ ·æœ¬åˆ†ææ‘˜è¦æ–‡ä»¶
                    summary_files = list(sample_dir.glob("*_analysis_summary.json"))
                    
                    if not summary_files:
                        continue
                    
                    with open(summary_files[0], 'r', encoding='utf-8') as f:
                        result_data = json.load(f)
                    
                    sample_file = result_data.get('sample_file', '')
                    if not sample_file:
                        sample_file = sample_dir.name.replace('sample_', '')
                    
                    # Q4çš„ç»“æœä¸»è¦æ˜¯é™å™ªï¼Œä½†å¦‚æœæœ‰æ··åˆæ¯”ä¾‹ä¼°è®¡ï¼Œæå–å®ƒä»¬
                    posterior_summary = result_data.get('posterior_summary', {})
                    if posterior_summary and 'mixture_ratio_summary' in posterior_summary:
                        mx_summary = posterior_summary['mixture_ratio_summary']
                        
                        # æ‰¾åˆ°æœ€å¯èƒ½çš„NoC
                        most_probable_noc = posterior_summary.get('most_probable_noc', 2)
                        noc_key = f'NoC_{most_probable_noc}'
                        
                        if noc_key in mx_summary:
                            predicted_ratios = []
                            for i in range(1, most_probable_noc + 1):
                                mx_key = f'Mx_{i}'
                                if mx_key in mx_summary[noc_key]:
                                    ratio = mx_summary[noc_key][mx_key].get('mean', 0)
                                    predicted_ratios.append(ratio)
                            
                            if predicted_ratios:
                                self.q4_results[sample_file] = {
                                    'ratios': predicted_ratios,
                                    'noc': most_probable_noc,
                                    'confidence': posterior_summary.get('noc_confidence', 0),
                                    'mcmc_converged': posterior_summary.get('model_quality', {}).get('converged', False)
                                }
                                logger.debug(f"Q4 - {sample_file}: {predicted_ratios}")
                
                except Exception as e:
                    logger.warning(f"åŠ è½½Q4ç»“æœå¤±è´¥ {sample_dir}: {e}")
            
            logger.info(f"æˆåŠŸåŠ è½½ {len(self.q4_results)} ä¸ªQ4é¢„æµ‹ç»“æœ")
            
        except Exception as e:
            logger.error(f"åŠ è½½Q4ç»“æœå¤±è´¥: {e}")
    
    def align_ratios(self, true_ratios: List[float], predicted_ratios: List[float]) -> Tuple[List[float], List[float]]:
        """
        å¯¹é½çœŸå®æ¯”ä¾‹å’Œé¢„æµ‹æ¯”ä¾‹
        å¤„ç†è´¡çŒ®è€…æ•°é‡ä¸åŒ¹é…çš„æƒ…å†µ
        """
        true_ratios = np.array(true_ratios)
        predicted_ratios = np.array(predicted_ratios)
        
        # å¦‚æœé•¿åº¦ç›¸åŒï¼Œç›´æ¥è¿”å›
        if len(true_ratios) == len(predicted_ratios):
            return true_ratios.tolist(), predicted_ratios.tolist()
        
        # å¦‚æœé¢„æµ‹çš„è´¡çŒ®è€…æ•°é‡å°‘äºçœŸå®æ•°é‡ï¼Œç”¨0å¡«å……
        if len(predicted_ratios) < len(true_ratios):
            padded_predicted = np.zeros(len(true_ratios))
            padded_predicted[:len(predicted_ratios)] = predicted_ratios
            return true_ratios.tolist(), padded_predicted.tolist()
        
        # å¦‚æœé¢„æµ‹çš„è´¡çŒ®è€…æ•°é‡å¤šäºçœŸå®æ•°é‡ï¼Œæˆªæ–­
        if len(predicted_ratios) > len(true_ratios):
            # æŒ‰ç…§é¢„æµ‹æ¯”ä¾‹å¤§å°æ’åºï¼Œå–å‰Nä¸ª
            sorted_indices = np.argsort(predicted_ratios)[::-1]
            top_predicted = predicted_ratios[sorted_indices[:len(true_ratios)]]
            # é‡æ–°æ ‡å‡†åŒ–
            top_predicted = top_predicted / np.sum(top_predicted)
            
            # å¯¹åº”çš„çœŸå®æ¯”ä¾‹ä¹Ÿéœ€è¦æ’åºï¼ˆæŒ‰ä»å¤§åˆ°å°ï¼‰
            true_sorted_indices = np.argsort(true_ratios)[::-1]
            aligned_true = true_ratios[true_sorted_indices]
            
            return aligned_true.tolist(), top_predicted.tolist()
        
        return true_ratios.tolist(), predicted_ratios.tolist()
    
    def calculate_mse_metrics(self, method_name: str, method_results: Dict) -> Dict:
        """è®¡ç®—å•ä¸ªæ–¹æ³•çš„MSEç­‰è¯„ä¼°æŒ‡æ ‡"""
        logger.info(f"è®¡ç®— {method_name} çš„è¯„ä¼°æŒ‡æ ‡")
        
        all_true_ratios = []
        all_predicted_ratios = []
        sample_wise_metrics = {}
        
        matched_samples = 0
        
        for sample_file, true_data in self.true_ratios.items():
            if sample_file not in method_results:
                continue
            
            true_ratios = true_data['ratios']
            predicted_data = method_results[sample_file]
            predicted_ratios = predicted_data['ratios']
            
            # å¯¹é½æ¯”ä¾‹
            aligned_true, aligned_predicted = self.align_ratios(true_ratios, predicted_ratios)
            
            # è®¡ç®—æ ·æœ¬çº§åˆ«çš„æŒ‡æ ‡
            mse = mean_squared_error(aligned_true, aligned_predicted)
            mae = mean_absolute_error(aligned_true, aligned_predicted)
            rmse = np.sqrt(mse)
            
            # è®¡ç®—æœ€å¤§ç»å¯¹è¯¯å·®
            max_error = np.max(np.abs(np.array(aligned_true) - np.array(aligned_predicted)))
            
            sample_wise_metrics[sample_file] = {
                'true_ratios': aligned_true,
                'predicted_ratios': aligned_predicted,
                'mse': mse,
                'mae': mae,
                'rmse': rmse,
                'max_error': max_error,
                'true_noc': true_data['noc'],
                'predicted_noc': predicted_data['noc'],
                'noc_correct': true_data['noc'] == predicted_data['noc']
            }
            
            # æ·»åŠ åˆ°æ€»ä½“è®¡ç®—
            all_true_ratios.extend(aligned_true)
            all_predicted_ratios.extend(aligned_predicted)
            matched_samples += 1
        
        if matched_samples == 0:
            logger.warning(f"{method_name} æ²¡æœ‰åŒ¹é…çš„æ ·æœ¬")
            return {}
        
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        overall_mse = mean_squared_error(all_true_ratios, all_predicted_ratios)
        overall_mae = mean_absolute_error(all_true_ratios, all_predicted_ratios)
        overall_rmse = np.sqrt(overall_mse)
        overall_max_error = np.max(np.abs(np.array(all_true_ratios) - np.array(all_predicted_ratios)))
        
        # è®¡ç®—NoCå‡†ç¡®ç‡
        noc_correct_count = sum(1 for m in sample_wise_metrics.values() if m['noc_correct'])
        noc_accuracy = noc_correct_count / matched_samples
        
        # è®¡ç®—ä¸åŒè¯¯å·®èŒƒå›´çš„æ ·æœ¬æ¯”ä¾‹
        sample_mses = [m['mse'] for m in sample_wise_metrics.values()]
        excellent_samples = sum(1 for mse in sample_mses if mse < 0.01)  # MSE < 0.01
        good_samples = sum(1 for mse in sample_mses if 0.01 <= mse < 0.05)  # 0.01 <= MSE < 0.05
        fair_samples = sum(1 for mse in sample_mses if 0.05 <= mse < 0.1)   # 0.05 <= MSE < 0.1
        poor_samples = matched_samples - excellent_samples - good_samples - fair_samples
        
        overall_metrics = {
            'method_name': method_name,
            'matched_samples': matched_samples,
            'total_samples': len(self.true_ratios),
            'coverage_rate': matched_samples / len(self.true_ratios),
            
            # æ€»ä½“è¯¯å·®æŒ‡æ ‡
            'overall_mse': overall_mse,
            'overall_mae': overall_mae,
            'overall_rmse': overall_rmse,
            'overall_max_error': overall_max_error,
            
            # NoCå‡†ç¡®æ€§
            'noc_accuracy': noc_accuracy,
            
            # æ ·æœ¬çº§åˆ«è¯¯å·®åˆ†å¸ƒ
            'mean_sample_mse': np.mean(sample_mses),
            'median_sample_mse': np.median(sample_mses),
            'std_sample_mse': np.std(sample_mses),
            
            # æ€§èƒ½ç­‰çº§åˆ†å¸ƒ
            'excellent_samples': excellent_samples,
            'good_samples': good_samples,
            'fair_samples': fair_samples,
            'poor_samples': poor_samples,
            'excellent_rate': excellent_samples / matched_samples,
            'good_rate': good_samples / matched_samples,
            'fair_rate': fair_samples / matched_samples,
            'poor_rate': poor_samples / matched_samples,
            
            # æ ·æœ¬çº§åˆ«è¯¦ç»†ç»“æœ
            'sample_wise_metrics': sample_wise_metrics
        }
        
        logger.info(f"{method_name} è¯„ä¼°å®Œæˆ:")
        logger.info(f"  åŒ¹é…æ ·æœ¬æ•°: {matched_samples}/{len(self.true_ratios)}")
        logger.info(f"  æ€»ä½“MSE: {overall_mse:.6f}")
        logger.info(f"  æ€»ä½“RMSE: {overall_rmse:.6f}")
        logger.info(f"  NoCå‡†ç¡®ç‡: {noc_accuracy:.3f}")
        
        return overall_metrics
    
    def calculate_all_metrics(self):
        """è®¡ç®—æ‰€æœ‰æ–¹æ³•çš„è¯„ä¼°æŒ‡æ ‡"""
        logger.info("å¼€å§‹è®¡ç®—æ‰€æœ‰æ–¹æ³•çš„è¯„ä¼°æŒ‡æ ‡")
        
        methods = {
            'Q2': self.q2_results,
            'Q3': self.q3_results,
            'Q4': self.q4_results
        }
        
        for method_name, method_results in methods.items():
            if method_results:
                metrics = self.calculate_mse_metrics(method_name, method_results)
                if metrics:
                    self.evaluation_results[method_name] = metrics
            else:
                logger.warning(f"{method_name} æ²¡æœ‰é¢„æµ‹ç»“æœ")
        
        logger.info(f"å®Œæˆ {len(self.evaluation_results)} ä¸ªæ–¹æ³•çš„è¯„ä¼°")
    
    def generate_comparison_report(self):
        """ç”Ÿæˆå¯¹æ¯”åˆ†ææŠ¥å‘Š"""
        logger.info("ç”Ÿæˆå¯¹æ¯”åˆ†ææŠ¥å‘Š")
        
        if not self.evaluation_results:
            logger.warning("æ²¡æœ‰è¯„ä¼°ç»“æœï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š")
            return
        
        print(f"\n{'='*80}")
        print("                    Q2/Q3/Q4æ··åˆæ¯”ä¾‹é¢„æµ‹æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š")
        print(f"{'='*80}")
        
        # æ±‡æ€»è¡¨æ ¼
        print(f"\nğŸ“Š æ€»ä½“æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”:")
        print(f"{'æ–¹æ³•':<8} {'æ ·æœ¬æ•°':<8} {'è¦†ç›–ç‡':<8} {'MSE':<12} {'RMSE':<12} {'MAE':<12} {'NoCå‡†ç¡®ç‡':<10}")
        print("-" * 80)
        
        for method_name, metrics in self.evaluation_results.items():
            print(f"{method_name:<8} "
                  f"{metrics['matched_samples']:<8} "
                  f"{metrics['coverage_rate']:<8.3f} "
                  f"{metrics['overall_mse']:<12.6f} "
                  f"{metrics['overall_rmse']:<12.6f} "
                  f"{metrics['overall_mae']:<12.6f} "
                  f"{metrics['noc_accuracy']:<10.3f}")
        
        # æ€§èƒ½ç­‰çº§åˆ†å¸ƒ
        print(f"\nğŸ“ˆ æ€§èƒ½ç­‰çº§åˆ†å¸ƒ:")
        print(f"{'æ–¹æ³•':<8} {'ä¼˜ç§€(<0.01)':<12} {'è‰¯å¥½(0.01-0.05)':<15} {'ä¸€èˆ¬(0.05-0.1)':<15} {'è¾ƒå·®(>0.1)':<12}")
        print("-" * 70)
        
        for method_name, metrics in self.evaluation_results.items():
            print(f"{method_name:<8} "
                  f"{metrics['excellent_rate']:<12.3f} "
                  f"{metrics['good_rate']:<15.3f} "
                  f"{metrics['fair_rate']:<15.3f} "
                  f"{metrics['poor_rate']:<12.3f}")
        
        # æœ€ä½³æ–¹æ³•æ¨è
        print(f"\nğŸ† æœ€ä½³æ–¹æ³•æ¨è:")
        
        best_mse_method = min(self.evaluation_results.keys(), 
                             key=lambda x: self.evaluation_results[x]['overall_mse'])
        best_noc_method = max(self.evaluation_results.keys(), 
                             key=lambda x: self.evaluation_results[x]['noc_accuracy'])
        best_coverage_method = max(self.evaluation_results.keys(), 
                                  key=lambda x: self.evaluation_results[x]['coverage_rate'])
        
        print(f"  â€¢ æœ€ä½MSE: {best_mse_method} (MSE={self.evaluation_results[best_mse_method]['overall_mse']:.6f})")
        print(f"  â€¢ æœ€é«˜NoCå‡†ç¡®ç‡: {best_noc_method} (å‡†ç¡®ç‡={self.evaluation_results[best_noc_method]['noc_accuracy']:.3f})")
        print(f"  â€¢ æœ€é«˜è¦†ç›–ç‡: {best_coverage_method} (è¦†ç›–ç‡={self.evaluation_results[best_coverage_method]['coverage_rate']:.3f})")
        
        # è¯¦ç»†æ ·æœ¬åˆ†æ
        print(f"\nğŸ“‹ è¯¦ç»†æ ·æœ¬åˆ†æ (å‰5ä¸ªæ ·æœ¬):")
        
        # æ‰¾åˆ°æ‰€æœ‰æ–¹æ³•éƒ½æœ‰ç»“æœçš„æ ·æœ¬
        common_samples = set(self.true_ratios.keys())
        for method_results in [self.q2_results, self.q3_results, self.q4_results]:
            if method_results:
                common_samples &= set(method_results.keys())
        
        common_samples = list(common_samples)[:5]  # åªæ˜¾ç¤ºå‰5ä¸ª
        
        for sample_file in common_samples:
            print(f"\næ ·æœ¬: {sample_file}")
            true_data = self.true_ratios[sample_file]
            print(f"  çœŸå®æ¯”ä¾‹: {true_data['ratios']} (NoC={true_data['noc']})")
            
            for method_name, metrics in self.evaluation_results.items():
                if sample_file in metrics['sample_wise_metrics']:
                    sample_metrics = metrics['sample_wise_metrics'][sample_file]
                    print(f"  {method_name}: {sample_metrics['predicted_ratios']} "
                          f"(MSE={sample_metrics['mse']:.6f}, NoC={sample_metrics['predicted_noc']})")
    
    def plot_comparison_charts(self):
        """ç»˜åˆ¶å¯¹æ¯”åˆ†æå›¾è¡¨"""
        logger.info("ç»˜åˆ¶å¯¹æ¯”åˆ†æå›¾è¡¨")
        
        if not self.evaluation_results:
            logger.warning("æ²¡æœ‰è¯„ä¼°ç»“æœï¼Œæ— æ³•ç»˜åˆ¶å›¾è¡¨")
            return
        
        # è®¾ç½®å›¾è¡¨æ ·å¼
        plt.style.use('default')
        sns.set_palette("husl")
        
        # 1. æ€»ä½“MSEå¯¹æ¯”
        plt.figure(figsize=(12, 8))
        
        methods = list(self.evaluation_results.keys())
        mse_values = [self.evaluation_results[m]['overall_mse'] for m in methods]
        rmse_values = [self.evaluation_results[m]['overall_rmse'] for m in methods]
        mae_values = [self.evaluation_results[m]['overall_mae'] for m in methods]
        
        x = np.arange(len(methods))
        width = 0.25
        
        plt.bar(x - width, mse_values, width, label='MSE', alpha=0.8)
        plt.bar(x, rmse_values, width, label='RMSE', alpha=0.8)
        plt.bar(x + width, mae_values, width, label='MAE', alpha=0.8)
        
        plt.xlabel('æ–¹æ³•', fontsize=12)
        plt.ylabel('è¯¯å·®å€¼', fontsize=12)
        plt.title('Q2/Q3/Q4æ··åˆæ¯”ä¾‹é¢„æµ‹è¯¯å·®å¯¹æ¯”', fontsize=14, fontweight='bold')
        plt.xticks(x, methods)
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (mse, rmse, mae) in enumerate(zip(mse_values, rmse_values, mae_values)):
            plt.text(i - width, mse + max(mse_values) * 0.01, f'{mse:.4f}', 
                    ha='center', va='bottom', fontsize=10)
            plt.text(i, rmse + max(rmse_values) * 0.01, f'{rmse:.4f}', 
                    ha='center', va='bottom', fontsize=10)
            plt.text(i + width, mae + max(mae_values) * 0.01, f'{mae:.4f}', 
                    ha='center', va='bottom', fontsize=10)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'overall_error_comparison.png'), 
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. æ€§èƒ½ç­‰çº§åˆ†å¸ƒå †å æ¡å½¢å›¾
        plt.figure(figsize=(10, 6))
        
        performance_data = []
        for method in methods:
            metrics = self.evaluation_results[method]
            performance_data.append([
                metrics['excellent_rate'],
                metrics['good_rate'], 
                metrics['fair_rate'],
                metrics['poor_rate']
            ])
        
        performance_data = np.array(performance_data).T
        
        labels = ['ä¼˜ç§€ (MSE<0.01)', 'è‰¯å¥½ (0.01â‰¤MSE<0.05)', 'ä¸€èˆ¬ (0.05â‰¤MSE<0.1)', 'è¾ƒå·® (MSEâ‰¥0.1)']
        colors = ['#2ecc71', '#f39c12', '#e74c3c', '#95a5a6']
        
        bottom = np.zeros(len(methods))
        for i, (data, label, color) in enumerate(zip(performance_data, labels, colors)):
            plt.bar(methods, data, bottom=bottom, label=label, color=color, alpha=0.8)
            
            # æ·»åŠ ç™¾åˆ†æ¯”æ ‡ç­¾
            for j, value in enumerate(data):
                if value > 0.05:  # åªæ˜¾ç¤ºå¤§äº5%çš„æ ‡ç­¾
                    plt.text(j, bottom[j] + value/2, f'{value:.1%}', 
                            ha='center', va='center', fontweight='bold', fontsize=10)
            
            bottom += data
        
        plt.xlabel('æ–¹æ³•', fontsize=12)
        plt.ylabel('æ ·æœ¬æ¯”ä¾‹', fontsize=12)
        plt.title('å„æ–¹æ³•æ€§èƒ½ç­‰çº§åˆ†å¸ƒ', fontsize=14, fontweight='bold')
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.grid(True, alpha=0.3, axis='y')
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'performance_distribution.png'), 
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        # 3. NoCå‡†ç¡®ç‡å¯¹æ¯”
        plt.figure(figsize=(8, 6))
        
        noc_accuracies = [self.evaluation_results[m]['noc_accuracy'] for m in methods]
        coverage_rates = [self.evaluation_results[m]['coverage_rate'] for m in methods]
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # NoCå‡†ç¡®ç‡
        bars1 = ax1.bar(methods, noc_accuracies, color=['#3498db', '#e74c3c', '#2ecc71'], alpha=0.8)
        ax1.set_ylabel('NoCå‡†ç¡®ç‡', fontsize=12)
        ax1.set_title('è´¡çŒ®è€…äººæ•°é¢„æµ‹å‡†ç¡®ç‡', fontsize=12, fontweight='bold')
        ax1.set_ylim(0, 1)
        ax1.grid(True, alpha=0.3)
        
        for bar, acc in zip(bars1, noc_accuracies):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
                    f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')
        
        # è¦†ç›–ç‡
        bars2 = ax2.bar(methods, coverage_rates, color=['#9b59b6', '#f39c12', '#1abc9c'], alpha=0.8)
        ax2.set_ylabel('è¦†ç›–ç‡', fontsize=12)
        ax2.set_title('æ ·æœ¬è¦†ç›–ç‡', fontsize=12, fontweight='bold')
        ax2.set_ylim(0, 1)
        ax2.grid(True, alpha=0.3)
        
        for bar, cov in zip(bars2, coverage_rates):
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
                    f'{cov:.3f}', ha='center', va='bottom', fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'accuracy_coverage_comparison.png'), 
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        # 4. æ ·æœ¬çº§MSEåˆ†å¸ƒç®±çº¿å›¾
        plt.figure(figsize=(10, 6))
        
        mse_data = []
        labels = []
        
        for method in methods:
            metrics = self.evaluation_results[method]
            sample_mses = [m['mse'] for m in metrics['sample_wise_metrics'].values()]
            mse_data.append(sample_mses)
            labels.append(f"{method}\n(n={len(sample_mses)})")
        
        box_plot = plt.boxplot(mse_data, labels=labels, patch_artist=True)
        
        colors = ['#3498db', '#e74c3c', '#2ecc71']
        for patch, color in zip(box_plot['boxes'], colors):
            patch.set_facecolor(color)
            patch.set_alpha(0.7)
        
        plt.ylabel('æ ·æœ¬çº§MSE', fontsize=12)
        plt.title('å„æ–¹æ³•æ ·æœ¬çº§MSEåˆ†å¸ƒ', fontsize=14, fontweight='bold')
        plt.yscale('log')  # ä½¿ç”¨å¯¹æ•°åˆ»åº¦
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'sample_mse_distribution.png'), 
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        # 5. æ•£ç‚¹å›¾ï¼šçœŸå®æ¯”ä¾‹ vs é¢„æµ‹æ¯”ä¾‹
        fig, axes = plt.subplots(1, len(methods), figsize=(5*len(methods), 5))
        if len(methods) == 1:
            axes = [axes]
        
        for i, method in enumerate(methods):
            metrics = self.evaluation_results[method]
            
            all_true = []
            all_pred = []
            
            for sample_metrics in metrics['sample_wise_metrics'].values():
                all_true.extend(sample_metrics['true_ratios'])
                all_pred.extend(sample_metrics['predicted_ratios'])
            
            axes[i].scatter(all_true, all_pred, alpha=0.6, s=30)
            
            # æ·»åŠ å®Œç¾é¢„æµ‹çº¿
            min_val = min(min(all_true), min(all_pred))
            max_val = max(max(all_true), max(all_pred))
            axes[i].plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8, linewidth=2)
            
            axes[i].set_xlabel('çœŸå®æ··åˆæ¯”ä¾‹', fontsize=11)
            axes[i].set_ylabel('é¢„æµ‹æ··åˆæ¯”ä¾‹', fontsize=11)
            axes[i].set_title(f'{method}\nMSE={metrics["overall_mse"]:.4f}', fontsize=12)
            axes[i].grid(True, alpha=0.3)
            axes[i].set_aspect('equal')
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'true_vs_predicted_scatter.png'), 
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"å›¾è¡¨å·²ä¿å­˜åˆ°: {self.output_dir}")
    
    def save_detailed_results(self):
        """ä¿å­˜è¯¦ç»†è¯„ä¼°ç»“æœ"""
        logger.info("ä¿å­˜è¯¦ç»†è¯„ä¼°ç»“æœ")
        
        # ä¿å­˜æ€»ä½“è¯„ä¼°ç»“æœ
        output_file = os.path.join(self.output_dir, 'mse_evaluation_results.json')
        
        # åˆ›å»ºå¯åºåˆ—åŒ–çš„ç»“æœ
        serializable_results = {}
        
        for method_name, metrics in self.evaluation_results.items():
            serializable_metrics = metrics.copy()
            
            # è½¬æ¢sample_wise_metricsä¸ºå¯åºåˆ—åŒ–æ ¼å¼
            serializable_sample_metrics = {}
            for sample_file, sample_metrics in metrics['sample_wise_metrics'].items():
                serializable_sample_metrics[sample_file] = {
                    k: (v.tolist() if isinstance(v, np.ndarray) else v)
                    for k, v in sample_metrics.items()
                }
            
            serializable_metrics['sample_wise_metrics'] = serializable_sample_metrics
            serializable_results[method_name] = serializable_metrics
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜CSVæ ¼å¼çš„æ±‡æ€»è¡¨
        summary_data = []
        for method_name, metrics in self.evaluation_results.items():
            summary_data.append({
                'Method': method_name,
                'Matched_Samples': metrics['matched_samples'],
                'Coverage_Rate': metrics['coverage_rate'],
                'Overall_MSE': metrics['overall_mse'],
                'Overall_RMSE': metrics['overall_rmse'],
                'Overall_MAE': metrics['overall_mae'],
                'NoC_Accuracy': metrics['noc_accuracy'],
                'Excellent_Rate': metrics['excellent_rate'],
                'Good_Rate': metrics['good_rate'],
                'Fair_Rate': metrics['fair_rate'],
                'Poor_Rate': metrics['poor_rate']
            })
        
        summary_df = pd.DataFrame(summary_data)
        summary_csv = os.path.join(self.output_dir, 'mse_summary.csv')
        summary_df.to_csv(summary_csv, index=False, encoding='utf-8-sig')
        
        # ä¿å­˜æ ·æœ¬çº§è¯¦ç»†ç»“æœ
        sample_details = []
        for method_name, metrics in self.evaluation_results.items():
            for sample_file, sample_metrics in metrics['sample_wise_metrics'].items():
                true_data = self.true_ratios[sample_file]
                
                sample_details.append({
                    'Method': method_name,
                    'Sample_File': sample_file,
                    'True_NoC': true_data['noc'],
                    'Predicted_NoC': sample_metrics['predicted_noc'],
                    'NoC_Correct': sample_metrics['noc_correct'],
                    'True_Ratios': str(sample_metrics['true_ratios']),
                    'Predicted_Ratios': str(sample_metrics['predicted_ratios']),
                    'MSE': sample_metrics['mse'],
                    'MAE': sample_metrics['mae'],
                    'RMSE': sample_metrics['rmse'],
                    'Max_Error': sample_metrics['max_error']
                })
        
        details_df = pd.DataFrame(sample_details)
        details_csv = os.path.join(self.output_dir, 'sample_level_results.csv')
        details_df.to_csv(details_csv, index=False, encoding='utf-8-sig')
        
        logger.info(f"è¯¦ç»†ç»“æœå·²ä¿å­˜:")
        logger.info(f"  - JSONæ ¼å¼: {output_file}")
        logger.info(f"  - æ±‡æ€»CSV: {summary_csv}")
        logger.info(f"  - è¯¦ç»†CSV: {details_csv}")
    
    def run_full_evaluation(self, data_path: str, q2_dir: str = None, q3_dir: str = None, q4_dir: str = None):
        """è¿è¡Œå®Œæ•´çš„MSEè¯„ä¼°æµç¨‹"""
        logger.info("å¼€å§‹å®Œæ•´çš„MSEè¯„ä¼°æµç¨‹")
        
        # 1. åŠ è½½çœŸå®æ··åˆæ¯”ä¾‹
        self.load_true_ratios_from_data(data_path)
        
        if not self.true_ratios:
            logger.error("æ²¡æœ‰åŠ è½½åˆ°çœŸå®æ··åˆæ¯”ä¾‹ï¼Œæ— æ³•è¿›è¡Œè¯„ä¼°")
            return
        
        # 2. åŠ è½½å„æ–¹æ³•çš„é¢„æµ‹ç»“æœ
        if q2_dir and os.path.exists(q2_dir):
            self.load_q2_results(q2_dir)
        else:
            logger.warning("Q2ç»“æœç›®å½•ä¸å­˜åœ¨æˆ–æœªæŒ‡å®š")
        
        if q3_dir and os.path.exists(q3_dir):
            self.load_q3_results(q3_dir)
        else:
            logger.warning("Q3ç»“æœç›®å½•ä¸å­˜åœ¨æˆ–æœªæŒ‡å®š")
        
        if q4_dir and os.path.exists(q4_dir):
            self.load_q4_results(q4_dir)
        else:
            logger.warning("Q4ç»“æœç›®å½•ä¸å­˜åœ¨æˆ–æœªæŒ‡å®š")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•æ–¹æ³•çš„ç»“æœ
        if not any([self.q2_results, self.q3_results, self.q4_results]):
            logger.error("æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æ–¹æ³•çš„é¢„æµ‹ç»“æœ")
            return
        
        # 3. è®¡ç®—MSEç­‰è¯„ä¼°æŒ‡æ ‡
        self.calculate_all_metrics()
        
        # 4. ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        self.generate_comparison_report()
        
        # 5. ç»˜åˆ¶å¯¹æ¯”å›¾è¡¨
        self.plot_comparison_charts()
        
        # 6. ä¿å­˜è¯¦ç»†ç»“æœ
        self.save_detailed_results()
        
        logger.info("MSEè¯„ä¼°æµç¨‹å®Œæˆï¼")

def create_demo_data():
    """åˆ›å»ºæ¼”ç¤ºæ•°æ®"""
    print("\nåˆ›å»ºæ¼”ç¤ºæ•°æ®ç”¨äºæµ‹è¯•...")
    
    # åˆ›å»ºæ¨¡æ‹Ÿçš„çœŸå®æ•°æ®
    demo_samples = [
        "A02_RD14-0003-40_41-1;4-M3S30-0.075IP-Q4.0_001.5sec.fsa",
        "A02_RD14-0004-42_43-2;3-M3S30-0.075IP-Q4.0_002.5sec.fsa", 
        "A02_RD14-0005-44_45_46-1;2;1-M3S30-0.075IP-Q4.0_003.5sec.fsa"
    ]
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®æ–‡ä»¶
    demo_data = []
    markers = ['D3S1358', 'vWA', 'FGA']
    
    for sample in demo_samples:
        for marker in markers:
            demo_data.append({
                'Sample File': sample,
                'Marker': marker,
                'Allele 1': '14',
                'Size 1': 150.0,
                'Height 1': 1000.0
            })
    
    demo_df = pd.DataFrame(demo_data)
    demo_path = './demo_mixture_data.csv'
    demo_df.to_csv(demo_path, index=False, encoding='utf-8-sig')
    
    # åˆ›å»ºæ¨¡æ‹Ÿçš„Q2ç»“æœ
    os.makedirs('./demo_q2_results', exist_ok=True)
    for i, sample in enumerate(demo_samples):
        q2_result = {
            'sample_file': sample,
            'predicted_noc': 2 if i < 2 else 3,
            'noc_confidence': 0.9,
            'posterior_summary': {}
        }
        
        # æ·»åŠ æ··åˆæ¯”ä¾‹
        if i == 0:  # ç¬¬ä¸€ä¸ªæ ·æœ¬ 1:4 æ¯”ä¾‹
            q2_result['posterior_summary'] = {
                'Mx_1': {'mean': 0.22},
                'Mx_2': {'mean': 0.78}
            }
        elif i == 1:  # ç¬¬äºŒä¸ªæ ·æœ¬ 2:3 æ¯”ä¾‹
            q2_result['posterior_summary'] = {
                'Mx_1': {'mean': 0.42},
                'Mx_2': {'mean': 0.58}
            }
        else:  # ç¬¬ä¸‰ä¸ªæ ·æœ¬ 1:2:1 æ¯”ä¾‹
            q2_result['posterior_summary'] = {
                'Mx_1': {'mean': 0.28},
                'Mx_2': {'mean': 0.48},
                'Mx_3': {'mean': 0.24}
            }
        
        result_file = f'./demo_q2_results/{sample}_result.json'
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(q2_result, f, ensure_ascii=False, indent=2)
    
    print(f"æ¼”ç¤ºæ•°æ®å·²åˆ›å»º:")
    print(f"  - æ•°æ®æ–‡ä»¶: {demo_path}")
    print(f"  - Q2ç»“æœç›®å½•: ./demo_q2_results")
    print(f"  - åŒ…å« {len(demo_samples)} ä¸ªæ··åˆæ ·æœ¬")
    
    return demo_path, './demo_q2_results'

def main():
    """ä¸»å‡½æ•°"""
    print("æ¬¢è¿ä½¿ç”¨Q2/Q3/Q4æ··åˆæ¯”ä¾‹é¢„æµ‹MSEè®¡ç®—å™¨ï¼")
    print("æ­¤å·¥å…·å°†è®¡ç®—å¹¶å¯¹æ¯”å„æ–¹æ³•çš„é¢„æµ‹ç²¾åº¦")
    
    # æ£€æŸ¥å¿…è¦çš„æ•°æ®æ–‡ä»¶
    data_files = [
        "é™„ä»¶1ï¼šä¸åŒäººæ•°çš„STRå›¾è°±æ•°æ®.csv",
        "é™„ä»¶2ï¼šæ··åˆSTRå›¾è°±æ•°æ®.csv"
    ]
    
    data_path = None
    for file_path in data_files:
        if os.path.exists(file_path):
            data_path = file_path
            break
    
    # æ£€æŸ¥ç»“æœç›®å½•
    result_dirs = {
        'Q2': './q2_mgm_rf_results',
        'Q3': './q3_enhanced_results', 
        'Q4': './q4_upg_denoising_results'
    }
    
    # å¦‚æœæ²¡æœ‰çœŸå®æ•°æ®ï¼Œæä¾›æ¼”ç¤ºé€‰é¡¹
    if not data_path:
        print("\næ²¡æœ‰æ‰¾åˆ°å¿…è¦çš„æ•°æ®æ–‡ä»¶ã€‚")
        print("å¯ç”¨é€‰é¡¹:")
        print("1. åˆ›å»ºæ¼”ç¤ºæ•°æ®è¿›è¡Œæµ‹è¯•")
        print("2. é€€å‡ºç¨‹åº")
        
        choice = input("è¯·é€‰æ‹© (1/2): ").strip()
        if choice == "1":
            data_path, demo_q2_dir = create_demo_data()
            result_dirs['Q2'] = demo_q2_dir
        else:
            print("ç¨‹åºé€€å‡º")
            return
    
    try:
        # åˆå§‹åŒ–MSEè®¡ç®—å™¨
        calculator = MixtureRatioMSECalculator()
        
        # è¿è¡Œå®Œæ•´è¯„ä¼°
        calculator.run_full_evaluation(
            data_path=data_path,
            q2_dir=result_dirs['Q2'] if os.path.exists(result_dirs['Q2']) else None,
            q3_dir=result_dirs['Q3'] if os.path.exists(result_dirs['Q3']) else None,
            q4_dir=result_dirs['Q4'] if os.path.exists(result_dirs['Q4']) else None
        )
        
        print(f"\n{'='*80}")
        print("MSEè¯„ä¼°å®Œæˆï¼")
        print(f"ç»“æœå·²ä¿å­˜åˆ°: {calculator.output_dir}")
        print("ç”Ÿæˆçš„æ–‡ä»¶åŒ…æ‹¬:")
        print("  - mse_evaluation_results.json (è¯¦ç»†JSONç»“æœ)")
        print("  - mse_summary.csv (æ±‡æ€»è¡¨æ ¼)")  
        print("  - sample_level_results.csv (æ ·æœ¬çº§è¯¦ç»†ç»“æœ)")
        print("  - *.png (å¯¹æ¯”å›¾è¡¨)")
        print(f"{'='*80}")
        
    except KeyboardInterrupt:
        print("\n\nç”¨æˆ·ä¸­æ–­ç¨‹åºæ‰§è¡Œ")
    except Exception as e:
        print(f"\nç¨‹åºæ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()