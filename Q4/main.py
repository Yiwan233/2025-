# -*- coding: utf-8 -*-
"""
STRæ•°æ®ç»¼åˆåˆ†æç³»ç»Ÿ
æ•´åˆé—®é¢˜1(NoCè¯†åˆ«)ã€é—®é¢˜2(MCMCæ··åˆæ¯”ä¾‹)å’Œé—®é¢˜4(å³°åˆ†ç±»)çš„ç»“æœåˆ†æ

ä½œè€…: æ•°å­¦å»ºæ¨¡å›¢é˜Ÿ
æ—¥æœŸ: 2025-06-01
ç‰ˆæœ¬: V1.0
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import json
import warnings
from scipy import stats
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.preprocessing import StandardScaler
import os

# é…ç½®è®¾ç½®
warnings.filterwarnings('ignore')
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# åˆ›å»ºè¾“å‡ºç›®å½•
OUTPUT_DIR = './analysis_results'
PLOTS_DIR = os.path.join(OUTPUT_DIR, 'plots')
REPORTS_DIR = os.path.join(OUTPUT_DIR, 'reports')

for directory in [OUTPUT_DIR, PLOTS_DIR, REPORTS_DIR]:
    if not os.path.exists(directory):
        os.makedirs(directory)
        print(f"åˆ›å»ºç›®å½•: {directory}")

print("=== STRæ•°æ®ç»¼åˆåˆ†æç³»ç»Ÿ ===")
print("æ•´åˆåˆ†æé—®é¢˜1(NoCè¯†åˆ«)ã€é—®é¢˜2(MCMCæ··åˆæ¯”ä¾‹)å’Œç›¸å…³æ•°æ®")

# ==================== ç¬¬ä¸€éƒ¨åˆ†ï¼šæ•°æ®åŠ è½½ ====================
print("\n--- ç¬¬ä¸€éƒ¨åˆ†ï¼šæ•°æ®åŠ è½½ ---")

class STRDataLoader:
    """STRæ•°æ®åŠ è½½å™¨"""
    
    def __init__(self):
        self.p1_data = None  # é—®é¢˜1ç‰¹å¾æ•°æ®
        self.p2_data = None  # é—®é¢˜2 MCMCç»“æœ
        self.combined_data = {}
    
    def load_problem1_data(self, filepath='prob1_features_enhanced.csv'):
        """åŠ è½½é—®é¢˜1çš„ç‰¹å¾æ•°æ®"""
        try:
            self.p1_data = pd.read_csv(filepath, encoding='utf-8')
            print(f"âœ“ æˆåŠŸåŠ è½½é—®é¢˜1æ•°æ®: {filepath}")
            print(f"  - æ ·æœ¬æ•°: {len(self.p1_data)}")
            print(f"  - ç‰¹å¾æ•°: {len(self.p1_data.columns) - 2}")  # æ’é™¤Sample Fileå’ŒNoC_True
            
            # æ£€æŸ¥æ•°æ®è´¨é‡
            missing_ratio = self.p1_data.isnull().sum().sum() / (len(self.p1_data) * len(self.p1_data.columns))
            print(f"  - ç¼ºå¤±å€¼æ¯”ä¾‹: {missing_ratio:.2%}")
            
            return True
        except FileNotFoundError:
            print(f"âœ— æ–‡ä»¶æœªæ‰¾åˆ°: {filepath}")
            return False
        except Exception as e:
            print(f"âœ— åŠ è½½é—®é¢˜1æ•°æ®æ—¶å‡ºé”™: {e}")
            return False
    
    def load_problem2_data(self, filepath='problem2_mcmc_results.json'):
        """åŠ è½½é—®é¢˜2çš„MCMCç»“æœ"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                self.p2_data = json.load(f)
            print(f"âœ“ æˆåŠŸåŠ è½½é—®é¢˜2æ•°æ®: {filepath}")
            
            # æå–å…³é”®ä¿¡æ¯
            if 'posterior_summary' in self.p2_data:
                mx1_mean = self.p2_data['posterior_summary']['Mx_1']['mean']
                mx2_mean = self.p2_data['posterior_summary']['Mx_2']['mean']
                print(f"  - Mx_1å¹³å‡å€¼: {mx1_mean:.4f}")
                print(f"  - Mx_2å¹³å‡å€¼: {mx2_mean:.4f}")
                print(f"  - æ”¶æ•›çŠ¶æ€: {self.p2_data.get('convergence_diagnostics', {}).get('convergence_status', 'Unknown')}")
            
            return True
        except FileNotFoundError:
            print(f"âœ— æ–‡ä»¶æœªæ‰¾åˆ°: {filepath}")
            return False
        except Exception as e:
            print(f"âœ— åŠ è½½é—®é¢˜2æ•°æ®æ—¶å‡ºé”™: {e}")
            return False
    
    def get_noc_distribution(self):
        """è·å–NoCåˆ†å¸ƒ"""
        if self.p1_data is not None:
            return self.p1_data['NoC_True'].value_counts().sort_index()
        return None
    
    def get_performance_metrics(self):
        """è·å–æ¨¡å‹æ€§èƒ½æŒ‡æ ‡"""
        if self.p1_data is not None and 'baseline_pred' in self.p1_data.columns:
            accuracy = accuracy_score(self.p1_data['NoC_True'], self.p1_data['baseline_pred'])
            return {
                'accuracy': accuracy,
                'total_samples': len(self.p1_data),
                'correct_predictions': (self.p1_data['NoC_True'] == self.p1_data['baseline_pred']).sum()
            }
        return None

# ==================== ç¬¬äºŒéƒ¨åˆ†ï¼šé—®é¢˜1æ•°æ®åˆ†æ ====================
print("\n--- ç¬¬äºŒéƒ¨åˆ†ï¼šé—®é¢˜1æ•°æ®åˆ†æ (NoCè¯†åˆ«) ---")

class Problem1Analyzer:
    """é—®é¢˜1åˆ†æå™¨"""
    
    def __init__(self, data):
        self.data = data
        self.feature_cols = [col for col in data.columns 
                           if col not in ['Sample File', 'NoC_True', 'baseline_pred', 'é¢„æµ‹æ­£ç¡®', 'corrected_pred']]
    
    def analyze_noc_distribution(self):
        """åˆ†æNoCåˆ†å¸ƒ"""
        print("\nNoCåˆ†å¸ƒåˆ†æ:")
        noc_dist = self.data['NoC_True'].value_counts().sort_index()
        for noc, count in noc_dist.items():
            percentage = count / len(self.data) * 100
            print(f"  {noc}äºº: {count}ä¸ªæ ·æœ¬ ({percentage:.1f}%)")
        
        # å¯è§†åŒ–NoCåˆ†å¸ƒ
        plt.figure(figsize=(10, 6))
        noc_dist.plot(kind='bar', color='steelblue', alpha=0.8)
        plt.title('STRæ ·æœ¬ä¸­è´¡çŒ®è€…äººæ•°(NoC)åˆ†å¸ƒ')
        plt.xlabel('è´¡çŒ®è€…äººæ•°')
        plt.ylabel('æ ·æœ¬æ•°é‡')
        plt.xticks(rotation=0)
        plt.grid(axis='y', alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, v in enumerate(noc_dist.values):
            plt.text(i, v + 0.5, str(v), ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig(os.path.join(PLOTS_DIR, 'noc_distribution.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        return noc_dist
    
    def analyze_model_performance(self):
        """åˆ†ææ¨¡å‹æ€§èƒ½"""
        if 'baseline_pred' not in self.data.columns:
            print("è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°é¢„æµ‹ç»“æœåˆ—")
            return None
        
        print("\næ¨¡å‹æ€§èƒ½åˆ†æ:")
        
        # æ•´ä½“å‡†ç¡®ç‡
        overall_accuracy = accuracy_score(self.data['NoC_True'], self.data['baseline_pred'])
        print(f"æ•´ä½“å‡†ç¡®ç‡: {overall_accuracy:.4f}")
        
        # å„NoCç±»åˆ«å‡†ç¡®ç‡
        print("\nå„NoCç±»åˆ«å‡†ç¡®ç‡:")
        for noc in sorted(self.data['NoC_True'].unique()):
            mask = self.data['NoC_True'] == noc
            if mask.sum() > 0:
                noc_accuracy = (self.data.loc[mask, 'NoC_True'] == 
                              self.data.loc[mask, 'baseline_pred']).mean()
                print(f"  {noc}äºº: {noc_accuracy:.4f} ({mask.sum()}ä¸ªæ ·æœ¬)")
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(self.data['NoC_True'], self.data['baseline_pred'])
        
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=sorted(self.data['NoC_True'].unique()),
                   yticklabels=sorted(self.data['NoC_True'].unique()))
        plt.title('NoCé¢„æµ‹æ··æ·†çŸ©é˜µ')
        plt.xlabel('é¢„æµ‹NoC')
        plt.ylabel('çœŸå®NoC')
        plt.tight_layout()
        plt.savefig(os.path.join(PLOTS_DIR, 'confusion_matrix.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        # åˆ†ç±»æŠ¥å‘Š
        class_names = [f"{i}äºº" for i in sorted(self.data['NoC_True'].unique())]
        report = classification_report(self.data['NoC_True'], self.data['baseline_pred'],
                                     target_names=class_names, output_dict=True)
        
        return {
            'overall_accuracy': overall_accuracy,
            'confusion_matrix': cm,
            'classification_report': report
        }
    
    def analyze_feature_importance(self):
        """åˆ†æç‰¹å¾é‡è¦æ€§ï¼ˆåŸºäºç›¸å…³æ€§ï¼‰"""
        print("\nç‰¹å¾é‡è¦æ€§åˆ†æ:")
        
        # è®¡ç®—ä¸NoCçš„ç›¸å…³æ€§
        correlations = []
        for feature in self.feature_cols:
            if self.data[feature].dtype in ['int64', 'float64']:
                try:
                    corr = abs(self.data[feature].corr(self.data['NoC_True']))
                    if not np.isnan(corr):
                        correlations.append((feature, corr))
                except:
                    continue
        
        # æ’åºå¹¶æ˜¾ç¤º
        correlations.sort(key=lambda x: x[1], reverse=True)
        
        print("ä¸NoCç›¸å…³æ€§æœ€é«˜çš„å‰10ä¸ªç‰¹å¾:")
        for i, (feature, corr) in enumerate(correlations[:10]):
            print(f"  {i+1:2d}. {feature}: {corr:.4f}")
        
        # å¯è§†åŒ–å‰15ä¸ªé‡è¦ç‰¹å¾
        if len(correlations) >= 15:
            top_features = correlations[:15]
            features, corr_values = zip(*top_features)
            
            plt.figure(figsize=(12, 8))
            y_pos = np.arange(len(features))
            plt.barh(y_pos, corr_values, color='lightcoral', alpha=0.8)
            plt.yticks(y_pos, features)
            plt.xlabel('ä¸NoCçš„ç»å¯¹ç›¸å…³ç³»æ•°')
            plt.title('ç‰¹å¾é‡è¦æ€§æ’å (å‰15ä½)')
            plt.grid(axis='x', alpha=0.3)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for i, v in enumerate(corr_values):
                plt.text(v + 0.01, i, f'{v:.3f}', va='center')
            
            plt.tight_layout()
            plt.savefig(os.path.join(PLOTS_DIR, 'feature_importance.png'), dpi=300, bbox_inches='tight')
            plt.close()
        
        return correlations
    
    def analyze_feature_distributions(self):
        """åˆ†æä¸åŒNoCä¸‹çš„ç‰¹å¾åˆ†å¸ƒ"""
        print("\nç‰¹å¾åˆ†å¸ƒåˆ†æ:")
        
        # é€‰æ‹©å‡ ä¸ªé‡è¦ç‰¹å¾è¿›è¡Œåˆ†å¸ƒåˆ†æ
        important_features = ['mac_profile', 'avg_alleles_per_locus', 'avg_peak_height', 
                            'std_peak_height', 'avg_phr', 'num_loci_with_phr']
        
        available_features = [f for f in important_features if f in self.data.columns]
        
        if len(available_features) >= 4:
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            axes = axes.ravel()
            
            for i, feature in enumerate(available_features[:4]):
                ax = axes[i]
                
                # ä¸ºæ¯ä¸ªNoCç»˜åˆ¶åˆ†å¸ƒ
                noc_values = sorted(self.data['NoC_True'].unique())
                for noc in noc_values:
                    data_subset = self.data[self.data['NoC_True'] == noc][feature]
                    if len(data_subset) > 0:
                        ax.hist(data_subset, alpha=0.6, label=f'{noc}äºº', bins=15)
                
                ax.set_xlabel(feature)
                ax.set_ylabel('é¢‘æ¬¡')
                ax.set_title(f'{feature}åœ¨ä¸åŒNoCä¸‹çš„åˆ†å¸ƒ')
                ax.legend()
                ax.grid(alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(os.path.join(PLOTS_DIR, 'feature_distributions.png'), dpi=300, bbox_inches='tight')
            plt.close()

# ==================== ç¬¬ä¸‰éƒ¨åˆ†ï¼šé—®é¢˜2æ•°æ®åˆ†æ ====================
print("\n--- ç¬¬ä¸‰éƒ¨åˆ†ï¼šé—®é¢˜2æ•°æ®åˆ†æ (MCMCæ··åˆæ¯”ä¾‹) ---")

class Problem2Analyzer:
    """é—®é¢˜2åˆ†æå™¨"""
    
    def __init__(self, data):
        self.data = data
    
    def analyze_posterior_summary(self):
        """åˆ†æåéªŒåˆ†å¸ƒæ‘˜è¦"""
        if 'posterior_summary' not in self.data:
            print("è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°posterior_summaryæ•°æ®")
            return None
        
        posterior = self.data['posterior_summary']
        print("\nåéªŒåˆ†å¸ƒåˆ†æ:")
        
        # æ··åˆæ¯”ä¾‹åˆ†æ
        mx1_stats = posterior['Mx_1']
        mx2_stats = posterior['Mx_2']
        
        print(f"Mx_1 (ä¸»è¦è´¡çŒ®è€…):")
        print(f"  å‡å€¼: {mx1_stats['mean']:.4f}")
        print(f"  æ ‡å‡†å·®: {mx1_stats['std']:.4f}")
        print(f"  95%ç½®ä¿¡åŒºé—´: [{mx1_stats['credible_interval_95'][0]:.4f}, {mx1_stats['credible_interval_95'][1]:.4f}]")
        
        print(f"\nMx_2 (æ¬¡è¦è´¡çŒ®è€…):")
        print(f"  å‡å€¼: {mx2_stats['mean']:.4f}")
        print(f"  æ ‡å‡†å·®: {mx2_stats['std']:.4f}")
        print(f"  95%ç½®ä¿¡åŒºé—´: [{mx2_stats['credible_interval_95'][0]:.4f}, {mx2_stats['credible_interval_95'][1]:.4f}]")
        
        # æ¨¡å‹è´¨é‡
        model_quality = posterior['model_quality']
        print(f"\næ¨¡å‹è´¨é‡:")
        print(f"  æ¥å—ç‡: {model_quality['acceptance_rate']:.4f}")
        print(f"  æœ‰æ•ˆæ ·æœ¬æ•°: {model_quality['n_effective_samples']}")
        print(f"  æ”¶æ•›çŠ¶æ€: {model_quality['converged']}")
        
        return {
            'mx1_mean': mx1_stats['mean'],
            'mx2_mean': mx2_stats['mean'],
            'mx1_ci': mx1_stats['credible_interval_95'],
            'mx2_ci': mx2_stats['credible_interval_95'],
            'acceptance_rate': model_quality['acceptance_rate'],
            'converged': model_quality['converged']
        }
    
    def analyze_convergence(self):
        """åˆ†ææ”¶æ•›è¯Šæ–­"""
        if 'convergence_diagnostics' not in self.data:
            print("è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°convergence_diagnosticsæ•°æ®")
            return None
        
        conv = self.data['convergence_diagnostics']
        print("\næ”¶æ•›è¯Šæ–­:")
        
        ess = conv['effective_sample_size']
        print(f"æœ‰æ•ˆæ ·æœ¬å¤§å°:")
        print(f"  Mx_1: {ess[0]:.2f}")
        print(f"  Mx_2: {ess[1]:.2f}")
        print(f"  æœ€å°ESS: {conv['min_ess']:.2f}")
        print(f"æ”¶æ•›çŠ¶æ€: {conv['convergence_status']}")
        
        return conv
    
    def plot_mixture_ratios_trace(self):
        """ç»˜åˆ¶æ··åˆæ¯”ä¾‹çš„è¿¹å›¾"""
        if 'sample_mixture_ratios' not in self.data:
            print("è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°æ ·æœ¬æ•°æ®")
            return
        
        samples = np.array(self.data['sample_mixture_ratios'])
        mx1_samples = samples[:, 0]
        mx2_samples = samples[:, 1]
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # Mx_1è¿¹å›¾
        axes[0, 0].plot(mx1_samples, alpha=0.7, color='blue')
        axes[0, 0].set_title('Mx_1 è¿¹å›¾')
        axes[0, 0].set_xlabel('è¿­ä»£æ¬¡æ•°')
        axes[0, 0].set_ylabel('Mx_1')
        axes[0, 0].grid(alpha=0.3)
        
        # Mx_2è¿¹å›¾
        axes[0, 1].plot(mx2_samples, alpha=0.7, color='red')
        axes[0, 1].set_title('Mx_2 è¿¹å›¾')
        axes[0, 1].set_xlabel('è¿­ä»£æ¬¡æ•°')
        axes[0, 1].set_ylabel('Mx_2')
        axes[0, 1].grid(alpha=0.3)
        
        # Mx_1åˆ†å¸ƒ
        axes[1, 0].hist(mx1_samples, bins=50, alpha=0.7, color='blue', density=True)
        axes[1, 0].axvline(np.mean(mx1_samples), color='darkblue', linestyle='--', 
                          label=f'å‡å€¼: {np.mean(mx1_samples):.3f}')
        axes[1, 0].set_title('Mx_1 åéªŒåˆ†å¸ƒ')
        axes[1, 0].set_xlabel('Mx_1')
        axes[1, 0].set_ylabel('å¯†åº¦')
        axes[1, 0].legend()
        axes[1, 0].grid(alpha=0.3)
        
        # Mx_2åˆ†å¸ƒ
        axes[1, 1].hist(mx2_samples, bins=50, alpha=0.7, color='red', density=True)
        axes[1, 1].axvline(np.mean(mx2_samples), color='darkred', linestyle='--',
                          label=f'å‡å€¼: {np.mean(mx2_samples):.3f}')
        axes[1, 1].set_title('Mx_2 åéªŒåˆ†å¸ƒ')
        axes[1, 1].set_xlabel('Mx_2')
        axes[1, 1].set_ylabel('å¯†åº¦')
        axes[1, 1].legend()
        axes[1, 1].grid(alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(PLOTS_DIR, 'mcmc_trace_plots.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        # ç»˜åˆ¶æ•£ç‚¹å›¾
        plt.figure(figsize=(10, 8))
        plt.scatter(mx1_samples, mx2_samples, alpha=0.5, s=1)
        plt.xlabel('Mx_1')
        plt.ylabel('Mx_2')
        plt.title('æ··åˆæ¯”ä¾‹è”åˆåˆ†å¸ƒ')
        
        # æ·»åŠ çº¦æŸçº¿ Mx_1 + Mx_2 = 1
        x_line = np.linspace(0, 1, 100)
        y_line = 1 - x_line
        plt.plot(x_line, y_line, 'r--', label='Mx_1 + Mx_2 = 1')
        plt.legend()
        plt.grid(alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(PLOTS_DIR, 'mixture_ratios_joint.png'), dpi=300, bbox_inches='tight')
        plt.close()

# ==================== ç¬¬å››éƒ¨åˆ†ï¼šç»¼åˆåˆ†æ ====================
print("\n--- ç¬¬å››éƒ¨åˆ†ï¼šç»¼åˆåˆ†æ ---")

class ComprehensiveAnalyzer:
    """ç»¼åˆåˆ†æå™¨"""
    
    def __init__(self, p1_data, p2_data):
        self.p1_data = p1_data
        self.p2_data = p2_data
    
    def analyze_model_consistency(self):
        """åˆ†ææ¨¡å‹ä¸€è‡´æ€§"""
        print("\næ¨¡å‹ä¸€è‡´æ€§åˆ†æ:")
        
        if self.p1_data is None or self.p2_data is None:
            print("æ•°æ®ä¸å®Œæ•´ï¼Œæ— æ³•è¿›è¡Œä¸€è‡´æ€§åˆ†æ")
            return
        
        # åˆ†æ2äººæ··åˆç‰©çš„é¢„æµ‹ç»“æœ
        two_person_samples = self.p1_data[self.p1_data['NoC_True'] == 2]
        if len(two_person_samples) > 0:
            accuracy_2p = (two_person_samples['NoC_True'] == 
                          two_person_samples['baseline_pred']).mean()
            print(f"é—®é¢˜1ä¸­2äººæ··åˆç‰©é¢„æµ‹å‡†ç¡®ç‡: {accuracy_2p:.4f}")
        
        # é—®é¢˜2çš„æ··åˆæ¯”ä¾‹
        if 'posterior_summary' in self.p2_data:
            mx1_mean = self.p2_data['posterior_summary']['Mx_1']['mean']
            mx2_mean = self.p2_data['posterior_summary']['Mx_2']['mean']
            
            print(f"é—®é¢˜2ä¸­æ··åˆæ¯”ä¾‹ä¼°è®¡:")
            print(f"  ä¸»è¦è´¡çŒ®è€… (Mx_1): {mx1_mean:.4f}")
            print(f"  æ¬¡è¦è´¡çŒ®è€… (Mx_2): {mx2_mean:.4f}")
            print(f"  æ¯”ä¾‹å·®å¼‚: {abs(mx1_mean - mx2_mean):.4f}")
            
            # åˆ¤æ–­æ··åˆç‰©ç±»å‹
            if abs(mx1_mean - mx2_mean) < 0.2:
                mixture_type = "å¹³è¡¡æ··åˆç‰©"
            elif mx1_mean > 0.7:
                mixture_type = "ä¸»è¦è´¡çŒ®è€…å ä¼˜"
            else:
                mixture_type = "ä¸­ç­‰ä¸å¹³è¡¡"
            
            print(f"  æ··åˆç‰©ç±»å‹: {mixture_type}")
    
    def generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        print("\nç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...")
        
        report = {
            "analysis_timestamp": pd.Timestamp.now().strftime("%Y-%m-%d %H:%M:%S"),
            "data_summary": {},
            "problem1_results": {},
            "problem2_results": {},
            "recommendations": []
        }
        
        # é—®é¢˜1ç»“æœæ‘˜è¦
        if self.p1_data is not None:
            if 'baseline_pred' in self.p1_data.columns:
                accuracy = accuracy_score(self.p1_data['NoC_True'], self.p1_data['baseline_pred'])
                report["problem1_results"] = {
                    "overall_accuracy": float(accuracy),
                    "total_samples": len(self.p1_data),
                    "noc_distribution": self.p1_data['NoC_True'].value_counts().to_dict()
                }
            
            report["data_summary"]["problem1_samples"] = len(self.p1_data)
            report["data_summary"]["problem1_features"] = len([col for col in self.p1_data.columns 
                                                              if col not in ['Sample File', 'NoC_True']])
        
        # é—®é¢˜2ç»“æœæ‘˜è¦
        if self.p2_data is not None and 'posterior_summary' in self.p2_data:
            posterior = self.p2_data['posterior_summary']
            report["problem2_results"] = {
                "mx1_mean": posterior['Mx_1']['mean'],
                "mx2_mean": posterior['Mx_2']['mean'],
                "mx1_credible_interval": posterior['Mx_1']['credible_interval_95'],
                "mx2_credible_interval": posterior['Mx_2']['credible_interval_95'],
                "model_converged": posterior['model_quality']['converged'],
                "acceptance_rate": posterior['model_quality']['acceptance_rate']
            }
        
        # ç”Ÿæˆå»ºè®®
        if self.p1_data is not None and 'baseline_pred' in self.p1_data.columns:
            accuracy = accuracy_score(self.p1_data['NoC_True'], self.p1_data['baseline_pred'])
            if accuracy > 0.9:
                report["recommendations"].append("é—®é¢˜1çš„NoCè¯†åˆ«æ¨¡å‹è¡¨ç°ä¼˜ç§€ï¼Œå»ºè®®æŠ•å…¥å®é™…åº”ç”¨")
            elif accuracy > 0.8:
                report["recommendations"].append("é—®é¢˜1çš„NoCè¯†åˆ«æ¨¡å‹è¡¨ç°è‰¯å¥½ï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–ç‰¹å¾å·¥ç¨‹")
            else:
                report["recommendations"].append("é—®é¢˜1çš„NoCè¯†åˆ«æ¨¡å‹éœ€è¦æ”¹è¿›ï¼Œå»ºè®®é‡æ–°å®¡è§†ç‰¹å¾é€‰æ‹©å’Œæ¨¡å‹é€‰æ‹©")
        
        if (self.p2_data is not None and 'posterior_summary' in self.p2_data and 
            self.p2_data['posterior_summary']['model_quality']['converged']):
            report["recommendations"].append("é—®é¢˜2çš„MCMCæ¨¡å‹æ”¶æ•›è‰¯å¥½ï¼Œæ··åˆæ¯”ä¾‹ä¼°è®¡å¯ä¿¡")
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = os.path.join(REPORTS_DIR, 'comprehensive_analysis_report.json')
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=4, ensure_ascii=False)
        
        print(f"âœ“ ç»¼åˆåˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        return report

# ==================== ä¸»æ‰§è¡Œå‡½æ•° ====================
def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    
    # 1. æ•°æ®åŠ è½½
    loader = STRDataLoader()
    
    # åŠ è½½é—®é¢˜1æ•°æ®
    p1_loaded = loader.load_problem1_data('prob1_features_enhanced.csv')
    
    # åŠ è½½é—®é¢˜2æ•°æ®
    p2_loaded = loader.load_problem2_data('problem2_mcmc_results.json')
    
    if not p1_loaded and not p2_loaded:
        print("é”™è¯¯: æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ•°æ®æ–‡ä»¶")
        return
    
    # 2. é—®é¢˜1åˆ†æ
    if p1_loaded:
        print("\n" + "="*50)
        print("å¼€å§‹é—®é¢˜1åˆ†æ...")
        p1_analyzer = Problem1Analyzer(loader.p1_data)
        
        # NoCåˆ†å¸ƒåˆ†æ
        noc_dist = p1_analyzer.analyze_noc_distribution()
        
        # æ¨¡å‹æ€§èƒ½åˆ†æ
        performance = p1_analyzer.analyze_model_performance()
        
        # ç‰¹å¾é‡è¦æ€§åˆ†æ
        feature_importance = p1_analyzer.analyze_feature_importance()
        
        # ç‰¹å¾åˆ†å¸ƒåˆ†æ
        p1_analyzer.analyze_feature_distributions()
    
    # 3. é—®é¢˜2åˆ†æ
    if p2_loaded:
        print("\n" + "="*50)
        print("å¼€å§‹é—®é¢˜2åˆ†æ...")
        p2_analyzer = Problem2Analyzer(loader.p2_data)
        
        # åéªŒåˆ†å¸ƒåˆ†æ
        posterior_analysis = p2_analyzer.analyze_posterior_summary()
        
        # æ”¶æ•›åˆ†æ
        convergence_analysis = p2_analyzer.analyze_convergence()
        
        # ç»˜åˆ¶è¿¹å›¾
        p2_analyzer.plot_mixture_ratios_trace()
    
    # 4. ç»¼åˆåˆ†æ
    if p1_loaded or p2_loaded:
        print("\n" + "="*50)
        print("å¼€å§‹ç»¼åˆåˆ†æ...")
        comp_analyzer = ComprehensiveAnalyzer(loader.p1_data, loader.p2_data)
        
        # æ¨¡å‹ä¸€è‡´æ€§åˆ†æ
        comp_analyzer.analyze_model_consistency()
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        report = comp_analyzer.generate_comprehensive_report()
    
    # 5. ç”Ÿæˆæ€»ç»“
    print("\n" + "="*60)
    print("ğŸ‰ STRæ•°æ®ç»¼åˆåˆ†æå®Œæˆ!")
    print("="*60)
    
    print(f"\nğŸ“Š åˆ†æç»“æœæ€»ç»“:")
    if p1_loaded and loader.p1_data is not None:
        if 'baseline_pred' in loader.p1_data.columns:
            accuracy = accuracy_score(loader.p1_data['NoC_True'], loader.p1_data['baseline_pred'])
            print(f"   é—®é¢˜1 NoCè¯†åˆ«å‡†ç¡®ç‡: {accuracy:.4f}")
        print(f"   é—®é¢˜1 æ ·æœ¬æ€»æ•°: {len(loader.p1_data)}")
    
    if p2_loaded and loader.p2_data is not None:
        if 'posterior_summary' in loader.p2_data:
            mx1_mean = loader.p2_data['posterior_summary']['Mx_1']['mean']
            mx2_mean = loader.p2_data['posterior_summary']['Mx_2']['mean']
            print(f"   é—®é¢˜2 ä¸»è¦è´¡çŒ®è€…æ¯”ä¾‹: {mx1_mean:.4f}")
            print(f"   é—®é¢˜2 æ¬¡è¦è´¡çŒ®è€…æ¯”ä¾‹: {mx2_mean:.4f}")
            converged = loader.p2_data['posterior_summary']['model_quality']['converged']
            print(f"   é—®é¢˜2 MCMCæ”¶æ•›çŠ¶æ€: {'âœ“' if converged else 'âœ—'}")
    
    print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print(f"   å›¾è¡¨ç›®å½•: {PLOTS_DIR}")
    print(f"   æŠ¥å‘Šç›®å½•: {REPORTS_DIR}")
    print(f"   - NoCåˆ†å¸ƒå›¾: noc_distribution.png")
    print(f"   - æ··æ·†çŸ©é˜µ: confusion_matrix.png")
    print(f"   - ç‰¹å¾é‡è¦æ€§: feature_importance.png")
    print(f"   - ç‰¹å¾åˆ†å¸ƒ: feature_distributions.png")
    if p2_loaded:
        print(f"   - MCMCè¿¹å›¾: mcmc_trace_plots.png")
        print(f"   - æ··åˆæ¯”ä¾‹è”åˆåˆ†å¸ƒ: mixture_ratios_joint.png")
    print(f"   - ç»¼åˆåˆ†ææŠ¥å‘Š: comprehensive_analysis_report.json")
    
    print(f"\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
    print(f"   1. æŸ¥çœ‹å›¾è¡¨äº†è§£æ•°æ®åˆ†å¸ƒå’Œæ¨¡å‹æ€§èƒ½")
    print(f"   2. é˜…è¯»JSONæŠ¥å‘Šè·å–è¯¦ç»†åˆ†æç»“æœ")
    print(f"   3. æ ¹æ®ç‰¹å¾é‡è¦æ€§ä¼˜åŒ–æ¨¡å‹")
    if p2_loaded:
        print(f"   4. æ£€æŸ¥MCMCæ”¶æ•›æ€§ç¡®ä¿ç»“æœå¯ä¿¡")

# ==================== ç¬¬äº”éƒ¨åˆ†ï¼šé«˜çº§åˆ†æåŠŸèƒ½ ====================
print("\n--- ç¬¬äº”éƒ¨åˆ†ï¼šé«˜çº§åˆ†æåŠŸèƒ½ ---")

class AdvancedAnalyzer:
    """é«˜çº§åˆ†æå™¨"""
    
    def __init__(self, p1_data, p2_data):
        self.p1_data = p1_data
        self.p2_data = p2_data
    
    def correlation_analysis(self):
        """ç›¸å…³æ€§åˆ†æ"""
        if self.p1_data is None:
            return
        
        print("\né«˜çº§ç›¸å…³æ€§åˆ†æ:")
        
        # é€‰æ‹©æ•°å€¼å‹ç‰¹å¾
        numeric_features = self.p1_data.select_dtypes(include=[np.number]).columns
        numeric_features = [col for col in numeric_features if col != 'NoC_True']
        
        if len(numeric_features) > 10:
            # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
            corr_matrix = self.p1_data[numeric_features[:20]].corr()  # é™åˆ¶å‰20ä¸ªç‰¹å¾
            
            # ç»˜åˆ¶ç›¸å…³æ€§çƒ­å›¾
            plt.figure(figsize=(16, 14))
            mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
            sns.heatmap(corr_matrix, mask=mask, annot=False, cmap='coolwarm', 
                       center=0, square=True, linewidths=0.5)
            plt.title('ç‰¹å¾ç›¸å…³æ€§çƒ­å›¾')
            plt.tight_layout()
            plt.savefig(os.path.join(PLOTS_DIR, 'feature_correlation_heatmap.png'), 
                       dpi=300, bbox_inches='tight')
            plt.close()
            
            # æ‰¾å‡ºé«˜ç›¸å…³æ€§ç‰¹å¾å¯¹
            high_corr_pairs = []
            for i in range(len(corr_matrix.columns)):
                for j in range(i+1, len(corr_matrix.columns)):
                    corr_val = abs(corr_matrix.iloc[i, j])
                    if corr_val > 0.8:  # é«˜ç›¸å…³æ€§é˜ˆå€¼
                        high_corr_pairs.append((
                            corr_matrix.columns[i], 
                            corr_matrix.columns[j], 
                            corr_val
                        ))
            
            if high_corr_pairs:
                print("å‘ç°é«˜ç›¸å…³æ€§ç‰¹å¾å¯¹ (|r| > 0.8):")
                for feat1, feat2, corr in sorted(high_corr_pairs, key=lambda x: x[2], reverse=True)[:10]:
                    print(f"  {feat1} <-> {feat2}: {corr:.3f}")
            else:
                print("æœªå‘ç°é«˜ç›¸å…³æ€§ç‰¹å¾å¯¹")
    
    def outlier_analysis(self):
        """å¼‚å¸¸å€¼åˆ†æ"""
        if self.p1_data is None:
            return
        
        print("\nå¼‚å¸¸å€¼åˆ†æ:")
        
        # é€‰æ‹©å‡ ä¸ªé‡è¦çš„æ•°å€¼ç‰¹å¾
        key_features = ['mac_profile', 'avg_alleles_per_locus', 'avg_peak_height', 'std_peak_height']
        available_features = [f for f in key_features if f in self.p1_data.columns]
        
        if len(available_features) >= 2:
            fig, axes = plt.subplots(1, len(available_features), figsize=(4*len(available_features), 6))
            if len(available_features) == 1:
                axes = [axes]
            
            outlier_summary = {}
            
            for i, feature in enumerate(available_features):
                # ä½¿ç”¨IQRæ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼
                Q1 = self.p1_data[feature].quantile(0.25)
                Q3 = self.p1_data[feature].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                outliers = self.p1_data[(self.p1_data[feature] < lower_bound) | 
                                       (self.p1_data[feature] > upper_bound)]
                
                outlier_summary[feature] = {
                    'count': len(outliers),
                    'percentage': len(outliers) / len(self.p1_data) * 100,
                    'lower_bound': lower_bound,
                    'upper_bound': upper_bound
                }
                
                # ç»˜åˆ¶ç®±çº¿å›¾
                axes[i].boxplot(self.p1_data[feature], vert=True)
                axes[i].set_title(f'{feature}\nå¼‚å¸¸å€¼: {len(outliers)}ä¸ª')
                axes[i].set_ylabel(feature)
                axes[i].grid(alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(os.path.join(PLOTS_DIR, 'outlier_analysis.png'), dpi=300, bbox_inches='tight')
            plt.close()
            
            # è¾“å‡ºå¼‚å¸¸å€¼ç»Ÿè®¡
            for feature, stats in outlier_summary.items():
                print(f"  {feature}: {stats['count']}ä¸ªå¼‚å¸¸å€¼ ({stats['percentage']:.1f}%)")
    
    def model_stability_analysis(self):
        """æ¨¡å‹ç¨³å®šæ€§åˆ†æ"""
        if self.p1_data is None or 'baseline_pred' not in self.p1_data.columns:
            return
        
        print("\næ¨¡å‹ç¨³å®šæ€§åˆ†æ:")
        
        # æŒ‰NoCåˆ†æé¢„æµ‹å‡†ç¡®ç‡çš„ç¨³å®šæ€§
        stability_results = {}
        
        for noc in sorted(self.p1_data['NoC_True'].unique()):
            noc_data = self.p1_data[self.p1_data['NoC_True'] == noc]
            if len(noc_data) > 5:  # è‡³å°‘5ä¸ªæ ·æœ¬æ‰åˆ†æ
                # è®¡ç®—å‡†ç¡®ç‡
                accuracy = (noc_data['NoC_True'] == noc_data['baseline_pred']).mean()
                
                # è®¡ç®—é¢„æµ‹ç½®ä¿¡åº¦ï¼ˆå¦‚æœæœ‰æ¦‚ç‡é¢„æµ‹çš„è¯ï¼‰
                # è¿™é‡Œç®€åŒ–ä¸ºé¢„æµ‹æ­£ç¡®æ€§çš„äºŒé¡¹åˆ†å¸ƒç½®ä¿¡åŒºé—´
                n = len(noc_data)
                p = accuracy
                se = np.sqrt(p * (1 - p) / n)
                ci_lower = max(0, p - 1.96 * se)
                ci_upper = min(1, p + 1.96 * se)
                
                stability_results[noc] = {
                    'accuracy': accuracy,
                    'samples': n,
                    'ci_lower': ci_lower,
                    'ci_upper': ci_upper,
                    'ci_width': ci_upper - ci_lower
                }
                
                print(f"  {noc}äººæ··åˆç‰©: å‡†ç¡®ç‡ {accuracy:.3f} "
                      f"(95% CI: [{ci_lower:.3f}, {ci_upper:.3f}], æ ·æœ¬æ•°: {n})")
        
        # å¯è§†åŒ–ç¨³å®šæ€§
        if len(stability_results) > 1:
            nocs = list(stability_results.keys())
            accuracies = [stability_results[noc]['accuracy'] for noc in nocs]
            ci_lowers = [stability_results[noc]['ci_lower'] for noc in nocs]
            ci_uppers = [stability_results[noc]['ci_upper'] for noc in nocs]
            
            plt.figure(figsize=(10, 6))
            x_pos = np.arange(len(nocs))
            
            # ç»˜åˆ¶å‡†ç¡®ç‡åŠç½®ä¿¡åŒºé—´
            plt.errorbar(x_pos, accuracies, 
                        yerr=[np.array(accuracies) - np.array(ci_lowers),
                              np.array(ci_uppers) - np.array(accuracies)],
                        fmt='o', capsize=5, capthick=2, markersize=8)
            
            plt.xlabel('è´¡çŒ®è€…äººæ•° (NoC)')
            plt.ylabel('é¢„æµ‹å‡†ç¡®ç‡')
            plt.title('æ¨¡å‹åœ¨ä¸åŒNoCä¸‹çš„ç¨³å®šæ€§')
            plt.xticks(x_pos, [f'{noc}äºº' for noc in nocs])
            plt.grid(alpha=0.3)
            plt.ylim(0, 1.1)
            
            # æ·»åŠ æ€»ä½“å‡†ç¡®ç‡çº¿
            overall_acc = (self.p1_data['NoC_True'] == self.p1_data['baseline_pred']).mean()
            plt.axhline(y=overall_acc, color='red', linestyle='--', alpha=0.7,
                       label=f'æ€»ä½“å‡†ç¡®ç‡: {overall_acc:.3f}')
            plt.legend()
            
            plt.tight_layout()
            plt.savefig(os.path.join(PLOTS_DIR, 'model_stability.png'), dpi=300, bbox_inches='tight')
            plt.close()
    
    def mcmc_diagnostics(self):
        """MCMCè¯Šæ–­"""
        if self.p2_data is None or 'sample_mixture_ratios' not in self.p2_data:
            return
        
        print("\nMCMCé«˜çº§è¯Šæ–­:")
        
        samples = np.array(self.p2_data['sample_mixture_ratios'])
        mx1_samples = samples[:, 0]
        mx2_samples = samples[:, 1]
        
        # è‡ªç›¸å…³å‡½æ•°åˆ†æ
        def autocorr(x, max_lag=100):
            """è®¡ç®—è‡ªç›¸å…³å‡½æ•°"""
            n = len(x)
            x = x - np.mean(x)
            autocorrs = np.correlate(x, x, mode='full')
            autocorrs = autocorrs[n-1:]
            autocorrs = autocorrs / autocorrs[0]
            return autocorrs[:max_lag+1]
        
        # è®¡ç®—è‡ªç›¸å…³
        mx1_autocorr = autocorr(mx1_samples)
        mx2_autocorr = autocorr(mx2_samples)
        
        # ä¼°è®¡æœ‰æ•ˆæ ·æœ¬å¤§å°
        def effective_sample_size(autocorr_func):
            """æ ¹æ®è‡ªç›¸å…³å‡½æ•°ä¼°è®¡æœ‰æ•ˆæ ·æœ¬å¤§å°"""
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ªè´Ÿå€¼æˆ–å°äº0.05çš„ä½ç½®
            tau_int = 1
            for i in range(1, len(autocorr_func)):
                if autocorr_func[i] <= 0.05:
                    break
                tau_int += 2 * autocorr_func[i]
            
            return len(mx1_samples) / (2 * tau_int + 1)
        
        ess_mx1 = effective_sample_size(mx1_autocorr)
        ess_mx2 = effective_sample_size(mx2_autocorr)
        
        print(f"  Mx_1 æœ‰æ•ˆæ ·æœ¬å¤§å°: {ess_mx1:.0f}")
        print(f"  Mx_2 æœ‰æ•ˆæ ·æœ¬å¤§å°: {ess_mx2:.0f}")
        
        # ç»˜åˆ¶è‡ªç›¸å…³å›¾
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        
        lags = np.arange(len(mx1_autocorr))
        axes[0].plot(lags, mx1_autocorr, 'b-', alpha=0.8)
        axes[0].axhline(y=0, color='black', linestyle='-', alpha=0.3)
        axes[0].axhline(y=0.05, color='red', linestyle='--', alpha=0.7, label='é˜ˆå€¼ (0.05)')
        axes[0].set_xlabel('æ»å (Lag)')
        axes[0].set_ylabel('è‡ªç›¸å…³')
        axes[0].set_title(f'Mx_1 è‡ªç›¸å…³å‡½æ•° (ESSâ‰ˆ{ess_mx1:.0f})')
        axes[0].legend()
        axes[0].grid(alpha=0.3)
        
        axes[1].plot(lags, mx2_autocorr, 'r-', alpha=0.8)
        axes[1].axhline(y=0, color='black', linestyle='-', alpha=0.3)
        axes[1].axhline(y=0.05, color='red', linestyle='--', alpha=0.7, label='é˜ˆå€¼ (0.05)')
        axes[1].set_xlabel('æ»å (Lag)')
        axes[1].set_ylabel('è‡ªç›¸å…³')
        axes[1].set_title(f'Mx_2 è‡ªç›¸å…³å‡½æ•° (ESSâ‰ˆ{ess_mx2:.0f})')
        axes[1].legend()
        axes[1].grid(alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(PLOTS_DIR, 'mcmc_autocorr.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        # Gewekeè¯Šæ–­
        def geweke_diagnostic(x, first=0.1, last=0.5):
            """Gewekeæ”¶æ•›è¯Šæ–­"""
            n = len(x)
            first_part = x[:int(first * n)]
            last_part = x[int((1-last) * n):]
            
            mean1, var1 = np.mean(first_part), np.var(first_part, ddof=1)
            mean2, var2 = np.mean(last_part), np.var(last_part, ddof=1)
            
            se1 = var1 / len(first_part)
            se2 = var2 / len(last_part)
            
            z_score = (mean1 - mean2) / np.sqrt(se1 + se2)
            return z_score
        
        geweke_mx1 = geweke_diagnostic(mx1_samples)
        geweke_mx2 = geweke_diagnostic(mx2_samples)
        
        print(f"  Gewekeè¯Šæ–­:")
        print(f"    Mx_1 Zåˆ†æ•°: {geweke_mx1:.3f} {'âœ“' if abs(geweke_mx1) < 2 else 'âœ—'}")
        print(f"    Mx_2 Zåˆ†æ•°: {geweke_mx2:.3f} {'âœ“' if abs(geweke_mx2) < 2 else 'âœ—'}")

# æ‰©å±•mainå‡½æ•°ä»¥åŒ…å«é«˜çº§åˆ†æ
def enhanced_main():
    """å¢å¼ºç‰ˆä¸»å‡½æ•°"""
    # æ‰§è¡ŒåŸºæœ¬åˆ†æ
    main()
    
    # æ‰§è¡Œé«˜çº§åˆ†æ
    print("\n" + "="*50)
    print("å¼€å§‹é«˜çº§åˆ†æ...")
    
    loader = STRDataLoader()
    loader.load_problem1_data('prob1_features_enhanced.csv')
    loader.load_problem2_data('problem2_mcmc_results.json')
    
    if loader.p1_data is not None or loader.p2_data is not None:
        advanced_analyzer = AdvancedAnalyzer(loader.p1_data, loader.p2_data)
        
        # ç›¸å…³æ€§åˆ†æ
        advanced_analyzer.correlation_analysis()
        
        # å¼‚å¸¸å€¼åˆ†æ
        advanced_analyzer.outlier_analysis()
        
        # æ¨¡å‹ç¨³å®šæ€§åˆ†æ
        advanced_analyzer.model_stability_analysis()
        
        # MCMCè¯Šæ–­
        advanced_analyzer.mcmc_diagnostics()
        
        print("\nâœ“ é«˜çº§åˆ†æå®Œæˆ")

# ==================== æ‰§è¡Œä¸»ç¨‹åº ====================
if __name__ == "__main__":
    try:
        enhanced_main()
        print("\nğŸ‰ æ‰€æœ‰åˆ†æä»»åŠ¡å®Œæˆ!")
        print("è¯·æŸ¥çœ‹ç”Ÿæˆçš„å›¾è¡¨å’ŒæŠ¥å‘Šæ–‡ä»¶ã€‚")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()