# -*- coding: utf-8 -*-
"""
æ•°å­¦å»ºæ¨¡ - æ³•åŒ»DNAåˆ†æ - é—®é¢˜1ï¼šè´¡çŒ®è€…äººæ•°è¯†åˆ« (éšæœºæ£®æ—ä¼˜åŒ–ç‰ˆ)

ç‰ˆæœ¬: V8.0 - Random Forest Optimization
æ—¥æœŸ: 2025-06-06
æè¿°: åŸºäºRFECVç‰¹å¾é€‰æ‹© + éšæœºæ£®æ—æ·±åº¦ä¼˜åŒ– + è¯¦ç»†NoCæ€§èƒ½åˆ†æ
ä¸»è¦ç‰¹ç‚¹:
1. ä½¿ç”¨éšæœºæ£®æ—æ›¿ä»£æ¢¯åº¦æå‡æœº
2. ä¸‰é˜¶æ®µå‚æ•°ä¼˜åŒ–ï¼ˆç²—è°ƒâ†’ç»†è°ƒâ†’å¾®è°ƒï¼‰
3. è¯¦ç»†çš„å„NoCç±»åˆ«æ€§èƒ½åˆ†æ
4. è¢‹å¤–(OOB)è¯„ä¼°å¢å¼ºæ¨¡å‹å¯é æ€§
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import warnings
import json
import re
from scipy import stats
from scipy.signal import find_peaks
from collections import Counter
from time import time
import random

# æœºå™¨å­¦ä¹ ç›¸å…³
from sklearn.model_selection import (train_test_split, cross_val_score, StratifiedKFold, 
                                   GridSearchCV, RandomizedSearchCV, validation_curve,
                                   cross_validate, RepeatedStratifiedKFold)
from sklearn.metrics import (accuracy_score, confusion_matrix, classification_report, 
                           roc_curve, auc, make_scorer, f1_score, balanced_accuracy_score,
                           precision_recall_curve, average_precision_score, precision_score, recall_score)
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.feature_selection import RFECV, SelectFromModel
from sklearn.utils import resample
from sklearn.utils.class_weight import compute_sample_weight
from sklearn.calibration import CalibratedClassifierCV

# å¯è§£é‡Šæ€§åˆ†æ
try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    print("SHAPä¸å¯ç”¨ï¼Œå°†è·³è¿‡å¯è§£é‡Šæ€§åˆ†æ")
    SHAP_AVAILABLE = False

# é…ç½®
plt.rcParams['font.sans-serif'] = ["Arial Unicode MS", "SimHei", "Microsoft YaHei"]
plt.rcParams['axes.unicode_minus'] = False
warnings.filterwarnings('ignore')

print("=== æ³•åŒ»æ··åˆSTRå›¾è°±NoCæ™ºèƒ½è¯†åˆ«ç³»ç»Ÿ V8.0 (éšæœºæ£®æ—ç‰ˆ) ===")
print("RFECVç‰¹å¾é€‰æ‹© + éšæœºæ£®æ—æ·±åº¦ä¼˜åŒ– + è¯¦ç»†æ€§èƒ½åˆ†æ")

# =====================
# 1. æ–‡ä»¶è·¯å¾„ä¸åŸºç¡€è®¾ç½®
# =====================
DATA_DIR = './'
file_path = os.path.join(DATA_DIR, 'é™„ä»¶1ï¼šä¸åŒäººæ•°çš„STRå›¾è°±æ•°æ®.csv')
PLOTS_DIR = os.path.join(DATA_DIR, 'noc_rf_optimization')
if not os.path.exists(PLOTS_DIR):
    os.makedirs(PLOTS_DIR)

# å…³é”®å‚æ•°è®¾ç½®
HEIGHT_THRESHOLD = 50
SATURATION_THRESHOLD = 30000
CTA_THRESHOLD = 0.5
PHR_IMBALANCE_THRESHOLD = 0.6

# =====================
# 2. ä¸­æ–‡ç‰¹å¾åç§°æ˜ å°„
# =====================
FEATURE_NAME_MAPPING = {
    # Aç±»ï¼šå›¾è°±å±‚é¢åŸºç¡€ç»Ÿè®¡ç‰¹å¾
    'mac_profile': 'æ ·æœ¬æœ€å¤§ç­‰ä½åŸºå› æ•°',
    'total_distinct_alleles': 'æ ·æœ¬æ€»ç‰¹å¼‚ç­‰ä½åŸºå› æ•°',
    'avg_alleles_per_locus': 'æ¯ä½ç‚¹å¹³å‡ç­‰ä½åŸºå› æ•°',
    'std_alleles_per_locus': 'æ¯ä½ç‚¹ç­‰ä½åŸºå› æ•°æ ‡å‡†å·®',
    'loci_gt2_alleles': 'ç­‰ä½åŸºå› æ•°å¤§äº2çš„ä½ç‚¹æ•°',
    'loci_gt3_alleles': 'ç­‰ä½åŸºå› æ•°å¤§äº3çš„ä½ç‚¹æ•°',
    'loci_gt4_alleles': 'ç­‰ä½åŸºå› æ•°å¤§äº4çš„ä½ç‚¹æ•°',
    'loci_gt5_alleles': 'ç­‰ä½åŸºå› æ•°å¤§äº5çš„ä½ç‚¹æ•°',
    'loci_gt6_alleles': 'ç­‰ä½åŸºå› æ•°å¤§äº6çš„ä½ç‚¹æ•°',
    'allele_count_dist_entropy': 'ç­‰ä½åŸºå› è®¡æ•°åˆ†å¸ƒç†µ',
    
    # Bç±»ï¼šå³°é«˜ã€å¹³è¡¡æ€§åŠéšæœºæ•ˆåº”ç‰¹å¾
    'avg_peak_height': 'å¹³å‡å³°é«˜',
    'std_peak_height': 'å³°é«˜æ ‡å‡†å·®',
    'min_peak_height': 'æœ€å°å³°é«˜',
    'max_peak_height': 'æœ€å¤§å³°é«˜',
    'peak_height_cv': 'å³°é«˜å˜å¼‚ç³»æ•°',
    'avg_phr': 'å¹³å‡å³°é«˜æ¯”',
    'std_phr': 'å³°é«˜æ¯”æ ‡å‡†å·®',
    'min_phr': 'æœ€å°å³°é«˜æ¯”',
    'median_phr': 'å³°é«˜æ¯”ä¸­ä½æ•°',
    'num_loci_with_phr': 'å¯è®¡ç®—å³°é«˜æ¯”çš„ä½ç‚¹æ•°',
    'num_severe_imbalance_loci': 'ä¸¥é‡å¤±è¡¡ä½ç‚¹æ•°',
    'ratio_severe_imbalance_loci': 'ä¸¥é‡å¤±è¡¡ä½ç‚¹æ¯”ä¾‹',
    'skewness_peak_height': 'å³°é«˜åˆ†å¸ƒååº¦',
    'kurtosis_peak_height': 'å³°é«˜åˆ†å¸ƒå³­åº¦',
    'modality_peak_height': 'å³°é«˜åˆ†å¸ƒå¤šå³°æ€§',
    'num_saturated_peaks': 'é¥±å’Œå³°æ•°é‡',
    'ratio_saturated_peaks': 'é¥±å’Œå³°æ¯”ä¾‹',
    
    # Cç±»ï¼šä¿¡æ¯è®ºåŠå›¾è°±å¤æ‚åº¦ç‰¹å¾
    'inter_locus_balance_entropy': 'ä½ç‚¹é—´å¹³è¡¡ç†µ',
    'avg_locus_allele_entropy': 'å¹³å‡ä½ç‚¹ç­‰ä½åŸºå› ç†µ',
    'peak_height_entropy': 'å³°é«˜åˆ†å¸ƒç†µ',
    'num_loci_with_effective_alleles': 'æœ‰æ•ˆç­‰ä½åŸºå› ä½ç‚¹æ•°',
    'num_loci_no_effective_alleles': 'æ— æœ‰æ•ˆç­‰ä½åŸºå› ä½ç‚¹æ•°',
    
    # Dç±»ï¼šDNAé™è§£ä¸ä¿¡æ¯ä¸¢å¤±ç‰¹å¾
    'height_size_correlation': 'å³°é«˜ç‰‡æ®µå¤§å°ç›¸å…³æ€§',
    'height_size_slope': 'å³°é«˜ç‰‡æ®µå¤§å°å›å½’æ–œç‡',
    'weighted_height_size_slope': 'åŠ æƒå³°é«˜ç‰‡æ®µå¤§å°æ–œç‡',
    'phr_size_slope': 'å³°é«˜æ¯”ç‰‡æ®µå¤§å°æ–œç‡',
    'locus_dropout_score_weighted_by_size': 'ç‰‡æ®µå¤§å°åŠ æƒä½ç‚¹ä¸¢å¤±è¯„åˆ†',
    'degradation_index_rfu_per_bp': 'RFUæ¯ç¢±åŸºå¯¹é™è§£æŒ‡æ•°',
    'info_completeness_ratio_small_large': 'å°å¤§ç‰‡æ®µä¿¡æ¯å®Œæ•´åº¦æ¯”ç‡',
    
    # æ–°å¢é«˜çº§ç‰¹å¾
    'peak_height_quantile_ratio': 'å³°é«˜åˆ†ä½æ•°æ¯”ç‡',
    'allele_diversity_index': 'ç­‰ä½åŸºå› å¤šæ ·æ€§æŒ‡æ•°',
    'peak_pattern_complexity': 'å³°æ¨¡å¼å¤æ‚åº¦',
    'heterozygosity_rate': 'æ‚åˆç‡',
    'peak_clustering_coefficient': 'å³°èšç±»ç³»æ•°'
}

def get_chinese_name(feature_name):
    """è·å–ç‰¹å¾çš„ä¸­æ–‡åç§°"""
    return FEATURE_NAME_MAPPING.get(feature_name, feature_name)

# =====================
# 3. è¾…åŠ©å‡½æ•°å®šä¹‰
# =====================
def extract_noc_from_filename(filename):
    """ä»æ–‡ä»¶åæå–è´¡çŒ®è€…äººæ•°ï¼ˆNoCï¼‰"""
    match = re.search(r'-(\d+(?:_\d+)*)-[\d;]+-', str(filename))
    if match:
        ids = [id_val for id_val in match.group(1).split('_') if id_val.isdigit()]
        return len(ids) if len(ids) > 0 else np.nan
    return np.nan

def calculate_entropy(probabilities):
    """è®¡ç®—é¦™å†œç†µ"""
    probabilities = np.array(probabilities)
    probabilities = probabilities[probabilities > 0]
    if len(probabilities) == 0:
        return 0.0
    return -np.sum(probabilities * np.log(probabilities + 1e-10))

def calculate_ols_slope(x, y):
    """è®¡ç®—OLSå›å½’æ–œç‡"""
    if len(x) < 2 or len(x) != len(y):
        return 0.0
    
    x = np.array(x)
    y = np.array(y)
    
    if len(np.unique(x)) < 2:
        return 0.0
    
    try:
        slope, _, _, _, _ = stats.linregress(x, y)
        return slope
    except:
        return 0.0

# =====================
# 4. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
# =====================
print("\n=== æ­¥éª¤1: æ•°æ®åŠ è½½ä¸é¢„å¤„ç† ===")

# åŠ è½½æ•°æ®
try:
    df = pd.read_csv(file_path, encoding='utf-8')
    print(f"æˆåŠŸåŠ è½½æ•°æ®ï¼Œå½¢çŠ¶: {df.shape}")
except Exception as e:
    print(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
    exit()

# æå–NoCæ ‡ç­¾
df['NoC_True'] = df['Sample File'].apply(extract_noc_from_filename)
df = df.dropna(subset=['NoC_True'])
df['NoC_True'] = df['NoC_True'].astype(int)

print(f"åŸå§‹æ•°æ®NoCåˆ†å¸ƒ: {df['NoC_True'].value_counts().sort_index().to_dict()}")
print(f"åŸå§‹æ ·æœ¬æ•°: {df['Sample File'].nunique()}")

# =====================
# 5. ç®€åŒ–å³°å¤„ç†ä¸CTAè¯„ä¼°
# =====================
print("\n=== æ­¥éª¤2: ç®€åŒ–å³°å¤„ç†ä¸ä¿¡å·è¡¨å¾ ===")

def process_peaks_simplified(sample_data):
    """ç®€åŒ–çš„å³°å¤„ç†å‡½æ•°"""
    processed_data = []
    
    for _, sample_row in sample_data.iterrows():
        sample_file = sample_row['Sample File']
        marker = sample_row['Marker']
        
        # æå–æ‰€æœ‰å³°
        peaks = []
        for i in range(1, 101):
            allele = sample_row.get(f'Allele {i}')
            size = sample_row.get(f'Size {i}')
            height = sample_row.get(f'Height {i}')
            
            if pd.notna(allele) and pd.notna(size) and pd.notna(height):
                original_height = float(height)
                corrected_height = min(original_height, SATURATION_THRESHOLD)
                
                if corrected_height >= HEIGHT_THRESHOLD:
                    peaks.append({
                        'allele': allele,
                        'size': float(size),
                        'height': corrected_height,
                        'original_height': original_height
                    })
        
        if not peaks:
            continue
            
        # ç®€åŒ–çš„CTAè¯„ä¼°
        peaks.sort(key=lambda x: x['height'], reverse=True)
        
        for peak in peaks:
            height_rank = peaks.index(peak) + 1
            total_peaks = len(peaks)
            
            # ç®€åŒ–çš„CTAè®¡ç®—
            if height_rank == 1:
                cta = 0.95
            elif height_rank == 2 and total_peaks >= 2:
                height_ratio = peak['height'] / peaks[0]['height']
                cta = 0.8 if height_ratio > 0.3 else 0.6
            else:
                height_ratio = peak['height'] / peaks[0]['height']
                cta = max(0.1, min(0.8, height_ratio))
            
            if cta >= CTA_THRESHOLD:
                processed_data.append({
                    'Sample File': sample_file,
                    'Marker': marker,
                    'Allele': peak['allele'],
                    'Size': peak['size'],
                    'Height': peak['height'],
                    'Original_Height': peak['original_height'],
                    'CTA': cta
                })
    
    return pd.DataFrame(processed_data)

# å¤„ç†æ‰€æœ‰æ ·æœ¬
print("å¤„ç†å³°æ•°æ®...")
all_processed_peaks = []
for sample_file, group in df.groupby('Sample File'):
    sample_peaks = process_peaks_simplified(group)
    if not sample_peaks.empty:
        all_processed_peaks.append(sample_peaks)

df_peaks = pd.concat(all_processed_peaks, ignore_index=True) if all_processed_peaks else pd.DataFrame()
print(f"å¤„ç†åçš„å³°æ•°æ®å½¢çŠ¶: {df_peaks.shape}")

# =====================
# 6. å¢å¼ºç‰ˆç‰¹å¾å·¥ç¨‹
# =====================
print("\n=== æ­¥éª¤3: å¢å¼ºç‰ˆç‰¹å¾å·¥ç¨‹ ===")

def extract_enhanced_features(sample_file, sample_peaks):
    """æå–å¢å¼ºçš„ç‰¹å¾é›†ï¼ŒåŒ…æ‹¬æ–°çš„é«˜çº§ç‰¹å¾"""
    if sample_peaks.empty:
        return {}
    
    features = {'æ ·æœ¬æ–‡ä»¶': sample_file}
    
    # åŸºç¡€æ•°æ®å‡†å¤‡
    total_peaks = len(sample_peaks)
    all_heights = sample_peaks['Height'].values
    all_sizes = sample_peaks['Size'].values
    
    # æŒ‰ä½ç‚¹åˆ†ç»„ç»Ÿè®¡
    locus_groups = sample_peaks.groupby('Marker')
    alleles_per_locus = locus_groups['Allele'].nunique()
    locus_heights = locus_groups['Height'].sum()
    
    # Aç±»ï¼šå›¾è°±å±‚é¢åŸºç¡€ç»Ÿè®¡ç‰¹å¾
    features['æ ·æœ¬æœ€å¤§ç­‰ä½åŸºå› æ•°'] = alleles_per_locus.max() if len(alleles_per_locus) > 0 else 0
    features['æ ·æœ¬æ€»ç‰¹å¼‚ç­‰ä½åŸºå› æ•°'] = sample_peaks['Allele'].nunique()
    features['æ¯ä½ç‚¹å¹³å‡ç­‰ä½åŸºå› æ•°'] = alleles_per_locus.mean() if len(alleles_per_locus) > 0 else 0
    features['æ¯ä½ç‚¹ç­‰ä½åŸºå› æ•°æ ‡å‡†å·®'] = alleles_per_locus.std() if len(alleles_per_locus) > 1 else 0
    
    # MGTNç³»åˆ—
    for N in [2, 3, 4, 5, 6]:
        features[f'ç­‰ä½åŸºå› æ•°å¤§äº{N}çš„ä½ç‚¹æ•°'] = (alleles_per_locus >= N).sum()
    
    # ç­‰ä½åŸºå› è®¡æ•°åˆ†å¸ƒçš„ç†µ
    if len(alleles_per_locus) > 0:
        counts = alleles_per_locus.value_counts(normalize=True)
        features['ç­‰ä½åŸºå› è®¡æ•°åˆ†å¸ƒç†µ'] = calculate_entropy(counts.values)
    else:
        features['ç­‰ä½åŸºå› è®¡æ•°åˆ†å¸ƒç†µ'] = 0
    
    # Bç±»ï¼šå³°é«˜ã€å¹³è¡¡æ€§åŠéšæœºæ•ˆåº”ç‰¹å¾
    if total_peaks > 0:
        # åŸºç¡€å³°é«˜ç»Ÿè®¡
        features['å¹³å‡å³°é«˜'] = np.mean(all_heights)
        features['å³°é«˜æ ‡å‡†å·®'] = np.std(all_heights) if total_peaks > 1 else 0
        features['æœ€å°å³°é«˜'] = np.min(all_heights)
        features['æœ€å¤§å³°é«˜'] = np.max(all_heights)
        features['å³°é«˜å˜å¼‚ç³»æ•°'] = features['å³°é«˜æ ‡å‡†å·®'] / features['å¹³å‡å³°é«˜'] if features['å¹³å‡å³°é«˜'] > 0 else 0
        
        # æ–°å¢ï¼šå³°é«˜åˆ†ä½æ•°æ¯”ç‡
        if total_peaks >= 4:
            q25 = np.percentile(all_heights, 25)
            q75 = np.percentile(all_heights, 75)
            features['å³°é«˜åˆ†ä½æ•°æ¯”ç‡'] = q75 / q25 if q25 > 0 else 0
        else:
            features['å³°é«˜åˆ†ä½æ•°æ¯”ç‡'] = 1.0
        
        # å³°é«˜æ¯”ç›¸å…³ç‰¹å¾
        phr_values = []
        for marker, marker_group in locus_groups:
            if len(marker_group) == 2:
                heights = marker_group['Height'].values
                phr = min(heights) / max(heights) if max(heights) > 0 else 0
                phr_values.append(phr)
        
        if phr_values:
            features['å¹³å‡å³°é«˜æ¯”'] = np.mean(phr_values)
            features['å³°é«˜æ¯”æ ‡å‡†å·®'] = np.std(phr_values) if len(phr_values) > 1 else 0
            features['æœ€å°å³°é«˜æ¯”'] = np.min(phr_values)
            features['å³°é«˜æ¯”ä¸­ä½æ•°'] = np.median(phr_values)
            features['å¯è®¡ç®—å³°é«˜æ¯”çš„ä½ç‚¹æ•°'] = len(phr_values)
            features['ä¸¥é‡å¤±è¡¡ä½ç‚¹æ•°'] = sum(phr <= PHR_IMBALANCE_THRESHOLD for phr in phr_values)
            features['ä¸¥é‡å¤±è¡¡ä½ç‚¹æ¯”ä¾‹'] = features['ä¸¥é‡å¤±è¡¡ä½ç‚¹æ•°'] / len(phr_values)
        else:
            for key in ['å¹³å‡å³°é«˜æ¯”', 'å³°é«˜æ¯”æ ‡å‡†å·®', 'æœ€å°å³°é«˜æ¯”', 'å³°é«˜æ¯”ä¸­ä½æ•°', 
                       'å¯è®¡ç®—å³°é«˜æ¯”çš„ä½ç‚¹æ•°', 'ä¸¥é‡å¤±è¡¡ä½ç‚¹æ•°', 'ä¸¥é‡å¤±è¡¡ä½ç‚¹æ¯”ä¾‹']:
                features[key] = 0
        
        # å³°é«˜åˆ†å¸ƒå½¢çŠ¶ç‰¹å¾
        if total_peaks > 2:
            features['å³°é«˜åˆ†å¸ƒååº¦'] = stats.skew(all_heights)
            features['å³°é«˜åˆ†å¸ƒå³­åº¦'] = stats.kurtosis(all_heights, fisher=False)
        else:
            features['å³°é«˜åˆ†å¸ƒååº¦'] = 0
            features['å³°é«˜åˆ†å¸ƒå³­åº¦'] = 0
        
        # å³°é«˜å¤šå³°æ€§æŒ‡æ ‡
        try:
            log_heights = np.log(all_heights + 1)
            if len(np.unique(log_heights)) > 1:
                hist, _ = np.histogram(log_heights, bins=min(10, total_peaks))
                peaks_found, _ = find_peaks(hist)
                features['å³°é«˜åˆ†å¸ƒå¤šå³°æ€§'] = len(peaks_found)
            else:
                features['å³°é«˜åˆ†å¸ƒå¤šå³°æ€§'] = 1
        except:
            features['å³°é«˜åˆ†å¸ƒå¤šå³°æ€§'] = 1
        
        # é¥±å’Œæ•ˆåº”ç‰¹å¾
        saturated_peaks = (sample_peaks['Original_Height'] >= SATURATION_THRESHOLD).sum()
        features['é¥±å’Œå³°æ•°é‡'] = saturated_peaks
        features['é¥±å’Œå³°æ¯”ä¾‹'] = saturated_peaks / total_peaks
    else:
        # ç©ºå€¼å¡«å……
        for key in ['å¹³å‡å³°é«˜', 'å³°é«˜æ ‡å‡†å·®', 'æœ€å°å³°é«˜', 'æœ€å¤§å³°é«˜', 'å³°é«˜å˜å¼‚ç³»æ•°',
                   'å³°é«˜åˆ†ä½æ•°æ¯”ç‡', 'å¹³å‡å³°é«˜æ¯”', 'å³°é«˜æ¯”æ ‡å‡†å·®', 'æœ€å°å³°é«˜æ¯”', 'å³°é«˜æ¯”ä¸­ä½æ•°',
                   'å¯è®¡ç®—å³°é«˜æ¯”çš„ä½ç‚¹æ•°', 'ä¸¥é‡å¤±è¡¡ä½ç‚¹æ•°', 'ä¸¥é‡å¤±è¡¡ä½ç‚¹æ¯”ä¾‹',
                   'å³°é«˜åˆ†å¸ƒååº¦', 'å³°é«˜åˆ†å¸ƒå³­åº¦', 'å³°é«˜åˆ†å¸ƒå¤šå³°æ€§',
                   'é¥±å’Œå³°æ•°é‡', 'é¥±å’Œå³°æ¯”ä¾‹']:
            features[key] = 0
    
    # Cç±»ï¼šä¿¡æ¯è®ºåŠå›¾è°±å¤æ‚åº¦ç‰¹å¾
    if len(locus_heights) > 0:
        total_height = locus_heights.sum()
        if total_height > 0:
            locus_probs = locus_heights / total_height
            features['ä½ç‚¹é—´å¹³è¡¡ç†µ'] = calculate_entropy(locus_probs.values)
        else:
            features['ä½ç‚¹é—´å¹³è¡¡ç†µ'] = 0
    else:
        features['ä½ç‚¹é—´å¹³è¡¡ç†µ'] = 0
    
    # å¹³å‡ä½ç‚¹ç­‰ä½åŸºå› åˆ†å¸ƒç†µ
    locus_entropies = []
    for marker, marker_group in locus_groups:
        if len(marker_group) > 1:
            heights = marker_group['Height'].values
            height_sum = heights.sum()
            if height_sum > 0:
                probs = heights / height_sum
                entropy = calculate_entropy(probs)
                locus_entropies.append(entropy)
    
    features['å¹³å‡ä½ç‚¹ç­‰ä½åŸºå› ç†µ'] = np.mean(locus_entropies) if locus_entropies else 0
    
    # æ ·æœ¬æ•´ä½“å³°é«˜åˆ†å¸ƒç†µ
    if total_peaks > 0:
        log_heights = np.log(all_heights + 1)
        hist, _ = np.histogram(log_heights, bins=min(15, total_peaks))
        hist_probs = hist / hist.sum()
        hist_probs = hist_probs[hist_probs > 0]
        features['å³°é«˜åˆ†å¸ƒç†µ'] = calculate_entropy(hist_probs)
    else:
        features['å³°é«˜åˆ†å¸ƒç†µ'] = 0
    
    # å›¾è°±å®Œæ•´æ€§æŒ‡æ ‡
    effective_loci_count = len(locus_groups)
    features['æœ‰æ•ˆç­‰ä½åŸºå› ä½ç‚¹æ•°'] = effective_loci_count
    features['æ— æœ‰æ•ˆç­‰ä½åŸºå› ä½ç‚¹æ•°'] = max(0, 20 - effective_loci_count)  # å‡è®¾20ä¸ªä½ç‚¹
    
    # Dç±»ï¼šDNAé™è§£ä¸ä¿¡æ¯ä¸¢å¤±ç‰¹å¾
    if total_peaks > 1:
        # å³°é«˜ä¸ç‰‡æ®µå¤§å°çš„ç›¸å…³æ€§
        if len(np.unique(all_heights)) > 1 and len(np.unique(all_sizes)) > 1:
            features['å³°é«˜ç‰‡æ®µå¤§å°ç›¸å…³æ€§'] = np.corrcoef(all_heights, all_sizes)[0, 1]
        else:
            features['å³°é«˜ç‰‡æ®µå¤§å°ç›¸å…³æ€§'] = 0
        
        # çº¿æ€§å›å½’æ–œç‡
        features['å³°é«˜ç‰‡æ®µå¤§å°å›å½’æ–œç‡'] = calculate_ols_slope(all_sizes, all_heights)
        
        # åŠ æƒå›å½’æ–œç‡ï¼ˆç®€åŒ–ç‰ˆï¼‰
        try:
            weights = all_heights / all_heights.sum()
            features['åŠ æƒå³°é«˜ç‰‡æ®µå¤§å°æ–œç‡'] = calculate_ols_slope(all_sizes, all_heights)
        except:
            features['åŠ æƒå³°é«˜ç‰‡æ®µå¤§å°æ–œç‡'] = 0
        
        # PHRéšç‰‡æ®µå¤§å°å˜åŒ–çš„æ–œç‡
        if len(phr_values) > 1:
            phr_sizes = []
            for marker, marker_group in locus_groups:
                if len(marker_group) == 2:
                    avg_size = marker_group['Size'].mean()
                    phr_sizes.append(avg_size)
            
            if len(phr_sizes) == len(phr_values) and len(phr_sizes) > 1:
                features['å³°é«˜æ¯”ç‰‡æ®µå¤§å°æ–œç‡'] = calculate_ols_slope(phr_sizes, phr_values)
            else:
                features['å³°é«˜æ¯”ç‰‡æ®µå¤§å°æ–œç‡'] = 0
        else:
            features['å³°é«˜æ¯”ç‰‡æ®µå¤§å°æ–œç‡'] = 0
    else:
        for key in ['å³°é«˜ç‰‡æ®µå¤§å°ç›¸å…³æ€§', 'å³°é«˜ç‰‡æ®µå¤§å°å›å½’æ–œç‡', 'åŠ æƒå³°é«˜ç‰‡æ®µå¤§å°æ–œç‡', 'å³°é«˜æ¯”ç‰‡æ®µå¤§å°æ–œç‡']:
            features[key] = 0
    
    # ä½ç‚¹ä¸¢å¤±è¯„åˆ†ï¼ˆç®€åŒ–ç‰ˆï¼‰
    dropout_score = features['æ— æœ‰æ•ˆç­‰ä½åŸºå› ä½ç‚¹æ•°'] / 20  # åŸºäº20ä¸ªä½ç‚¹çš„å‡è®¾
    features['ç‰‡æ®µå¤§å°åŠ æƒä½ç‚¹ä¸¢å¤±è¯„åˆ†'] = dropout_score
    
    # RFUæ¯ç¢±åŸºå¯¹è¡°å‡æŒ‡æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰
    if len(locus_groups) > 1:
        locus_max_heights = []
        locus_avg_sizes = []
        for marker, marker_group in locus_groups:
            max_height = marker_group['Height'].max()
            avg_size = marker_group['Size'].mean()
            locus_max_heights.append(max_height)
            locus_avg_sizes.append(avg_size)
        
        features['RFUæ¯ç¢±åŸºå¯¹é™è§£æŒ‡æ•°'] = calculate_ols_slope(locus_avg_sizes, locus_max_heights)
    else:
        features['RFUæ¯ç¢±åŸºå¯¹é™è§£æŒ‡æ•°'] = 0
    
    # å°å¤§ç‰‡æ®µä¿¡æ¯å®Œæ•´åº¦æ¯”ç‡ï¼ˆç®€åŒ–ç‰ˆï¼‰
    small_fragment_effective = sum(1 for marker, group in locus_groups if group['Size'].mean() < 200)
    large_fragment_effective = sum(1 for marker, group in locus_groups if group['Size'].mean() >= 200)
    
    if large_fragment_effective > 0:
        features['å°å¤§ç‰‡æ®µä¿¡æ¯å®Œæ•´åº¦æ¯”ç‡'] = small_fragment_effective / large_fragment_effective
    else:
        features['å°å¤§ç‰‡æ®µä¿¡æ¯å®Œæ•´åº¦æ¯”ç‡'] = small_fragment_effective / 0.001
    
    # æ–°å¢é«˜çº§ç‰¹å¾
    # 1. ç­‰ä½åŸºå› å¤šæ ·æ€§æŒ‡æ•° (Simpson's diversity index)
    if total_peaks > 0:
        allele_counts = sample_peaks['Allele'].value_counts()
        total_alleles = allele_counts.sum()
        diversity = 1 - sum((n/total_alleles)**2 for n in allele_counts.values)
        features['ç­‰ä½åŸºå› å¤šæ ·æ€§æŒ‡æ•°'] = diversity
    else:
        features['ç­‰ä½åŸºå› å¤šæ ·æ€§æŒ‡æ•°'] = 0
    
    # 2. å³°æ¨¡å¼å¤æ‚åº¦
    if len(locus_groups) > 0:
        pattern_complexity = 0
        for marker, group in locus_groups:
            n_alleles = len(group)
            if n_alleles > 2:
                # è®¡ç®—å³°é«˜çš„æ ‡å‡†å·®ä½œä¸ºå¤æ‚åº¦çš„ä¸€éƒ¨åˆ†
                height_std = group['Height'].std()
                pattern_complexity += height_std / group['Height'].mean() if group['Height'].mean() > 0 else 0
        features['å³°æ¨¡å¼å¤æ‚åº¦'] = pattern_complexity / len(locus_groups)
    else:
        features['å³°æ¨¡å¼å¤æ‚åº¦'] = 0
    
    # 3. æ‚åˆç‡
    heterozygous_loci = sum(1 for marker, group in locus_groups if len(group) == 2)
    features['æ‚åˆç‡'] = heterozygous_loci / len(locus_groups) if len(locus_groups) > 0 else 0
    
    # 4. å³°èšç±»ç³»æ•°ï¼ˆæµ‹é‡å³°çš„èšé›†ç¨‹åº¦ï¼‰
    if total_peaks > 2:
        # ä½¿ç”¨å³°é«˜çš„å˜å¼‚ç³»æ•°å’Œå³°é—´è·ç¦»
        height_cv = np.std(all_heights) / np.mean(all_heights) if np.mean(all_heights) > 0 else 0
        size_range = np.max(all_sizes) - np.min(all_sizes)
        features['å³°èšç±»ç³»æ•°'] = height_cv * (1 - size_range/1000)  # å½’ä¸€åŒ–
    else:
        features['å³°èšç±»ç³»æ•°'] = 0
    
    return features

# æå–æ‰€æœ‰æ ·æœ¬çš„ç‰¹å¾
print("å¼€å§‹å¢å¼ºç‰ˆç‰¹å¾æå–...")
all_features = []

if df_peaks.empty:
    print("è­¦å‘Š: å¤„ç†åçš„å³°æ•°æ®ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤ç‰¹å¾")
    for sample_file in df['Sample File'].unique():
        features = {'æ ·æœ¬æ–‡ä»¶': sample_file, 'æ ·æœ¬æœ€å¤§ç­‰ä½åŸºå› æ•°': 0}
        all_features.append(features)
else:
    for sample_file, group in df_peaks.groupby('Sample File'):
        features = extract_enhanced_features(sample_file, group)
        if features:
            all_features.append(features)

df_features = pd.DataFrame(all_features)

# åˆå¹¶NoCæ ‡ç­¾
noc_map = df.groupby('Sample File')['NoC_True'].first().to_dict()
df_features['è´¡çŒ®è€…äººæ•°'] = df_features['æ ·æœ¬æ–‡ä»¶'].map(noc_map)
df_features = df_features.dropna(subset=['è´¡çŒ®è€…äººæ•°'])

# å¡«å……ç¼ºå¤±å€¼
numeric_cols = df_features.select_dtypes(include=[np.number]).columns
df_features[numeric_cols] = df_features[numeric_cols].fillna(0)

print(f"ç‰¹å¾æ•°æ®å½¢çŠ¶: {df_features.shape}")
print(f"ç‰¹å¾æ•°é‡: {len([col for col in df_features.columns if col not in ['æ ·æœ¬æ–‡ä»¶', 'è´¡çŒ®è€…äººæ•°']])}")

# =====================
# 7. RFECVç‰¹å¾é€‰æ‹©
# =====================
print("\n=== æ­¥éª¤4: RFECVé€’å½’ç‰¹å¾æ¶ˆé™¤ ===")

# å‡†å¤‡æ•°æ®
feature_cols = [col for col in df_features.columns if col not in ['æ ·æœ¬æ–‡ä»¶', 'è´¡çŒ®è€…äººæ•°']]
X = df_features[feature_cols].fillna(0)
y = df_features['è´¡çŒ®è€…äººæ•°']

print(f"åŸå§‹ç‰¹å¾æ•°: {len(feature_cols)}")
print(f"æ ·æœ¬æ•°: {len(X)}")
print(f"NoCåˆ†å¸ƒ: {y.value_counts().sort_index().to_dict()}")

# æ ‡ç­¾ç¼–ç 
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# ç‰¹å¾æ ‡å‡†åŒ–
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# æ•°æ®å¹³è¡¡æ€§æ£€æŸ¥
print("\næ•°æ®å¹³è¡¡æ€§æ£€æŸ¥:")
class_distribution = pd.Series(y_encoded).value_counts().sort_index()
for cls, count in class_distribution.items():
    original_cls = label_encoder.inverse_transform([cls])[0]
    print(f"  {original_cls}äºº: {count}ä¸ªæ ·æœ¬")

min_samples = class_distribution.min()
imbalance_ratio = class_distribution.max() / min_samples
print(f"ä¸å¹³è¡¡æ¯”ä¾‹: {imbalance_ratio:.2f}")

# å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
if imbalance_ratio > 3:
    try:
        from imblearn.over_sampling import SMOTE
        print("ä½¿ç”¨SMOTEå¤„ç†ç±»åˆ«ä¸å¹³è¡¡...")
        k_neighbors = min(3, min_samples - 1) if min_samples > 1 else 1
        smote = SMOTE(random_state=42, k_neighbors=k_neighbors)
        X_scaled, y_encoded = smote.fit_resample(X_scaled, y_encoded)
        print(f"SMOTEåæ ·æœ¬æ•°: {len(X_scaled)}")
    except ImportError:
        print("æœªå®‰è£…imblearnï¼Œè·³è¿‡SMOTEå¤„ç†")
    except Exception as e:
        print(f"SMOTEå¤„ç†å¤±è´¥: {e}")

# è®¾ç½®äº¤å‰éªŒè¯
min_class_size = pd.Series(y_encoded).value_counts().min()
cv_folds = min(5, min_class_size)
cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)

print(f"\nå¼€å§‹RFECVç‰¹å¾é€‰æ‹©ï¼ˆ{cv_folds}æŠ˜äº¤å‰éªŒè¯ï¼‰...")

# åˆ›å»ºåŸºç¡€éšæœºæ£®æ—ç”¨äºRFECV
base_estimator = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    min_samples_split=2,
    min_samples_leaf=1,
    max_features='sqrt',
    random_state=42,
    n_jobs=-1
)

# è‡ªå®šä¹‰è¯„åˆ†å‡½æ•°ï¼ˆå¹³è¡¡å‡†ç¡®ç‡ï¼‰
from sklearn.metrics import balanced_accuracy_score
balanced_scorer = make_scorer(balanced_accuracy_score)

# æ‰§è¡ŒRFECV
print("æ‰§è¡Œé€’å½’ç‰¹å¾æ¶ˆé™¤äº¤å‰éªŒè¯...")
start_time = time()

rfecv = RFECV(
    estimator=base_estimator,
    step=1,
    cv=cv,
    scoring=balanced_scorer,
    n_jobs=-1,
    verbose=1
)

rfecv.fit(X_scaled, y_encoded)

elapsed_time = time() - start_time
print(f"RFECVå®Œæˆï¼Œè€—æ—¶: {elapsed_time:.1f}ç§’")

# è·å–é€‰æ‹©çš„ç‰¹å¾
selected_features = [feature_cols[i] for i in range(len(feature_cols)) if rfecv.support_[i]]
selected_indices = [i for i in range(len(feature_cols)) if rfecv.support_[i]]

print(f"\nRFECVé€‰æ‹©çš„ç‰¹å¾æ•°: {len(selected_features)}")
print(f"æœ€ä¼˜ç‰¹å¾æ•°: {rfecv.n_features_}")

# è·å–æœ€ä½³åˆ†æ•°
if hasattr(rfecv, 'cv_results_'):
    best_score = max(rfecv.cv_results_['mean_test_score'])
elif hasattr(rfecv, 'grid_scores_'):
    best_score = max(rfecv.grid_scores_)
else:
    best_score = 0.0

print(f"æœ€ä½³äº¤å‰éªŒè¯åˆ†æ•°: {best_score:.4f}")

print("\nRFECVé€‰æ‹©çš„ç‰¹å¾:")
for i, feature in enumerate(selected_features, 1):
    print(f"  {i:2d}. {feature}")

# åˆ›å»ºæœ€ç»ˆçš„ç‰¹å¾çŸ©é˜µ
X_selected = X_scaled[:, selected_indices]

# å¯è§†åŒ–RFECVç»“æœ
plt.figure(figsize=(12, 8))

if hasattr(rfecv, 'cv_results_'):
    scores = rfecv.cv_results_['mean_test_score']
elif hasattr(rfecv, 'grid_scores_'):
    scores = rfecv.grid_scores_
else:
    scores = [0.0] * len(feature_cols)

plt.plot(range(1, len(scores) + 1), scores, 'bo-')
plt.axvline(x=rfecv.n_features_, color='red', linestyle='--', 
           label=f'æœ€ä¼˜ç‰¹å¾æ•°: {rfecv.n_features_}')
plt.xlabel('ç‰¹å¾æ•°é‡')
plt.ylabel('äº¤å‰éªŒè¯åˆ†æ•°')
plt.title('RFECVç‰¹å¾é€‰æ‹©ç»“æœ')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig(os.path.join(PLOTS_DIR, 'RFECVç»“æœ.png'), dpi=300, bbox_inches='tight')
plt.close()

# =====================
# 8. éšæœºæ£®æ—ä¼˜åŒ–å™¨
# =====================
class RandomForestOptimizer:
    """éšæœºæ£®æ—ä¼˜åŒ–å™¨ï¼Œé’ˆå¯¹ä¸åŒNoCä½¿ç”¨ä¸åŒç­–ç•¥"""
    
    def __init__(self):
        self.noc_specific_configs = {
            2: {
                'class_weight': {2: 1.0},
                'feature_selection_params': {'min_features_to_select': 8},
                'model_params': {
                    'n_estimators': 200,
                    'max_depth': 10,
                    'min_samples_split': 2,
                    'min_samples_leaf': 1,
                    'max_features': 'sqrt',
                    'bootstrap': True
                }
            },
            3: {
                'class_weight': {3: 1.2},
                'feature_selection_params': {'min_features_to_select': 10},
                'model_params': {
                    'n_estimators': 300,
                    'max_depth': 12,
                    'min_samples_split': 3,
                    'min_samples_leaf': 2,
                    'max_features': 'sqrt',
                    'bootstrap': True
                }
            },
            4: {
                'class_weight': {4: 2.0},
                'feature_selection_params': {'min_features_to_select': 12},
                'model_params': {
                    'n_estimators': 500,
                    'max_depth': 15,
                    'min_samples_split': 2,
                    'min_samples_leaf': 1,
                    'max_features': 'sqrt',
                    'bootstrap': True,
                    'oob_score': True
                }
            },
            5: {
                'class_weight': {5: 3.0},
                'feature_selection_params': {'min_features_to_select': 15},
                'model_params': {
                    'n_estimators': 800,
                    'max_depth': 20,
                    'min_samples_split': 2,
                    'min_samples_leaf': 1,
                    'max_features': 'sqrt',
                    'bootstrap': True,
                    'oob_score': True
                }
            }
        }
    
    def get_sample_weights(self, y):
        """è®¡ç®—æ ·æœ¬æƒé‡ï¼Œå¯¹å°‘æ•°ç±»ç»™äºˆæ›´é«˜æƒé‡"""
        # åŸºç¡€æƒé‡
        base_weights = compute_sample_weight('balanced', y)
        
        # å¯¹4äººå’Œ5äººæ ·æœ¬é¢å¤–åŠ æƒ
        enhanced_weights = base_weights.copy()
        for i, label in enumerate(y):
            if label == 4:
                enhanced_weights[i] *= 2.5
            elif label == 5:
                enhanced_weights[i] *= 4.0
        
        return enhanced_weights

# =====================
# 9. éšæœºæ£®æ—æ·±åº¦ä¼˜åŒ–
# =====================
print("\n=== æ­¥éª¤5: éšæœºæ£®æ—æ·±åº¦ä¼˜åŒ– ===")

# è‡ªå®šä¹‰åˆ†å±‚åˆ’åˆ†
def custom_stratified_split(X, y, test_size=0.25, random_state=42):
    """ç¡®ä¿æ¯ä¸ªç±»åˆ«åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­éƒ½æœ‰ä»£è¡¨"""
    np.random.seed(random_state)
    unique_classes = np.unique(y)
    
    X_train_list, X_test_list = [], []
    y_train_list, y_test_list = [], []
    
    for cls in unique_classes:
        cls_indices = np.where(y == cls)[0]
        n_cls = len(cls_indices)
        
        if n_cls <= 2:
            n_test = 1
            n_train = n_cls - 1
        else:
            n_test = max(1, int(n_cls * test_size))
            n_train = n_cls - n_test
        
        np.random.shuffle(cls_indices)
        test_indices = cls_indices[:n_test]
        train_indices = cls_indices[n_test:n_test+n_train]
        
        X_train_list.append(X[train_indices])
        X_test_list.append(X[test_indices])
        y_train_list.append(y[train_indices])
        y_test_list.append(y[test_indices])
    
    X_train = np.vstack(X_train_list)
    X_test = np.vstack(X_test_list)
    y_train = np.hstack(y_train_list)
    y_test = np.hstack(y_test_list)
    
    return X_train, X_test, y_train, y_test

# åˆ’åˆ†æ•°æ®é›†
X_train, X_test, y_train, y_test = custom_stratified_split(X_selected, y_encoded, test_size=0.25)

print(f"è®­ç»ƒé›†å½¢çŠ¶: {X_train.shape}")
print(f"æµ‹è¯•é›†å½¢çŠ¶: {X_test.shape}")
print(f"è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ: {pd.Series(y_train).value_counts().sort_index().to_dict()}")
print(f"æµ‹è¯•é›†æ ‡ç­¾åˆ†å¸ƒ: {pd.Series(y_test).value_counts().sort_index().to_dict()}")

# åˆå§‹åŒ–ä¼˜åŒ–å™¨
rf_optimizer = RandomForestOptimizer()

# è®¡ç®—ç±»åˆ«æƒé‡
class_weights = rf_optimizer.get_sample_weights(y_train)

# è®¾ç½®ä¼˜åŒ–çš„äº¤å‰éªŒè¯
min_class_size_train = pd.Series(y_train).value_counts().min()
cv_folds_opt = min(5, min_class_size_train)
cv_opt = StratifiedKFold(n_splits=cv_folds_opt, shuffle=True, random_state=42)

# === é˜¶æ®µ1: ç²—è°ƒå‚æ•° ===
print("\né˜¶æ®µ1: éšæœºæ£®æ—ç²—è°ƒå‚æ•°...")

# ç²—è°ƒå‚æ•°ç½‘æ ¼
coarse_param_grid = {
    'n_estimators': [100, 200, 300, 500, 800],
    'max_depth': [5, 10, 15, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2', None],
    'bootstrap': [True],
    'oob_score': [True]
}

# ç²—è°ƒæœç´¢
print("æ‰§è¡Œç²—è°ƒéšæœºæœç´¢...")
coarse_search = RandomizedSearchCV(
    RandomForestClassifier(random_state=42, n_jobs=-1),
    coarse_param_grid,
    n_iter=50,
    cv=cv_opt,
    scoring=balanced_scorer,
    n_jobs=-1,
    random_state=42,
    verbose=1
)

coarse_search.fit(X_train, y_train, sample_weight=class_weights)

print(f"ç²—è°ƒæœ€ä½³å‚æ•°: {coarse_search.best_params_}")
print(f"ç²—è°ƒæœ€ä½³åˆ†æ•°: {coarse_search.best_score_:.4f}")

# === é˜¶æ®µ2: ç»†è°ƒå‚æ•° ===
print("\né˜¶æ®µ2: åŸºäºç²—è°ƒç»“æœè¿›è¡Œç»†è°ƒ...")

# æå–ç²—è°ƒæœ€ä½³å‚æ•°
best_coarse = coarse_search.best_params_

# ç»†è°ƒå‚æ•°ç½‘æ ¼
fine_param_grid = {}

# n_estimators ç»†è°ƒ
if best_coarse['n_estimators'] == 100:
    fine_param_grid['n_estimators'] = [80, 100, 150, 200]
elif best_coarse['n_estimators'] == 800:
    fine_param_grid['n_estimators'] = [600, 800, 1000, 1200]
else:
    base_n = best_coarse['n_estimators']
    fine_param_grid['n_estimators'] = [base_n - 100, base_n, base_n + 100, base_n + 200]

# max_depth ç»†è°ƒ
if best_coarse['max_depth'] is None:
    fine_param_grid['max_depth'] = [15, 20, 25, None]
else:
    base_depth = best_coarse['max_depth']
    fine_param_grid['max_depth'] = [max(5, base_depth - 2), base_depth, base_depth + 2, base_depth + 5]

# å…¶ä»–å‚æ•°ç»†è°ƒ
fine_param_grid.update({
    'min_samples_split': [max(2, best_coarse['min_samples_split'] - 1), 
                         best_coarse['min_samples_split'],
                         best_coarse['min_samples_split'] + 2],
    'min_samples_leaf': [max(1, best_coarse['min_samples_leaf'] - 1),
                        best_coarse['min_samples_leaf'],
                        best_coarse['min_samples_leaf'] + 1],
    'max_features': [best_coarse['max_features']],
    'bootstrap': [True],
    'oob_score': [True]
})

print("æ‰§è¡Œç»†è°ƒç½‘æ ¼æœç´¢...")
fine_search = GridSearchCV(
    RandomForestClassifier(random_state=42, n_jobs=-1),
    fine_param_grid,
    cv=cv_opt,
    scoring=balanced_scorer,
    n_jobs=-1,
    verbose=1
)

fine_search.fit(X_train, y_train, sample_weight=class_weights)

print(f"ç»†è°ƒæœ€ä½³å‚æ•°: {fine_search.best_params_}")
print(f"ç»†è°ƒæœ€ä½³åˆ†æ•°: {fine_search.best_score_:.4f}")

# === é˜¶æ®µ3: æœ€ç»ˆä¼˜åŒ– ===
print("\né˜¶æ®µ3: æœ€ç»ˆå‚æ•°å¾®è°ƒ...")

# æœ€ç»ˆå¾®è°ƒå‚æ•°
final_params = fine_search.best_params_.copy()

# é’ˆå¯¹ç±»åˆ«ä¸å¹³è¡¡çš„æœ€ç»ˆä¼˜åŒ–
validation_param_grid = {
    'class_weight': [None, 'balanced', 'balanced_subsample'],
    'criterion': ['gini', 'entropy'],
    'max_leaf_nodes': [None, 50, 100, 200]
}

# å›ºå®šå…¶ä»–æœ€ä¼˜å‚æ•°
base_estimator_final = RandomForestClassifier(
    n_estimators=final_params['n_estimators'],
    max_depth=final_params['max_depth'],
    min_samples_split=final_params['min_samples_split'],
    min_samples_leaf=final_params['min_samples_leaf'],
    max_features=final_params['max_features'],
    bootstrap=final_params['bootstrap'],
    oob_score=final_params['oob_score'],
    random_state=42,
    n_jobs=-1
)

print("æ‰§è¡Œæœ€ç»ˆå¾®è°ƒ...")
final_search = GridSearchCV(
    base_estimator_final,
    validation_param_grid,
    cv=cv_opt,
    scoring=balanced_scorer,
    n_jobs=-1,
    verbose=1
)

final_search.fit(X_train, y_train, sample_weight=class_weights)

print(f"æœ€ç»ˆæœ€ä½³å‚æ•°: {final_search.best_params_}")
print(f"æœ€ç»ˆæœ€ä½³åˆ†æ•°: {final_search.best_score_:.4f}")

# åˆ›å»ºæœ€ä¼˜æ¨¡å‹
final_params.update(final_search.best_params_)
optimal_rf_model = RandomForestClassifier(**final_params, random_state=42, n_jobs=-1)
optimal_rf_model.fit(X_train, y_train, sample_weight=class_weights)

# åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
y_pred_rf = optimal_rf_model.predict(X_test)
rf_accuracy = accuracy_score(y_test, y_pred_rf)
balanced_acc = balanced_accuracy_score(y_test, y_pred_rf)
f1_weighted = f1_score(y_test, y_pred_rf, average='weighted')

print(f"\næœ€ä¼˜éšæœºæ£®æ—æµ‹è¯•é›†æ€§èƒ½:")
print(f"  å‡†ç¡®ç‡: {rf_accuracy:.4f}")
print(f"  å¹³è¡¡å‡†ç¡®ç‡: {balanced_acc:.4f}")
print(f"  åŠ æƒF1åˆ†æ•°: {f1_weighted:.4f}")

# å¦‚æœæ¨¡å‹æ”¯æŒOOBè¯„ä¼°
if hasattr(optimal_rf_model, 'oob_score_') and optimal_rf_model.oob_score_ is not None:
    print(f"  è¢‹å¤–(OOB)è¯„ä¼°åˆ†æ•°: {optimal_rf_model.oob_score_:.4f}")

# =====================
# 10. éªŒè¯æ›²çº¿åˆ†æ
# =====================
print("\n=== æ­¥éª¤6: éšæœºæ£®æ—éªŒè¯æ›²çº¿åˆ†æ ===")

# åˆ†æéšæœºæ£®æ—å…³é”®è¶…å‚æ•°çš„å½±å“
key_params = ['n_estimators', 'max_depth', 'max_features']

fig, axes = plt.subplots(1, 3, figsize=(18, 6))

for i, param in enumerate(key_params):
    print(f"åˆ†æå‚æ•°: {param}")
    
    # è®¾ç½®å‚æ•°èŒƒå›´
    if param == 'n_estimators':
        param_range = [50, 100, 200, 300, 500, 800]
    elif param == 'max_depth':
        param_range = [5, 10, 15, 20, 25, None]
    elif param == 'max_features':
        param_range = ['sqrt', 'log2', None, 0.3, 0.5, 0.8]
    
    # åˆ›å»ºåŸºç¡€æ¨¡å‹
    base_params = final_params.copy()
    base_params.pop(param, None)
    
    try:
        # å¤„ç†Noneå€¼çš„ç‰¹æ®Šæƒ…å†µ
        if param == 'max_depth' and None in param_range:
            # å¯¹äºmax_depthï¼Œç‰¹åˆ«å¤„ç†Noneå€¼
            param_range_for_validation = []
            param_labels = []
            for p in param_range:
                if p is None:
                    param_range_for_validation.append(100)  # ç”¨ä¸€ä¸ªå¤§æ•°ä»£æ›¿None
                    param_labels.append('None')
                else:
                    param_range_for_validation.append(p)
                    param_labels.append(str(p))
            
            train_scores, validation_scores = validation_curve(
                RandomForestClassifier(**{k: v for k, v in base_params.items() if k != 'max_depth'}, 
                                     random_state=42, n_jobs=-1),
                X_train, y_train,
                param_name='max_depth', 
                param_range=param_range,
                cv=cv_opt, scoring=balanced_scorer,
                n_jobs=-1
            )
        else:
            train_scores, validation_scores = validation_curve(
                RandomForestClassifier(**base_params, random_state=42, n_jobs=-1),
                X_train, y_train,
                param_name=param, 
                param_range=param_range,
                cv=cv_opt, scoring=balanced_scorer,
                n_jobs=-1
            )
            param_labels = [str(p) for p in param_range]
        
        train_mean = np.mean(train_scores, axis=1)
        train_std = np.std(train_scores, axis=1)
        validation_mean = np.mean(validation_scores, axis=1)
        validation_std = np.std(validation_scores, axis=1)
        
        x_axis = range(len(param_range))
        
        axes[i].plot(x_axis, train_mean, 'o-', color='blue', label='è®­ç»ƒé›†')
        axes[i].plot(x_axis, validation_mean, 'o-', color='red', label='éªŒè¯é›†')
        axes[i].fill_between(x_axis, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')
        axes[i].fill_between(x_axis, validation_mean - validation_std, validation_mean + validation_std, alpha=0.1, color='red')
        
        # è®¾ç½®xè½´æ ‡ç­¾
        axes[i].set_xticks(x_axis)
        axes[i].set_xticklabels(param_labels, rotation=45)
        
        # æ ‡è®°æœ€ä¼˜å€¼
        optimal_value = final_params.get(param)
        if optimal_value in param_range:
            optimal_idx = param_range.index(optimal_value)
            axes[i].axvline(x=optimal_idx, color='green', linestyle='--', alpha=0.7, 
                           label=f'æœ€ä¼˜å€¼: {optimal_value}')
        
        axes[i].set_xlabel(param)
        axes[i].set_ylabel('å¹³è¡¡å‡†ç¡®ç‡')
        axes[i].set_title(f'{param} éªŒè¯æ›²çº¿')
        axes[i].legend()
        axes[i].grid(True, alpha=0.3)
        
    except Exception as e:
        print(f"å‚æ•° {param} çš„éªŒè¯æ›²çº¿ç”Ÿæˆå¤±è´¥: {e}")
        axes[i].text(0.5, 0.5, f'{param}\néªŒè¯æ›²çº¿ç”Ÿæˆå¤±è´¥', 
                    ha='center', va='center', transform=axes[i].transAxes)

plt.tight_layout()
plt.savefig(os.path.join(PLOTS_DIR, 'rf_validation_curves.png'), dpi=300, bbox_inches='tight')
plt.close()

# =====================
# 11. è¯¦ç»†çš„NoCæ€§èƒ½åˆ†æ
# =====================
def detailed_noc_performance_analysis(y_true, y_pred, label_encoder):
    """è¯¦ç»†çš„NoCæ€§èƒ½åˆ†æ"""
    
    # è½¬æ¢ä¸ºåŸå§‹æ ‡ç­¾
    y_true_orig = label_encoder.inverse_transform(y_true)
    y_pred_orig = label_encoder.inverse_transform(y_pred)
    
    print("\n" + "="*80)
    print("                      å„è´¡çŒ®è€…äººæ•°è¯¦ç»†æ€§èƒ½åˆ†æ")
    print("="*80)
    
    # æ€»ä½“æ€§èƒ½
    overall_accuracy = accuracy_score(y_true_orig, y_pred_orig)
    print(f"\nğŸ“Š æ€»ä½“æ€§èƒ½æŒ‡æ ‡:")
    print(f"   æ•´ä½“å‡†ç¡®ç‡: {overall_accuracy:.4f} ({overall_accuracy*100:.2f}%)")
    
    # å„ç±»åˆ«è¯¦ç»†åˆ†æ
    unique_classes = sorted(list(set(y_true_orig)))
    
    print(f"\nğŸ“ˆ å„è´¡çŒ®è€…äººæ•°ç±»åˆ«è¯¦ç»†æ€§èƒ½:")
    print("-" * 100)
    print(f"{'è´¡çŒ®è€…äººæ•°':^12} {'æ ·æœ¬æ•°':^8} {'æ­£ç¡®é¢„æµ‹':^10} {'å‡†ç¡®ç‡':^10} {'ç²¾ç¡®ç‡':^10} {'å¬å›ç‡':^10} {'F1åˆ†æ•°':^10} {'æ€§èƒ½ç­‰çº§':^12}")
    print("-" * 100)
    
    # è®¡ç®—å„ç±»åˆ«æŒ‡æ ‡
    precision_scores = precision_score(y_true_orig, y_pred_orig, average=None, labels=unique_classes, zero_division=0)
    recall_scores = recall_score(y_true_orig, y_pred_orig, average=None, labels=unique_classes, zero_division=0)
    f1_scores = f1_score(y_true_orig, y_pred_orig, average=None, labels=unique_classes, zero_division=0)
    
    performance_data = []
    
    for i, noc in enumerate(unique_classes):
        # ç»Ÿè®¡ä¿¡æ¯
        true_mask = (y_true_orig == noc)
        total_samples = true_mask.sum()
        correct_predictions = ((y_true_orig == noc) & (y_pred_orig == noc)).sum()
        accuracy = correct_predictions / total_samples if total_samples > 0 else 0
        
        precision = precision_scores[i]
        recall = recall_scores[i]
        f1 = f1_scores[i]
        
        # æ€§èƒ½ç­‰çº§è¯„å®š
        if accuracy >= 0.95:
            grade = "ğŸŸ¢ ä¼˜ç§€"
        elif accuracy >= 0.85:
            grade = "ğŸŸ¡ è‰¯å¥½" 
        elif accuracy >= 0.70:
            grade = "ğŸŸ  ä¸€èˆ¬"
        elif accuracy >= 0.50:
            grade = "ğŸ”´ è¾ƒå·®"
        else:
            grade = "âš« å¾ˆå·®"
        
        print(f"{noc:^12}äºº {total_samples:^8} {correct_predictions:^10} {accuracy:^10.4f} {precision:^10.4f} {recall:^10.4f} {f1:^10.4f} {grade:^12}")
        
        performance_data.append({
            'noc': noc,
            'total_samples': total_samples,
            'correct_predictions': correct_predictions,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'grade': grade
        })
    
    print("-" * 100)
    
    # æ··æ·†çŸ©é˜µåˆ†æ
    print(f"\nğŸ” é¢„æµ‹é”™è¯¯è¯¦ç»†åˆ†æ:")
    cm = confusion_matrix(y_true_orig, y_pred_orig, labels=unique_classes)
    
    for i, true_noc in enumerate(unique_classes):
        errors = []
        for j, pred_noc in enumerate(unique_classes):
            if i != j and cm[i, j] > 0:
                error_rate = cm[i, j] / cm[i].sum() * 100
                errors.append(f"{pred_noc}äºº({cm[i, j]}æ¬¡, {error_rate:.1f}%)")
        
        if errors:
            print(f"   {true_noc}äºº â†’ è¯¯åˆ¤ä¸º: {', '.join(errors)}")
        else:
            print(f"   {true_noc}äºº â†’ æ— è¯¯åˆ¤ âœ…")
    
    # ç‰¹æ®Šå…³æ³¨å°‘æ•°ç±»
    print(f"\nâš ï¸  å°‘æ•°ç±»åˆ«ç‰¹åˆ«å…³æ³¨:")
    minority_classes = [data for data in performance_data if data['total_samples'] < 20]
    
    if minority_classes:
        for data in minority_classes:
            print(f"   {data['noc']}äººæ··åˆæ ·æœ¬ (æ ·æœ¬æ•°: {data['total_samples']}):")
            print(f"      å‡†ç¡®ç‡: {data['accuracy']:.4f} - {data['grade']}")
            if data['accuracy'] < 0.8:
                print(f"      âš ï¸  æ€§èƒ½åä½ï¼Œå»ºè®®:")
                print(f"         - å¢åŠ è®­ç»ƒæ ·æœ¬")
                print(f"         - è°ƒæ•´ç±»åˆ«æƒé‡") 
                print(f"         - ä½¿ç”¨ä¸“é—¨çš„å°‘æ•°ç±»å­¦ä¹ ç­–ç•¥")
    else:
        print("   æ‰€æœ‰ç±»åˆ«æ ·æœ¬æ•°é‡å……è¶³ âœ…")
    
    # å®è§‚æŒ‡æ ‡
    macro_precision = np.mean(precision_scores)
    macro_recall = np.mean(recall_scores) 
    macro_f1 = np.mean(f1_scores)
    
    weighted_precision = precision_score(y_true_orig, y_pred_orig, average='weighted', zero_division=0)
    weighted_recall = recall_score(y_true_orig, y_pred_orig, average='weighted', zero_division=0)
    weighted_f1 = f1_score(y_true_orig, y_pred_orig, average='weighted', zero_division=0)
    
    print(f"\nğŸ“Š å®è§‚æ€§èƒ½æŒ‡æ ‡:")
    print(f"   å®å¹³å‡ - ç²¾ç¡®ç‡: {macro_precision:.4f}, å¬å›ç‡: {macro_recall:.4f}, F1åˆ†æ•°: {macro_f1:.4f}")
    print(f"   åŠ æƒå¹³å‡ - ç²¾ç¡®ç‡: {weighted_precision:.4f}, å¬å›ç‡: {weighted_recall:.4f}, F1åˆ†æ•°: {weighted_f1:.4f}")
    
    return performance_data

# =====================
# 12. ç»“æœåˆ†æä¸å¯è§†åŒ–
# =====================
print("\n=== æ­¥éª¤7: ç»“æœåˆ†æä¸å¯è§†åŒ– ===")

# è°ƒç”¨è¯¦ç»†æ€§èƒ½åˆ†æ
performance_results = detailed_noc_performance_analysis(y_test, y_pred_rf, label_encoder)

# è½¬æ¢æ ‡ç­¾ç”¨äºæ˜¾ç¤º
y_test_orig = label_encoder.inverse_transform(y_test)
y_pred_orig = label_encoder.inverse_transform(y_pred_rf)

# åˆ†ç±»æŠ¥å‘Š
class_names = [f"{x}äºº" for x in sorted(label_encoder.classes_)]
print(f"\néšæœºæ£®æ—è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test_orig, y_pred_orig, target_names=class_names))

# æ··æ·†çŸ©é˜µå¯è§†åŒ–
plt.figure(figsize=(10, 8))
cm = confusion_matrix(y_test_orig, y_pred_orig)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
           xticklabels=class_names, yticklabels=class_names)
plt.title('ä¼˜åŒ–éšæœºæ£®æ—æ··æ·†çŸ©é˜µ')
plt.ylabel('çœŸå®NoC')
plt.xlabel('é¢„æµ‹NoC')
plt.tight_layout()
plt.savefig(os.path.join(PLOTS_DIR, 'æ··æ·†çŸ©é˜µ.png'), dpi=300, bbox_inches='tight')
plt.close()

# ç‰¹å¾é‡è¦æ€§åˆ†æ
plt.figure(figsize=(14, 10))
feature_importance = pd.DataFrame({
    'ç‰¹å¾': selected_features,
    'é‡è¦æ€§': optimal_rf_model.feature_importances_
}).sort_values('é‡è¦æ€§', ascending=False)

# æ˜¾ç¤ºæ‰€æœ‰é€‰æ‹©çš„ç‰¹å¾
sns.barplot(data=feature_importance, x='é‡è¦æ€§', y='ç‰¹å¾')
plt.title(f'éšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§æ’å (RFECVé€‰æ‹©çš„{len(selected_features)}ä¸ªç‰¹å¾)')
plt.xlabel('ç‰¹å¾é‡è¦æ€§')
plt.tight_layout()
plt.savefig(os.path.join(PLOTS_DIR, 'ç‰¹å¾é‡è¦æ€§.png'), dpi=300, bbox_inches='tight')
plt.close()

print(f"\néšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§æ’å:")
for idx, row in feature_importance.iterrows():
    print(f"  {row['ç‰¹å¾']:35} {row['é‡è¦æ€§']:.4f}")

# å­¦ä¹ æ›²çº¿
from sklearn.model_selection import learning_curve

try:
    train_sizes, train_scores, val_scores = learning_curve(
        optimal_rf_model, X_selected, y_encoded, cv=cv_opt, 
        train_sizes=np.linspace(0.1, 1.0, 10),
        scoring=balanced_scorer, random_state=42,
        n_jobs=-1
    )

    plt.figure(figsize=(12, 8))
    train_mean = np.mean(train_scores, axis=1)
    train_std = np.std(train_scores, axis=1)
    val_mean = np.mean(val_scores, axis=1)
    val_std = np.std(val_scores, axis=1)

    plt.plot(train_sizes, train_mean, 'o-', color='red', label='è®­ç»ƒé›†')
    plt.plot(train_sizes, val_mean, 'o-', color='green', label='éªŒè¯é›†')
    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='red')
    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='green')

    plt.xlabel('è®­ç»ƒæ ·æœ¬æ•°')
    plt.ylabel('å¹³è¡¡å‡†ç¡®ç‡')
    plt.title('ä¼˜åŒ–éšæœºæ£®æ—å­¦ä¹ æ›²çº¿')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(os.path.join(PLOTS_DIR, 'å­¦ä¹ æ›²çº¿.png'), dpi=300, bbox_inches='tight')
    plt.close()
except Exception as e:
    print(f"å­¦ä¹ æ›²çº¿ç”Ÿæˆå¤±è´¥: {e}")

# ä¼˜åŒ–è¿‡ç¨‹å¯è§†åŒ–
optimization_scores = [
    coarse_search.best_score_,
    fine_search.best_score_,
    final_search.best_score_
]

optimization_stages = ['ç²—è°ƒ', 'ç»†è°ƒ', 'æœ€ç»ˆå¾®è°ƒ']

plt.figure(figsize=(10, 6))
plt.plot(optimization_stages, optimization_scores, 'bo-', linewidth=2, markersize=8)
plt.xlabel('ä¼˜åŒ–é˜¶æ®µ')
plt.ylabel('äº¤å‰éªŒè¯åˆ†æ•°')
plt.title('éšæœºæ£®æ—å‚æ•°ä¼˜åŒ–è¿‡ç¨‹')
plt.grid(True, alpha=0.3)

for i, score in enumerate(optimization_scores):
    plt.text(i, score + 0.005, f'{score:.4f}', ha='center', va='bottom')

plt.tight_layout()
plt.savefig(os.path.join(PLOTS_DIR, 'ä¼˜åŒ–è¿‡ç¨‹.png'), dpi=300, bbox_inches='tight')
plt.close()

# =====================
# 13. SHAPå¯è§£é‡Šæ€§åˆ†æ
# =====================
if SHAP_AVAILABLE:
    print("\n=== æ­¥éª¤8: SHAPå¯è§£é‡Šæ€§åˆ†æ ===")
    
    try:
        # åˆ›å»ºSHAPè§£é‡Šå™¨
        explainer = shap.TreeExplainer(optimal_rf_model)
        
        # è®¡ç®—SHAPå€¼
        shap_sample_size = min(30, len(X_test))
        X_shap = X_test[:shap_sample_size]
        shap_values = explainer.shap_values(X_shap)
        
        # å¤„ç†å¤šåˆ†ç±»æƒ…å†µ
        if isinstance(shap_values, list):
            shap_values_mean = np.mean([np.abs(sv) for sv in shap_values], axis=0)
        else:
            shap_values_mean = np.abs(shap_values)
        
        # SHAPç‰¹å¾é‡è¦æ€§
        feature_shap_importance = np.mean(shap_values_mean, axis=0)
        shap_importance_df = pd.DataFrame({
            'ç‰¹å¾': selected_features,
            'SHAPé‡è¦æ€§': feature_shap_importance
        }).sort_values('SHAPé‡è¦æ€§', ascending=False)
        
        plt.figure(figsize=(12, 8))
        sns.barplot(data=shap_importance_df, x='SHAPé‡è¦æ€§', y='ç‰¹å¾')
        plt.title('SHAPç‰¹å¾é‡è¦æ€§æ’å')
        plt.xlabel('å¹³å‡SHAPé‡è¦æ€§')
        plt.tight_layout()
        plt.savefig(os.path.join(PLOTS_DIR, 'SHAPé‡è¦æ€§.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        print("SHAPç‰¹å¾é‡è¦æ€§æ’å:")
        for idx, row in shap_importance_df.iterrows():
            print(f"  {row['ç‰¹å¾']:35} {row['SHAPé‡è¦æ€§']:.4f}")
        
        # SHAPæ‘˜è¦å›¾
        try:
            if isinstance(shap_values, list):
                shap_values_plot = shap_values[0]
            else:
                shap_values_plot = shap_values
                
            plt.figure(figsize=(10, 8))
            shap.summary_plot(shap_values_plot, X_shap, feature_names=selected_features, 
                            plot_type="bar", show=False)
            plt.title("SHAPæ‘˜è¦å›¾")
            plt.tight_layout()
            plt.savefig(os.path.join(PLOTS_DIR, 'SHAPæ‘˜è¦å›¾.png'), dpi=300, bbox_inches='tight')
            plt.close()
        except Exception as e:
            print(f"SHAPæ‘˜è¦å›¾ç”Ÿæˆå¤±è´¥: {e}")
            
    except Exception as e:
        print(f"SHAPåˆ†æå¤±è´¥: {e}")

# =====================
# 14. æ¨¡å‹é¢„æµ‹ä¸ä¿å­˜
# =====================
print("\n=== æ­¥éª¤9: æ¨¡å‹é¢„æµ‹ä¸ä¿å­˜ ===")

# å¯¹æ‰€æœ‰æ ·æœ¬è¿›è¡Œé¢„æµ‹
y_pred_all = optimal_rf_model.predict(X_selected)
y_pred_all_orig = label_encoder.inverse_transform(y_pred_all)

# æ·»åŠ é¢„æµ‹ç»“æœåˆ°ç‰¹å¾æ•°æ®æ¡†
df_features['é¢„æµ‹è´¡çŒ®è€…äººæ•°'] = y_pred_all_orig

# è®¡ç®—æ•´ä½“å‡†ç¡®ç‡
overall_accuracy = (df_features['é¢„æµ‹è´¡çŒ®è€…äººæ•°'] == df_features['è´¡çŒ®è€…äººæ•°']).mean()
print(f"æ•´ä½“é¢„æµ‹å‡†ç¡®ç‡: {overall_accuracy:.4f}")

# å„NoCç±»åˆ«çš„å‡†ç¡®ç‡
noc_accuracy = df_features.groupby('è´¡çŒ®è€…äººæ•°').apply(
    lambda x: (x['é¢„æµ‹è´¡çŒ®è€…äººæ•°'] == x['è´¡çŒ®è€…äººæ•°']).mean()
).reset_index(name='å‡†ç¡®ç‡')

print("\nå„NoCç±»åˆ«é¢„æµ‹å‡†ç¡®ç‡:")
for _, row in noc_accuracy.iterrows():
    print(f"  {int(row['è´¡çŒ®è€…äººæ•°'])}äºº: {row['å‡†ç¡®ç‡']:.4f}")

# å¯è§†åŒ–å„ç±»åˆ«å‡†ç¡®ç‡
plt.figure(figsize=(10, 6))
sns.barplot(data=noc_accuracy, x='è´¡çŒ®è€…äººæ•°', y='å‡†ç¡®ç‡')
plt.ylim(0, 1.1)
plt.xlabel('çœŸå®NoC')
plt.ylabel('é¢„æµ‹å‡†ç¡®ç‡')
plt.title('å„NoCç±»åˆ«é¢„æµ‹å‡†ç¡®ç‡')

for i, row in noc_accuracy.iterrows():
    plt.text(i, row['å‡†ç¡®ç‡'] + 0.03, f"{row['å‡†ç¡®ç‡']:.3f}", 
             ha='center', va='bottom')

plt.tight_layout()
plt.savefig(os.path.join(PLOTS_DIR, 'å„ç±»åˆ«å‡†ç¡®ç‡.png'), dpi=300, bbox_inches='tight')
plt.close()

# ä¿å­˜ç»“æœ
df_features.to_csv(os.path.join(DATA_DIR, 'NoCè¯†åˆ«ç»“æœ_RFECV_RFä¼˜åŒ–ç‰ˆ.csv'), 
                   index=False, encoding='utf-8-sig')

# ä¿å­˜æ¨¡å‹
import joblib
model_filename = os.path.join(DATA_DIR, 'noc_optimized_random_forest_model.pkl')
joblib.dump({
    'model': optimal_rf_model,
    'scaler': scaler,
    'label_encoder': label_encoder,
    'selected_features': selected_features,
    'selected_indices': selected_indices,
    'rfecv': rfecv,
    'optimization_history': {
        'coarse_best': coarse_search.best_params_,
        'fine_best': fine_search.best_params_,
        'final_best': final_search.best_params_,
        'scores': optimization_scores
    }
}, model_filename)

print(f"ä¼˜åŒ–æ¨¡å‹å·²ä¿å­˜è‡³: {model_filename}")

# ä¿å­˜è¯¦ç»†æ‘˜è¦
summary = {
    'æ¨¡å‹ä¿¡æ¯': {
        'æ¨¡å‹ç±»å‹': 'OptimizedRandomForestClassifier',
        'ç‰¹å¾é€‰æ‹©æ–¹æ³•': 'RFECV',
        'ä¼˜åŒ–é˜¶æ®µ': ['ç²—è°ƒ', 'ç»†è°ƒ', 'æœ€ç»ˆå¾®è°ƒ'],
        'æµ‹è¯•é›†å‡†ç¡®ç‡': float(rf_accuracy),
        'å¹³è¡¡å‡†ç¡®ç‡': float(balanced_acc),
        'åŠ æƒF1åˆ†æ•°': float(f1_weighted),
        'æ•´ä½“å‡†ç¡®ç‡': float(overall_accuracy)
    },
    'ç‰¹å¾é€‰æ‹©ç»“æœ': {
        'åŸå§‹ç‰¹å¾æ•°': len(feature_cols),
        'æœ€ç»ˆç‰¹å¾æ•°': len(selected_features),
        'RFECVæœ€ä¼˜ç‰¹å¾æ•°': int(rfecv.n_features_),
        'RFECVæœ€ä½³åˆ†æ•°': float(best_score),
        'é€‰æ‹©çš„ç‰¹å¾': selected_features
    },
    'ä¼˜åŒ–è¿‡ç¨‹': {
        'ç²—è°ƒæœ€ä½³å‚æ•°': coarse_search.best_params_,
        'ç²—è°ƒæœ€ä½³åˆ†æ•°': float(coarse_search.best_score_),
        'ç»†è°ƒæœ€ä½³å‚æ•°': fine_search.best_params_,
        'ç»†è°ƒæœ€ä½³åˆ†æ•°': float(fine_search.best_score_),
        'æœ€ç»ˆæœ€ä½³å‚æ•°': final_search.best_params_,
        'æœ€ç»ˆæœ€ä½³åˆ†æ•°': float(final_search.best_score_)
    },
    'æœ€ç»ˆæ¨¡å‹å‚æ•°': final_params,
    'æ•°æ®ä¿¡æ¯': {
        'æ€»æ ·æœ¬æ•°': len(df_features),
        'NoCåˆ†å¸ƒ': df_features['è´¡çŒ®è€…äººæ•°'].value_counts().sort_index().to_dict(),
        'è®­ç»ƒé›†å¤§å°': len(X_train),
        'æµ‹è¯•é›†å¤§å°': len(X_test)
    },
    'å„ç±»åˆ«å‡†ç¡®ç‡': {
        int(row['è´¡çŒ®è€…äººæ•°']): float(row['å‡†ç¡®ç‡']) 
        for _, row in noc_accuracy.iterrows()
    },
    'ç‰¹å¾é‡è¦æ€§å‰10': [
        {
            'ç‰¹å¾': row['ç‰¹å¾'],
            'é‡è¦æ€§': float(row['é‡è¦æ€§'])
        }
        for _, row in feature_importance.head(10).iterrows()
    ],
    'æ—¶é—´æˆ³': pd.Timestamp.now().strftime("%Y-%m-%d %H:%M:%S")
}

# å¦‚æœæœ‰OOBåˆ†æ•°ï¼Œæ·»åŠ åˆ°æ‘˜è¦ä¸­
if hasattr(optimal_rf_model, 'oob_score_') and optimal_rf_model.oob_score_ is not None:
    summary['æ¨¡å‹ä¿¡æ¯']['OOBè¯„ä¼°åˆ†æ•°'] = float(optimal_rf_model.oob_score_)

with open(os.path.join(DATA_DIR, 'NoCåˆ†ææ‘˜è¦_RFECV_RFä¼˜åŒ–ç‰ˆ.json'), 'w', encoding='utf-8') as f:
    json.dump(summary, f, ensure_ascii=False, indent=2)

# =====================
# 15. æœ€ç»ˆæŠ¥å‘Š
# =====================
print("\n" + "="*80)
print("         æ³•åŒ»æ··åˆSTRå›¾è°±NoCè¯†åˆ« - RFECV+éšæœºæ£®æ—ä¼˜åŒ–ç‰ˆæœ€ç»ˆæŠ¥å‘Š")
print("="*80)

print(f"\nğŸ“Š æ•°æ®æ¦‚å†µ:")
print(f"   â€¢ æ€»æ ·æœ¬æ•°: {len(df_features)}")
print(f"   â€¢ NoCåˆ†å¸ƒ: {dict(df_features['è´¡çŒ®è€…äººæ•°'].value_counts().sort_index())}")
print(f"   â€¢ åŸå§‹ç‰¹å¾æ•°: {len(feature_cols)}")
print(f"   â€¢ RFECVé€‰æ‹©ç‰¹å¾æ•°: {len(selected_features)}")

print(f"\nğŸ”§ æŠ€æœ¯åˆ›æ–°:")
print(f"   â€¢ ä½¿ç”¨RFECVé€’å½’ç‰¹å¾æ¶ˆé™¤è¿›è¡Œç‰¹å¾é€‰æ‹©")
print(f"   â€¢ ä¸‰é˜¶æ®µéšæœºæ£®æ—å‚æ•°ä¼˜åŒ–ï¼ˆç²—è°ƒâ†’ç»†è°ƒâ†’å¾®è°ƒï¼‰")
print(f"   â€¢ å¹³è¡¡å‡†ç¡®ç‡ä½œä¸ºä¼˜åŒ–ç›®æ ‡ï¼Œå¤„ç†ç±»åˆ«ä¸å¹³è¡¡")
print(f"   â€¢ éªŒè¯æ›²çº¿åˆ†æå…³é”®è¶…å‚æ•°å½±å“")
print(f"   â€¢ åˆ©ç”¨è¢‹å¤–(OOB)è¯„ä¼°æé«˜æ¨¡å‹å¯é æ€§")

print(f"\nğŸ¯ RFECVç‰¹å¾é€‰æ‹©ç»“æœ:")
print(f"   â€¢ æœ€ä¼˜ç‰¹å¾æ•°: {rfecv.n_features_}")
print(f"   â€¢ æœ€ä½³äº¤å‰éªŒè¯åˆ†æ•°: {best_score:.4f}")
print(f"   â€¢ ç‰¹å¾å‡å°‘ç‡: {(1 - len(selected_features)/len(feature_cols)):.1%}")

print(f"\nğŸš€ éšæœºæ£®æ—ä¼˜åŒ–å†ç¨‹:")
print(f"   â€¢ ç²—è°ƒé˜¶æ®µ: {coarse_search.best_score_:.4f}")
print(f"   â€¢ ç»†è°ƒé˜¶æ®µ: {fine_search.best_score_:.4f}")
print(f"   â€¢ æœ€ç»ˆå¾®è°ƒé˜¶æ®µ: {final_search.best_score_:.4f}")
print(f"   â€¢ æœ€ç»ˆæ¨¡å‹å‚æ•°: {final_params}")
print(f"   â€¢ æµ‹è¯•é›†å‡†ç¡®ç‡: {rf_accuracy:.4f}")
print(f"   â€¢ å¹³è¡¡å‡†ç¡®ç‡: {balanced_acc:.4f}")
print(f"   â€¢ åŠ æƒF1åˆ†æ•°: {f1_weighted:.4f}")
print(f"   â€¢ æ•´ä½“é¢„æµ‹å‡†ç¡®ç‡: {overall_accuracy:.4f}")

if hasattr(optimal_rf_model, 'oob_score_') and optimal_rf_model.oob_score_ is not None:
    print(f"   â€¢ è¢‹å¤–è¯„ä¼°åˆ†æ•°: {optimal_rf_model.oob_score_:.4f}")

print(f"\nğŸ“ˆ å„ç±»åˆ«æ€§èƒ½è¡¨ç°:")
performance_summary = []
for _, row in noc_accuracy.iterrows():
    noc = int(row['è´¡çŒ®è€…äººæ•°'])
    acc = row['å‡†ç¡®ç‡']
    sample_count = len(df_features[df_features['è´¡çŒ®è€…äººæ•°'] == noc])
    
    if acc >= 0.9:
        performance = "ğŸŸ¢ ä¼˜ç§€"
    elif acc >= 0.8:
        performance = "ğŸŸ¡ è‰¯å¥½"
    elif acc >= 0.6:
        performance = "ğŸŸ  ä¸€èˆ¬"
    else:
        performance = "ğŸ”´ éœ€æ”¹è¿›"
    
    print(f"   â€¢ {noc}äººæ··åˆæ ·æœ¬: {acc:.4f} ({acc*100:.1f}%) - {performance} ({sample_count}ä¸ªæ ·æœ¬)")
    performance_summary.append((noc, acc, sample_count))

print(f"\nğŸ” å‰5ä½æœ€é‡è¦ç‰¹å¾:")
top_5_features = feature_importance.head(5)
for i, (_, row) in enumerate(top_5_features.iterrows(), 1):
    feature_cn = FEATURE_NAME_MAPPING.get(row['ç‰¹å¾'], row['ç‰¹å¾'])
    print(f"   {i}. {feature_cn:<25} (é‡è¦æ€§: {row['é‡è¦æ€§']:.4f})")

print(f"\nğŸ“‹ æ¨¡å‹ç‰¹ç‚¹è¯´æ˜:")
print(f"   â€¢ åŸºäº {len(selected_features)} ä¸ªç²¾é€‰ç”Ÿç‰©ç‰¹å¾è¿›è¡Œé¢„æµ‹")
print(f"   â€¢ ç‰¹å¾æ¶µç›–: å›¾è°±ç»Ÿè®¡ç‰¹æ€§ã€å³°é«˜åˆ†å¸ƒç‰¹å¾ã€ä½ç‚¹å¹³è¡¡æ€§ã€")
print(f"     ä¿¡æ¯ç†µæŒ‡æ ‡ã€DNAé™è§£æ ‡å¿—ç­‰å¤šä¸ªç»´åº¦")
print(f"   â€¢ é‡‡ç”¨éšæœºæ£®æ—ç®—æ³•ï¼Œåˆ©ç”¨Bootstrapèšåˆå’Œç‰¹å¾éšæœºé‡‡æ ·")
print(f"   â€¢ é’ˆå¯¹å°‘æ•°ç±»æ ·æœ¬è¿›è¡Œç‰¹æ®Šä¼˜åŒ–ï¼Œä½¿ç”¨ç±»åˆ«æƒé‡å¹³è¡¡")
print(f"   â€¢ æ•´ä½“é¢„æµ‹å‡†ç¡®ç‡è¾¾åˆ° {overall_accuracy:.1%}ï¼Œå…·æœ‰è¾ƒå¥½çš„å®ç”¨ä»·å€¼")

# æ•°æ®è´¨é‡è¯„ä¼°
print(f"\nğŸ“Š æ•°æ®è´¨é‡è¯„ä¼°:")
noc_distribution = df_features['è´¡çŒ®è€…äººæ•°'].value_counts().sort_index()
max_samples = noc_distribution.max()
min_samples = noc_distribution.min()
imbalance_ratio = max_samples / min_samples

print(f"   â€¢ æ ·æœ¬ä¸å¹³è¡¡ç¨‹åº¦: {imbalance_ratio:.1f}:1")
if imbalance_ratio > 10:
    print(f"   â€¢ âš ï¸  æ•°æ®ä¸¥é‡ä¸å¹³è¡¡ï¼Œå·²é‡‡ç”¨åŠ æƒé‡‡æ ·ç­–ç•¥")
elif imbalance_ratio > 5:
    print(f"   â€¢ âš ï¸  æ•°æ®ä¸­åº¦ä¸å¹³è¡¡ï¼Œå·²é‡‡ç”¨æƒé‡å¹³è¡¡ç­–ç•¥")
else:
    print(f"   â€¢ âœ… æ•°æ®å¹³è¡¡æ€§è‰¯å¥½")

# éšæœºæ£®æ—ç‰¹æœ‰ä¼˜åŠ¿
print(f"\nğŸŒ² éšæœºæ£®æ—ç®—æ³•ä¼˜åŠ¿:")
print(f"   â€¢ Bootstrapèšåˆå‡å°‘è¿‡æ‹Ÿåˆé£é™©")
print(f"   â€¢ ç‰¹å¾éšæœºé‡‡æ ·æé«˜æ³›åŒ–èƒ½åŠ›")
print(f"   â€¢ å¯¹å™ªå£°å’Œå¼‚å¸¸å€¼å…·æœ‰è¾ƒå¼ºé²æ£’æ€§")
print(f"   â€¢ æä¾›ç‰¹å¾é‡è¦æ€§æ’åï¼Œå¢å¼ºå¯è§£é‡Šæ€§")
if hasattr(optimal_rf_model, 'oob_score_') and optimal_rf_model.oob_score_ is not None:
    print(f"   â€¢ è¢‹å¤–è¯„ä¼°æä¾›æ— åæ€§èƒ½ä¼°è®¡")

# æ”¹è¿›å»ºè®®
print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
low_performance_classes = [noc for noc, acc, _ in performance_summary if acc < 0.8]
if low_performance_classes:
    print(f"   â€¢ é’ˆå¯¹ {', '.join(map(str, low_performance_classes))} äººæ··åˆæ ·æœ¬:")
    print(f"     - å¢åŠ è®­ç»ƒæ ·æœ¬æ•°é‡")
    print(f"     - è°ƒæ•´éšæœºæ£®æ—çš„class_weightå‚æ•°")
    print(f"     - è€ƒè™‘ä½¿ç”¨cost-sensitive learning")
    print(f"     - å°è¯•é›†æˆå­¦ä¹ æ–¹æ³•")
else:
    print(f"   â€¢ âœ… æ‰€æœ‰ç±»åˆ«æ€§èƒ½å‡è¾¾åˆ°è‰¯å¥½æ°´å¹³")

print(f"\nğŸ’¾ è¾“å‡ºæ–‡ä»¶:")
print(f"   â€¢ ç‰¹å¾æ•°æ®æ–‡ä»¶: NoCè¯†åˆ«ç»“æœ_RFECV_RFä¼˜åŒ–ç‰ˆ.csv")
print(f"   â€¢ æ¨¡å‹æ€§èƒ½æ‘˜è¦: NoCåˆ†ææ‘˜è¦_RFECV_RFä¼˜åŒ–ç‰ˆ.json")
print(f"   â€¢ è®­ç»ƒå¥½çš„æ¨¡å‹: noc_optimized_random_forest_model.pkl")
print(f"   â€¢ å›¾è¡¨è¾“å‡ºç›®å½•: {PLOTS_DIR}")

if SHAP_AVAILABLE:
    print(f"   â€¢ SHAPå¯è§£é‡Šæ€§åˆ†æå›¾è¡¨å·²ç”Ÿæˆï¼Œæå‡æ¨¡å‹é€æ˜åº¦")

print(f"\nâ° åˆ†æå®Œæˆæ—¶é—´: {pd.Timestamp.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}")
print("\nâœ… æ³•åŒ»æ··åˆSTRå›¾è°±NoCæ™ºèƒ½è¯†åˆ«åˆ†æå®Œæˆï¼")
print("="*80)