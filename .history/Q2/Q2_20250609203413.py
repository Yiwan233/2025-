# -*- coding: utf-8 -*-
"""
é—®é¢˜äºŒä¼˜åŒ–ç‰ˆï¼šåŸºäºV5ç‰¹å¾çš„æ™ºèƒ½å…ˆéªŒæ··åˆæ¯”ä¾‹æ¨æ–­ç³»ç»Ÿ
ç‰ˆæœ¬: V2.0 - Enhanced Priors with V5 Feature-Driven Informative Priors
æ ¸å¿ƒæ”¹è¿›ï¼š
1. V5ç‰¹å¾é©±åŠ¨çš„åŠ¨æ€å…ˆéªŒè°ƒæ•´
2. æ··åˆæ¯”ä¾‹çš„ä¸å¹³è¡¡æ€§é¢„æµ‹
3. è´¡çŒ®è€…ä¸»æ¬¡å…³ç³»çš„å…ˆéªŒå»ºæ¨¡
4. å¢å¼ºçš„MCMCé‡‡æ ·ç­–ç•¥
"""

import numpy as np
import pandas as pd
from scipy import stats
from scipy.special import gammaln
import logging
from typing import Dict, List, Tuple, Optional
import matplotlib.pyplot as plt

logger = logging.getLogger(__name__)

class EnhancedPriorCalculator:
    """å¢å¼ºçš„å…ˆéªŒè®¡ç®—å™¨ï¼šåŸºäºV5ç‰¹å¾åŠ¨æ€è°ƒæ•´æ··åˆæ¯”ä¾‹å…ˆéªŒ"""
    
    def __init__(self):
        self.feature_weights = {
            # ä¸å¹³è¡¡æ€§æŒ‡æ ‡
            'skewness_peak_height': 0.3,           # å³°é«˜åˆ†å¸ƒååº¦
            'ratio_severe_imbalance_loci': 0.25,   # ä¸¥é‡å¤±è¡¡ä½ç‚¹æ¯”ä¾‹
            'avg_locus_allele_entropy': 0.2,       # å¹³å‡ä½ç‚¹ç­‰ä½åŸºå› ç†µ
            'inter_locus_balance_entropy': 0.15,   # ä½ç‚¹é—´å¹³è¡¡ç†µ
            'mac_profile': 0.1                     # æ ·æœ¬æœ€å¤§ç­‰ä½åŸºå› æ•°
        }
        
        # æ··åˆæ¨¡å¼åˆ†ç±»é˜ˆå€¼
        self.thresholds = {
            'highly_imbalanced': 0.75,    # é«˜åº¦ä¸å¹³è¡¡
            'moderately_imbalanced': 0.5, # ä¸­åº¦ä¸å¹³è¡¡
            'balanced': 0.25              # ç›¸å¯¹å¹³è¡¡
        }
        
        logger.info("å¢å¼ºå…ˆéªŒè®¡ç®—å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def predict_mixture_pattern(self, v5_features: Dict) -> Dict:
        """åŸºäºV5ç‰¹å¾é¢„æµ‹æ··åˆæ¨¡å¼"""
        
        # 1. è®¡ç®—ä¸å¹³è¡¡æ€§æŒ‡æ ‡
        imbalance_score = 0.0
        feature_contributions = {}
        
        # å³°é«˜åˆ†å¸ƒååº¦ (è¶Šå¤§è¶Šä¸å¹³è¡¡)
        skewness = abs(v5_features.get('skewness_peak_height', 0.0))
        skew_score = min(1.0, skewness / 2.0)  # æ ‡å‡†åŒ–åˆ°[0,1]
        imbalance_score += skew_score * self.feature_weights['skewness_peak_height']
        feature_contributions['skewness'] = skew_score
        
        # ä¸¥é‡å¤±è¡¡ä½ç‚¹æ¯”ä¾‹ (è¶Šå¤§è¶Šä¸å¹³è¡¡)
        severe_imbalance_ratio = v5_features.get('ratio_severe_imbalance_loci', 0.0)
        imbalance_score += severe_imbalance_ratio * self.feature_weights['ratio_severe_imbalance_loci']
        feature_contributions['severe_imbalance'] = severe_imbalance_ratio
        
        # å¹³å‡ä½ç‚¹ç­‰ä½åŸºå› ç†µ (è¶Šå°è¶Šä¸å¹³è¡¡)
        avg_entropy = v5_features.get('avg_locus_allele_entropy', 1.0)
        max_entropy = np.log(4)  # å‡è®¾æœ€å¤š4ä¸ªç­‰ä½åŸºå› 
        entropy_score = 1.0 - min(1.0, avg_entropy / max_entropy)
        imbalance_score += entropy_score * self.feature_weights['avg_locus_allele_entropy']
        feature_contributions['low_entropy'] = entropy_score
        
        # ä½ç‚¹é—´å¹³è¡¡ç†µ (è¶Šå°è¶Šä¸å¹³è¡¡)
        inter_entropy = v5_features.get('inter_locus_balance_entropy', 1.0)
        max_inter_entropy = np.log(20)  # å‡è®¾20ä¸ªä½ç‚¹
        inter_entropy_score = 1.0 - min(1.0, inter_entropy / max_inter_entropy)
        imbalance_score += inter_entropy_score * self.feature_weights['inter_locus_balance_entropy']
        feature_contributions['low_inter_entropy'] = inter_entropy_score
        
        # æœ€å¤§ç­‰ä½åŸºå› æ•° (è¶Šå¤§å¯èƒ½è¶Šä¸å¹³è¡¡)
        mac = v5_features.get('mac_profile', 2)
        mac_score = min(1.0, (mac - 2) / 6)  # æ ‡å‡†åŒ–ï¼Œ2ä¸ªç­‰ä½åŸºå› æ˜¯æœ€å¹³è¡¡çš„
        imbalance_score += mac_score * self.feature_weights['mac_profile']
        feature_contributions['high_mac'] = mac_score
        
        # 2. åˆ†ç±»æ··åˆæ¨¡å¼
        if imbalance_score >= self.thresholds['highly_imbalanced']:
            mixture_pattern = 'highly_imbalanced'
            pattern_description = "é«˜åº¦ä¸å¹³è¡¡æ··åˆï¼ˆä¸»è¦è´¡çŒ®è€…å ä¼˜ï¼‰"
        elif imbalance_score >= self.thresholds['moderately_imbalanced']:
            mixture_pattern = 'moderately_imbalanced'
            pattern_description = "ä¸­åº¦ä¸å¹³è¡¡æ··åˆï¼ˆå­˜åœ¨ä¸»æ¬¡è´¡çŒ®è€…ï¼‰"
        else:
            mixture_pattern = 'balanced'
            pattern_description = "ç›¸å¯¹å¹³è¡¡æ··åˆï¼ˆè´¡çŒ®è€…æ¯”ä¾‹ç›¸è¿‘ï¼‰"
        
        # 3. é¢„æµ‹ä¸»è¦è´¡çŒ®è€…æ¯”ä¾‹
        if mixture_pattern == 'highly_imbalanced':
            predicted_major_ratio = 0.7 + 0.2 * (imbalance_score - self.thresholds['highly_imbalanced']) / (1.0 - self.thresholds['highly_imbalanced'])
        elif mixture_pattern == 'moderately_imbalanced':
            predicted_major_ratio = 0.5 + 0.2 * (imbalance_score - self.thresholds['moderately_imbalanced']) / (self.thresholds['highly_imbalanced'] - self.thresholds['moderately_imbalanced'])
        else:
            predicted_major_ratio = 0.4 + 0.1 * imbalance_score / self.thresholds['balanced']
        
        return {
            'mixture_pattern': mixture_pattern,
            'pattern_description': pattern_description,
            'imbalance_score': imbalance_score,
            'predicted_major_ratio': predicted_major_ratio,
            'feature_contributions': feature_contributions,
            'confidence': min(1.0, imbalance_score * 2)  # é¢„æµ‹ç½®ä¿¡åº¦
        }
    
    def calculate_informative_prior_alpha(self, N: int, mixture_prediction: Dict) -> np.ndarray:
        """åŸºäºæ··åˆæ¨¡å¼é¢„æµ‹è®¡ç®—Dirichletå…ˆéªŒçš„alphaå‚æ•°"""
        
        pattern = mixture_prediction['mixture_pattern']
        predicted_major_ratio = mixture_prediction['predicted_major_ratio']
        confidence = mixture_prediction['confidence']
        
        # åŸºç¡€alphaå€¼
        base_alpha = 1.0
        
        if pattern == 'highly_imbalanced':
            # é«˜åº¦ä¸å¹³è¡¡ï¼šå¼ºçƒˆå€¾å‘äºä¸€ä¸ªä¸»è¦è´¡çŒ®è€…
            alpha = np.ones(N) * 0.3  # æ¬¡è¦è´¡çŒ®è€…çš„ä½alpha
            alpha[0] = base_alpha * 3.0  # ä¸»è¦è´¡çŒ®è€…çš„é«˜alpha
            
            # è¿›ä¸€æ­¥è°ƒæ•´ä»¥åæ˜ é¢„æµ‹çš„ä¸»è¦æ¯”ä¾‹
            concentration_factor = confidence * 2.0
            alpha[0] = alpha[0] * (1 + concentration_factor)
            
        elif pattern == 'moderately_imbalanced':
            # ä¸­åº¦ä¸å¹³è¡¡ï¼šå­˜åœ¨ä¸»æ¬¡å…³ç³»ä½†ä¸æç«¯
            alpha = np.ones(N) * 0.7
            alpha[0] = base_alpha * 1.8
            
            # å¦‚æœN=2ï¼Œè°ƒæ•´ç¬¬äºŒä¸ªè´¡çŒ®è€…
            if N == 2:
                ratio_factor = predicted_major_ratio / (1 - predicted_major_ratio)
                alpha[0] = base_alpha * ratio_factor
                alpha[1] = base_alpha
            
        else:
            # ç›¸å¯¹å¹³è¡¡ï¼šä½¿ç”¨æ¥è¿‘å‡åŒ€çš„å…ˆéªŒï¼Œä½†ä»ç•¥æœ‰åå‘
            alpha = np.ones(N) * 0.8
            alpha[0] = base_alpha * 1.2
        
        # åº”ç”¨ç½®ä¿¡åº¦è°ƒæ•´
        concentration_adjustment = 1 + confidence
        alpha = alpha * concentration_adjustment
        
        # ç¡®ä¿alphaå€¼åœ¨åˆç†èŒƒå›´å†…
        alpha = np.clip(alpha, 0.1, 10.0)
        
        logger.info(f"è®¡ç®—å¾—åˆ°çš„Dirichletå…ˆéªŒalphaå‚æ•°: {alpha}")
        logger.info(f"å¯¹åº”çš„æœŸæœ›æ¯”ä¾‹: {alpha / np.sum(alpha)}")
        
        return alpha
    
    def calculate_enhanced_prior_log_probability(self, mixture_ratios: np.ndarray, 
                                               alpha: np.ndarray) -> float:
        """è®¡ç®—å¢å¼ºçš„æ··åˆæ¯”ä¾‹å…ˆéªŒå¯¹æ•°æ¦‚ç‡"""
        
        # Dirichletåˆ†å¸ƒçš„å¯¹æ•°æ¦‚ç‡å¯†åº¦
        log_prior = (gammaln(np.sum(alpha)) - np.sum(gammaln(alpha)) + 
                    np.sum((alpha - 1) * np.log(mixture_ratios + 1e-10)))
        
        return log_prior
    
    def sample_informative_prior(self, alpha: np.ndarray, n_samples: int = 1000) -> np.ndarray:
        """ä»ä¿¡æ¯å…ˆéªŒä¸­é‡‡æ ·ï¼Œç”¨äºéªŒè¯å…ˆéªŒåˆç†æ€§"""
        samples = np.random.dirichlet(alpha, n_samples)
        return samples
    
    def visualize_prior_distribution(self, alpha: np.ndarray, title: str = "å…ˆéªŒåˆ†å¸ƒ"):
        """å¯è§†åŒ–å…ˆéªŒåˆ†å¸ƒ"""
        n_samples = 5000
        samples = self.sample_informative_prior(alpha, n_samples)
        N = len(alpha)
        
        fig, axes = plt.subplots(1, N, figsize=(4*N, 4))
        if N == 1:
            axes = [axes]
        
        for i in range(N):
            axes[i].hist(samples[:, i], bins=50, alpha=0.7, density=True, 
                        color=f'C{i}', edgecolor='black', linewidth=0.5)
            axes[i].set_title(f'Mx_{i+1} å…ˆéªŒåˆ†å¸ƒ')
            axes[i].set_xlabel(f'Mx_{i+1}')
            axes[i].set_ylabel('æ¦‚ç‡å¯†åº¦')
            
            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
            mean_val = np.mean(samples[:, i])
            median_val = np.median(samples[:, i])
            axes[i].axvline(mean_val, color='red', linestyle='--', 
                           label=f'å‡å€¼: {mean_val:.3f}')
            axes[i].axvline(median_val, color='green', linestyle='--', 
                           label=f'ä¸­ä½æ•°: {median_val:.3f}')
            axes[i].legend()
            axes[i].grid(True, alpha=0.3)
        
        plt.suptitle(title, fontsize=16)
        plt.tight_layout()
        return fig

class EnhancedMGM_RF_Inferencer:
    """å¢å¼ºçš„MGM-RFæ¨æ–­å™¨ï¼Œé›†æˆæ™ºèƒ½å…ˆéªŒç³»ç»Ÿ"""
    
    def __init__(self, original_inferencer, enhanced_prior_calculator):
        # ç»§æ‰¿åŸå§‹æ¨æ–­å™¨çš„æ‰€æœ‰åŠŸèƒ½
        self.__dict__.update(original_inferencer.__dict__)
        
        # æ·»åŠ å¢å¼ºçš„å…ˆéªŒè®¡ç®—å™¨
        self.enhanced_prior_calculator = enhanced_prior_calculator
        self.current_alpha = None
        self.mixture_prediction = None
        
        logger.info("å¢å¼ºMGM-RFæ¨æ–­å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def predict_and_set_informative_prior(self, v5_features: Dict, N: int):
        """é¢„æµ‹æ··åˆæ¨¡å¼å¹¶è®¾ç½®ä¿¡æ¯å…ˆéªŒ"""
        
        # é¢„æµ‹æ··åˆæ¨¡å¼
        self.mixture_prediction = self.enhanced_prior_calculator.predict_mixture_pattern(v5_features)
        
        # è®¡ç®—å…ˆéªŒå‚æ•°
        self.current_alpha = self.enhanced_prior_calculator.calculate_informative_prior_alpha(
            N, self.mixture_prediction)
        
        logger.info(f"æ··åˆæ¨¡å¼é¢„æµ‹: {self.mixture_prediction['pattern_description']}")
        logger.info(f"ä¸å¹³è¡¡è¯„åˆ†: {self.mixture_prediction['imbalance_score']:.3f}")
        logger.info(f"é¢„æµ‹ä¸»è¦è´¡çŒ®è€…æ¯”ä¾‹: {self.mixture_prediction['predicted_major_ratio']:.3f}")
        logger.info(f"é¢„æµ‹ç½®ä¿¡åº¦: {self.mixture_prediction['confidence']:.3f}")
    
    def calculate_enhanced_prior_mixture_ratios(self, mixture_ratios: np.ndarray) -> float:
        """è®¡ç®—å¢å¼ºçš„æ··åˆæ¯”ä¾‹å…ˆéªŒæ¦‚ç‡"""
        if self.current_alpha is None:
            # å¦‚æœæ²¡æœ‰è®¾ç½®ä¿¡æ¯å…ˆéªŒï¼Œä½¿ç”¨åŸå§‹çš„æ— ä¿¡æ¯å…ˆéªŒ
            alpha = np.ones(len(mixture_ratios))
            logger.warning("ä½¿ç”¨é»˜è®¤æ— ä¿¡æ¯å…ˆéªŒ")
        else:
            alpha = self.current_alpha
        
        return self.enhanced_prior_calculator.calculate_enhanced_prior_log_probability(
            mixture_ratios, alpha)
    
    def propose_enhanced_mixture_ratios(self, current_ratios: np.ndarray, 
                                      step_size: float = 0.05) -> np.ndarray:
        """å¢å¼ºçš„æ··åˆæ¯”ä¾‹æè®®å‡½æ•°ï¼Œè€ƒè™‘å…ˆéªŒä¿¡æ¯"""
        
        if self.current_alpha is not None:
            # ä½¿ç”¨å…ˆéªŒä¿¡æ¯æŒ‡å¯¼æè®®
            proposal_concentration = self.current_alpha * step_size * 10
            proposal_concentration = np.maximum(proposal_concentration, 0.1)
            
            # æ··åˆå½“å‰çŠ¶æ€å’Œå…ˆéªŒä¿¡æ¯
            effective_concentration = (current_ratios * (1 - step_size) + 
                                     proposal_concentration * step_size)
            effective_concentration = np.maximum(effective_concentration, 0.1)
            
            new_ratios = np.random.dirichlet(effective_concentration)
        else:
            # å›é€€åˆ°åŸå§‹æè®®æ–¹æ³•
            concentration = current_ratios / step_size
            concentration = np.maximum(concentration, 0.1)
            new_ratios = np.random.dirichlet(concentration)
        
        new_ratios = np.maximum(new_ratios, 1e-6)
        new_ratios = new_ratios / np.sum(new_ratios)
        
        return new_ratios
    
    def enhanced_mcmc_sampler(self, observed_data: Dict, N: int, 
                            att2_data: Dict = None) -> Dict:
        """å¢å¼ºçš„MCMCé‡‡æ ·å™¨ï¼Œä½¿ç”¨æ™ºèƒ½å…ˆéªŒ"""
        
        if self.v5_integrator is None:
            raise ValueError("è¯·å…ˆè®¾ç½®V5ç‰¹å¾")
        
        # ç¡®ä¿å·²è®¾ç½®ä¿¡æ¯å…ˆéªŒ
        if self.current_alpha is None:
            logger.warning("æœªè®¾ç½®ä¿¡æ¯å…ˆéªŒï¼Œä½¿ç”¨V5ç‰¹å¾è‡ªåŠ¨è®¾ç½®")
            self.predict_and_set_informative_prior(self.v5_integrator.v5_features, N)
        
        logger.info(f"å¼€å§‹å¢å¼ºMGM-RF MCMCé‡‡æ ·ï¼Œè´¡çŒ®è€…æ•°é‡: {N}")
        logger.info(f"ä½¿ç”¨ä¿¡æ¯å…ˆéªŒ: alpha = {self.current_alpha}")
        
        # åˆå§‹åŒ–æ··åˆæ¯”ä¾‹ - ä½¿ç”¨å…ˆéªŒä¿¡æ¯
        if self.current_alpha is not None:
            # ä»å…ˆéªŒåˆ†å¸ƒé‡‡æ ·åˆå§‹å€¼
            mixture_ratios = np.random.dirichlet(self.current_alpha)
        else:
            mixture_ratios = np.random.dirichlet(np.ones(N))
        
        logger.info(f"åˆå§‹æ··åˆæ¯”ä¾‹: {mixture_ratios}")
        
        # å­˜å‚¨MCMCæ ·æœ¬
        samples = {
            'mixture_ratios': [],
            'log_likelihood': [],
            'log_posterior': [],
            'log_prior': [],
            'acceptance_info': []
        }
        
        # è®¡ç®—åˆå§‹æ¦‚ç‡
        current_log_likelihood = self.calculate_total_marginalized_likelihood(
            observed_data, N, mixture_ratios, att2_data)
        current_log_prior = self.calculate_enhanced_prior_mixture_ratios(mixture_ratios)
        current_log_posterior = current_log_likelihood + current_log_prior
        
        logger.info(f"åˆå§‹ä¼¼ç„¶: {current_log_likelihood:.2f}")
        logger.info(f"åˆå§‹å…ˆéªŒ: {current_log_prior:.2f}")
        logger.info(f"åˆå§‹åéªŒ: {current_log_posterior:.2f}")
        
        # MCMCä¸»å¾ªç¯
        n_accepted = 0
        acceptance_details = []
        
        # è‡ªé€‚åº”æ­¥é•¿ - æ ¹æ®å…ˆéªŒä¿¡æ¯è°ƒæ•´
        if self.mixture_prediction and self.mixture_prediction['confidence'] > 0.7:
            step_size = 0.03  # é«˜ç½®ä¿¡åº¦æ—¶ä½¿ç”¨è¾ƒå°æ­¥é•¿
        else:
            step_size = 0.05  # é»˜è®¤æ­¥é•¿
        
        adaptation_interval = 500
        target_acceptance = 0.4
        
        for iteration in range(self.n_iterations):
            if iteration % 1000 == 0:
                acceptance_rate = n_accepted / max(iteration, 1)
                logger.info(f"è¿­ä»£ {iteration}/{self.n_iterations}, "
                          f"æ¥å—ç‡: {acceptance_rate:.3f}, "
                          f"ä¼¼ç„¶: {current_log_likelihood:.2f}, "
                          f"å…ˆéªŒ: {current_log_prior:.2f}")
            
            # ä½¿ç”¨å¢å¼ºçš„æè®®å‡½æ•°
            proposed_ratios = self.propose_enhanced_mixture_ratios(mixture_ratios, step_size)
            
            # è®¡ç®—æè®®çŠ¶æ€çš„æ¦‚ç‡
            proposed_log_likelihood = self.calculate_total_marginalized_likelihood(
                observed_data, N, proposed_ratios, att2_data)
            proposed_log_prior = self.calculate_enhanced_prior_mixture_ratios(proposed_ratios)
            proposed_log_posterior = proposed_log_likelihood + proposed_log_prior
            
            # Metropolis-Hastingsæ¥å—/æ‹’ç»
            log_ratio = proposed_log_posterior - current_log_posterior
            accept_prob = min(1.0, np.exp(log_ratio))
            
            if np.random.random() < accept_prob:
                mixture_ratios = proposed_ratios
                current_log_likelihood = proposed_log_likelihood
                current_log_prior = proposed_log_prior
                current_log_posterior = proposed_log_posterior
                n_accepted += 1
                accepted = True
            else:
                accepted = False
            
            # è®°å½•è¯¦ç»†æ¥å—ä¿¡æ¯
            acceptance_details.append({
                'iteration': iteration,
                'accepted': accepted,
                'log_ratio': log_ratio,
                'accept_prob': accept_prob,
                'current_ratios': mixture_ratios.copy(),
                'log_likelihood': current_log_likelihood,
                'log_prior': current_log_prior
            })
            
            # è‡ªé€‚åº”æ­¥é•¿è°ƒæ•´
            if iteration > 0 and iteration % adaptation_interval == 0 and iteration < self.n_warmup:
                recent_acceptance = np.mean([a['accepted'] for a in acceptance_details[-adaptation_interval:]])
                if recent_acceptance < target_acceptance - 0.05:
                    step_size *= 0.9
                elif recent_acceptance > target_acceptance + 0.05:
                    step_size *= 1.1
                step_size = np.clip(step_size, 0.01, 0.2)
                
                if iteration % (adaptation_interval * 4) == 0:
                    logger.info(f"  æ­¥é•¿è°ƒæ•´: {step_size:.4f}, æœ€è¿‘æ¥å—ç‡: {recent_acceptance:.3f}")
            
            # å­˜å‚¨æ ·æœ¬ï¼ˆé¢„çƒ­åï¼‰
            if iteration >= self.n_warmup and iteration % self.thinning == 0:
                samples['mixture_ratios'].append(mixture_ratios.copy())
                samples['log_likelihood'].append(current_log_likelihood)
                samples['log_posterior'].append(current_log_posterior)
                samples['log_prior'].append(current_log_prior)
        
        final_acceptance_rate = n_accepted / self.n_iterations
        logger.info(f"å¢å¼ºMCMCå®Œæˆï¼Œæ€»æ¥å—ç‡: {final_acceptance_rate:.3f}")
        logger.info(f"æœ‰æ•ˆæ ·æœ¬æ•°: {len(samples['mixture_ratios'])}")
        
        # åˆ†æå…ˆéªŒvsåéªŒ
        self._analyze_prior_posterior_comparison(samples, N)
        
        return {
            'samples': samples,
            'acceptance_rate': final_acceptance_rate,
            'n_samples': len(samples['mixture_ratios']),
            'acceptance_details': acceptance_details,
            'final_step_size': step_size,
            'converged': 0.15 <= final_acceptance_rate <= 0.6,
            'prior_alpha': self.current_alpha.copy() if self.current_alpha is not None else None,
            'mixture_prediction': self.mixture_prediction
        }
    
    def _analyze_prior_posterior_comparison(self, samples: Dict, N: int):
        """åˆ†æå…ˆéªŒä¸åéªŒçš„å¯¹æ¯”"""
        if not samples['mixture_ratios']:
            return
        
        posterior_samples = np.array(samples['mixture_ratios'])
        posterior_means = np.mean(posterior_samples, axis=0)
        
        if self.current_alpha is not None:
            prior_means = self.current_alpha / np.sum(self.current_alpha)
            
            logger.info("å…ˆéªŒvsåéªŒå¯¹æ¯”:")
            for i in range(N):
                shift = posterior_means[i] - prior_means[i]
                logger.info(f"  Mx_{i+1}: å…ˆéªŒ={prior_means[i]:.3f}, åéªŒ={posterior_means[i]:.3f}, åç§»={shift:+.3f}")

def create_enhanced_pipeline(q1_model_path: str = None):
    """åˆ›å»ºå¢å¼ºçš„MGM-RFæµæ°´çº¿"""
    
    # å¯¼å…¥åŸå§‹ç»„ä»¶
    from Q2_MGM_RF_Solution import MGM_RF_Inferencer, MGM_RF_Pipeline
    
    # åˆ›å»ºåŸå§‹æ¨æ–­å™¨
    original_inferencer = MGM_RF_Inferencer(q1_model_path)
    
    # åˆ›å»ºå¢å¼ºå…ˆéªŒè®¡ç®—å™¨
    enhanced_prior_calculator = EnhancedPriorCalculator()
    
    # åˆ›å»ºå¢å¼ºæ¨æ–­å™¨
    enhanced_inferencer = EnhancedMGM_RF_Inferencer(
        original_inferencer, enhanced_prior_calculator)
    
    # åˆ›å»ºå¢å¼ºæµæ°´çº¿
    class EnhancedMGM_RF_Pipeline(MGM_RF_Pipeline):
        def __init__(self, enhanced_inferencer):
            self.mgm_rf_inferencer = enhanced_inferencer
            self.results = {}
            logger.info("å¢å¼ºMGM-RFæµæ°´çº¿åˆå§‹åŒ–å®Œæˆ")
        
        def analyze_sample(self, sample_data: pd.DataFrame, 
                         att2_freq_data: Dict[str, List[str]] = None) -> Dict:
            """åˆ†æå•ä¸ªæ ·æœ¬ï¼Œä½¿ç”¨å¢å¼ºçš„å…ˆéªŒç³»ç»Ÿ"""
            sample_file = sample_data.iloc[0]['Sample File']
            logger.info(f"å¼€å§‹å¢å¼ºåˆ†æ: {sample_file}")
            
            # æ­¥éª¤1: é¢„æµ‹NoCå’Œæå–V5ç‰¹å¾
            predicted_noc, noc_confidence, v5_features = \
                self.mgm_rf_inferencer.predict_noc_from_sample(sample_data)
            
            # æ­¥éª¤2: è®¾ç½®V5ç‰¹å¾
            self.mgm_rf_inferencer.set_v5_features(v5_features)
            
            # æ­¥éª¤3: é¢„æµ‹æ··åˆæ¨¡å¼å¹¶è®¾ç½®ä¿¡æ¯å…ˆéªŒ
            self.mgm_rf_inferencer.predict_and_set_informative_prior(
                v5_features, predicted_noc)
            
            # æ­¥éª¤4: å¤„ç†STRæ•°æ®
            sample_peaks = self.mgm_rf_inferencer.q1_feature_engineering.process_peaks_simplified(sample_data)
            
            if sample_peaks.empty:
                logger.warning(f"æ ·æœ¬ {sample_file} æ²¡æœ‰æœ‰æ•ˆå³°æ•°æ®")
                return self._get_enhanced_default_result(
                    sample_file, predicted_noc, noc_confidence, v5_features)
            
            # æ­¥éª¤5: å‡†å¤‡è§‚æµ‹æ•°æ®
            observed_data = {}
            for locus, locus_group in sample_peaks.groupby('Marker'):
                alleles = locus_group['Allele'].tolist()
                heights = dict(zip(locus_group['Allele'], locus_group['Height']))
                
                observed_data[locus] = {
                    'locus': locus,
                    'alleles': alleles,
                    'heights': heights
                }
            
            # æ­¥éª¤6: å¢å¼ºMCMCæ¨æ–­
            import time
            start_time = time.time()
            mcmc_results = self.mgm_rf_inferencer.enhanced_mcmc_sampler(
                observed_data, predicted_noc, att2_freq_data)
            end_time = time.time()
            
            # æ­¥éª¤7: ç”ŸæˆåéªŒæ‘˜è¦å’Œè¯Šæ–­
            posterior_summary = self.generate_posterior_summary(mcmc_results, predicted_noc)
            convergence_diagnostics = self.analyze_convergence(mcmc_results['samples'], predicted_noc)
            
            # æ­¥éª¤8: å¢å¼ºç»“æœåˆ†æ
            enhanced_analysis = self._analyze_enhanced_results(mcmc_results, v5_features)
            
            result = {
                'sample_file': sample_file,
                'predicted_noc': predicted_noc,
                'noc_confidence': noc_confidence,
                'v5_features': v5_features,
                'mixture_prediction': mcmc_results['mixture_prediction'],
                'mcmc_results': mcmc_results,
                'posterior_summary': posterior_summary,
                'convergence_diagnostics': convergence_diagnostics,
                'enhanced_analysis': enhanced_analysis,
                'computation_time': end_time - start_time,
                'observed_data': observed_data
            }
            
            logger.info(f"å¢å¼ºåˆ†æå®Œæˆ: {sample_file}, è€—æ—¶: {end_time - start_time:.1f}ç§’")
            return result
        
        def _get_enhanced_default_result(self, sample_file: str, predicted_noc: int, 
                                       noc_confidence: float, v5_features: Dict) -> Dict:
            """è·å–å¢å¼ºçš„é»˜è®¤ç»“æœ"""
            # ä½¿ç”¨å…ˆéªŒé¢„æµ‹çš„æ¯”ä¾‹ä½œä¸ºé»˜è®¤å€¼
            mixture_prediction = self.mgm_rf_inferencer.enhanced_prior_calculator.predict_mixture_pattern(v5_features)
            
            if predicted_noc == 2 and mixture_prediction['mixture_pattern'] != 'balanced':
                major_ratio = mixture_prediction['predicted_major_ratio']
                default_ratios = [major_ratio, 1 - major_ratio]
            else:
                default_ratios = [1.0/predicted_noc] * predicted_noc
            
            default_summary = {}
            for i in range(predicted_noc):
                default_summary[f'Mx_{i+1}'] = {
                    'mean': default_ratios[i],
                    'std': 0.0,
                    'median': default_ratios[i],
                    'mode': default_ratios[i],
                    'credible_interval_95': [default_ratios[i], default_ratios[i]],
                    'credible_interval_90': [default_ratios[i], default_ratios[i]],
                    'hpdi_95': [default_ratios[i], default_ratios[i]]
                }
            
            return {
                'sample_file': sample_file,
                'predicted_noc': predicted_noc,
                'noc_confidence': noc_confidence,
                'v5_features': v5_features,
                'mixture_prediction': mixture_prediction,
                'mcmc_results': None,
                'posterior_summary': default_summary,
                'convergence_diagnostics': {'status': 'No valid peaks - used prior prediction'},
                'enhanced_analysis': {'used_default': True},
                'computation_time': 0.0,
                'observed_data': {}
            }
        
        def _analyze_enhanced_results(self, mcmc_results: Dict, v5_features: Dict) -> Dict:
            """åˆ†æå¢å¼ºç»“æœ"""
            analysis = {
                'prior_effectiveness': {},
                'posterior_concentration': {},
                'prediction_accuracy': {}
            }
            
            if mcmc_results['samples']['mixture_ratios']:
                posterior_samples = np.array(mcmc_results['samples']['mixture_ratios'])
                
                # åˆ†æåéªŒåˆ†å¸ƒçš„é›†ä¸­ç¨‹åº¦
                posterior_stds = np.std(posterior_samples, axis=0)
                analysis['posterior_concentration'] = {
                    'std_values': posterior_stds.tolist(),
                    'avg_std': np.mean(posterior_stds),
                    'concentration_level': 'high' if np.mean(posterior_stds) < 0.1 else 'medium' if np.mean(posterior_stds) < 0.2 else 'low'
                }
                
                # åˆ†æå…ˆéªŒçš„æœ‰æ•ˆæ€§
                if mcmc_results.get('prior_alpha') is not None:
                    prior_alpha = mcmc_results['prior_alpha']
                    prior_means = prior_alpha / np.sum(prior_alpha)
                    posterior_means = np.mean(posterior_samples, axis=0)
                    
                    # è®¡ç®—å…ˆéªŒå’ŒåéªŒçš„KLæ•£åº¦
                    kl_divergence = self._calculate_kl_divergence(prior_means, posterior_means)
                    
                    analysis['prior_effectiveness'] = {
                        'prior_means': prior_means.tolist(),
                        'posterior_means': posterior_means.tolist(),
                        'mean_absolute_shift': np.mean(np.abs(posterior_means - prior_means)),
                        'kl_divergence': kl_divergence,
                        'prior_influence': 'strong' if kl_divergence < 0.1 else 'moderate' if kl_divergence < 0.5 else 'weak'
                    }
                
                # åˆ†æé¢„æµ‹å‡†ç¡®æ€§
                mixture_prediction = mcmc_results.get('mixture_prediction', {})
                if mixture_prediction:
                    predicted_major_ratio = mixture_prediction.get('predicted_major_ratio', 0.5)
                    actual_major_ratio = np.max(posterior_means)
                    
                    analysis['prediction_accuracy'] = {
                        'predicted_major_ratio': predicted_major_ratio,
                        'actual_major_ratio': actual_major_ratio,
                        'prediction_error': abs(actual_major_ratio - predicted_major_ratio),
                        'accuracy_level': 'excellent' if abs(actual_major_ratio - predicted_major_ratio) < 0.1 
                                        else 'good' if abs(actual_major_ratio - predicted_major_ratio) < 0.2 
                                        else 'fair'
                    }
            
            return analysis
        
        def _calculate_kl_divergence(self, p: np.ndarray, q: np.ndarray) -> float:
            """è®¡ç®—KLæ•£åº¦"""
            p = np.clip(p, 1e-10, 1.0)
            q = np.clip(q, 1e-10, 1.0)
            return np.sum(p * np.log(p / q))
        
        def visualize_enhanced_results(self, results: Dict, output_dir: str):
            """å¯è§†åŒ–å¢å¼ºåˆ†æç»“æœ"""
            import os
            os.makedirs(output_dir, exist_ok=True)
            
            sample_file = results['sample_file']
            
            # 1. å…ˆéªŒé¢„æµ‹å¯è§†åŒ–
            if 'mixture_prediction' in results and results['mixture_prediction']:
                mixture_pred = results['mixture_prediction']
                
                fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
                
                # ç‰¹å¾è´¡çŒ®å›¾
                contributions = mixture_pred['feature_contributions']
                features = list(contributions.keys())
                values = list(contributions.values())
                
                ax1.barh(features, values, color='skyblue', edgecolor='navy', alpha=0.7)
                ax1.set_xlabel('è´¡çŒ®åˆ†æ•°')
                ax1.set_title('V5ç‰¹å¾å¯¹ä¸å¹³è¡¡æ€§çš„è´¡çŒ®')
                ax1.grid(True, alpha=0.3)
                
                # æ··åˆæ¨¡å¼é¢„æµ‹
                patterns = ['balanced', 'moderately_imbalanced', 'highly_imbalanced']
                pattern_scores = [0, 0, 0]
                current_pattern = mixture_pred['mixture_pattern']
                pattern_scores[patterns.index(current_pattern)] = mixture_pred['imbalance_score']
                
                colors = ['green', 'orange', 'red']
                ax2.bar(patterns, pattern_scores, color=colors, alpha=0.7, edgecolor='black')
                ax2.set_ylabel('ä¸å¹³è¡¡è¯„åˆ†')
                ax2.set_title(f'æ··åˆæ¨¡å¼é¢„æµ‹: {mixture_pred["pattern_description"]}')
                ax2.tick_params(axis='x', rotation=45)
                ax2.grid(True, alpha=0.3)
                
                # å…ˆéªŒåˆ†å¸ƒå¯è§†åŒ–
                if results['mcmc_results'] and results['mcmc_results'].get('prior_alpha') is not None:
                    prior_alpha = results['mcmc_results']['prior_alpha']
                    N = len(prior_alpha)
                    
                    # ä»å…ˆéªŒé‡‡æ ·
                    prior_samples = np.random.dirichlet(prior_alpha, 5000)
                    
                    for i in range(min(N, 2)):  # æœ€å¤šæ˜¾ç¤º2ä¸ªç»„åˆ†
                        ax = ax3 if i == 0 else ax4
                        ax.hist(prior_samples[:, i], bins=50, alpha=0.7, density=True,
                               color=f'C{i}', edgecolor='black', linewidth=0.5)
                        ax.set_title(f'Mx_{i+1} å…ˆéªŒåˆ†å¸ƒ')
                        ax.set_xlabel(f'Mx_{i+1}')
                        ax.set_ylabel('æ¦‚ç‡å¯†åº¦')
                        ax.grid(True, alpha=0.3)
                        
                        # æ·»åŠ é¢„æµ‹å€¼
                        predicted_val = mixture_pred['predicted_major_ratio'] if i == 0 else (1 - mixture_pred['predicted_major_ratio'])
                        ax.axvline(predicted_val, color='red', linestyle='--', linewidth=2,
                                  label=f'é¢„æµ‹å€¼: {predicted_val:.3f}')
                        ax.legend()
                
                plt.tight_layout()
                plt.savefig(os.path.join(output_dir, f'{sample_file}_enhanced_prediction.png'),
                           dpi=300, bbox_inches='tight')
                plt.close()
            
            # 2. å…ˆéªŒvsåéªŒå¯¹æ¯”
            if (results['mcmc_results'] and results['mcmc_results']['samples']['mixture_ratios'] and
                results['mcmc_results'].get('prior_alpha') is not None):
                
                posterior_samples = np.array(results['mcmc_results']['samples']['mixture_ratios'])
                prior_alpha = results['mcmc_results']['prior_alpha']
                N = len(prior_alpha)
                
                fig, axes = plt.subplots(N, 1, figsize=(12, 4*N))
                if N == 1:
                    axes = [axes]
                
                # ä»å…ˆéªŒé‡‡æ ·ç”¨äºå¯¹æ¯”
                prior_samples = np.random.dirichlet(prior_alpha, len(posterior_samples))
                
                for i in range(N):
                    # ç»˜åˆ¶å…ˆéªŒå’ŒåéªŒåˆ†å¸ƒ
                    axes[i].hist(prior_samples[:, i], bins=50, alpha=0.5, density=True,
                                color='blue', label='å…ˆéªŒåˆ†å¸ƒ', edgecolor='navy')
                    axes[i].hist(posterior_samples[:, i], bins=50, alpha=0.7, density=True,
                                color='red', label='åéªŒåˆ†å¸ƒ', edgecolor='darkred')
                    
                    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
                    prior_mean = np.mean(prior_samples[:, i])
                    posterior_mean = np.mean(posterior_samples[:, i])
                    
                    axes[i].axvline(prior_mean, color='blue', linestyle='--', linewidth=2,
                                   label=f'å…ˆéªŒå‡å€¼: {prior_mean:.3f}')
                    axes[i].axvline(posterior_mean, color='red', linestyle='--', linewidth=2,
                                   label=f'åéªŒå‡å€¼: {posterior_mean:.3f}')
                    
                    axes[i].set_title(f'Mx_{i+1}: å…ˆéªŒ vs åéªŒåˆ†å¸ƒå¯¹æ¯”')
                    axes[i].set_xlabel(f'Mx_{i+1}')
                    axes[i].set_ylabel('æ¦‚ç‡å¯†åº¦')
                    axes[i].legend()
                    axes[i].grid(True, alpha=0.3)
                
                plt.tight_layout()
                plt.savefig(os.path.join(output_dir, f'{sample_file}_prior_posterior_comparison.png'),
                           dpi=300, bbox_inches='tight')
                plt.close()
            
            # è°ƒç”¨åŸå§‹çš„å¯è§†åŒ–æ–¹æ³•
            super().plot_results(results, output_dir)
    
    return EnhancedMGM_RF_Pipeline(enhanced_inferencer)

# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•å‡½æ•°
def test_enhanced_prior_system():
    """æµ‹è¯•å¢å¼ºå…ˆéªŒç³»ç»Ÿ"""
    print("="*80)
    print("å¢å¼ºå…ˆéªŒç³»ç»Ÿæµ‹è¯•")
    print("="*80)
    
    # åˆ›å»ºæµ‹è¯•V5ç‰¹å¾
    test_features = [
        {
            'name': 'é«˜åº¦ä¸å¹³è¡¡æ ·æœ¬',
            'features': {
                'skewness_peak_height': 2.5,      # é«˜ååº¦
                'ratio_severe_imbalance_loci': 0.8, # 80%ä½ç‚¹ä¸¥é‡å¤±è¡¡
                'avg_locus_allele_entropy': 0.3,   # ä½ç†µ
                'inter_locus_balance_entropy': 1.0, # ä½ä½ç‚¹é—´ç†µ
                'mac_profile': 6                    # é«˜æœ€å¤§ç­‰ä½åŸºå› æ•°
            }
        },
        {
            'name': 'ä¸­åº¦ä¸å¹³è¡¡æ ·æœ¬',
            'features': {
                'skewness_peak_height': 1.2,
                'ratio_severe_imbalance_loci': 0.4,
                'avg_locus_allele_entropy': 0.8,
                'inter_locus_balance_entropy': 2.0,
                'mac_profile': 4
            }
        },
        {
            'name': 'ç›¸å¯¹å¹³è¡¡æ ·æœ¬',
            'features': {
                'skewness_peak_height': 0.3,
                'ratio_severe_imbalance_loci': 0.1,
                'avg_locus_allele_entropy': 1.2,
                'inter_locus_balance_entropy': 2.8,
                'mac_profile': 3
            }
        }
    ]
    
    # åˆ›å»ºå¢å¼ºå…ˆéªŒè®¡ç®—å™¨
    enhanced_prior_calc = EnhancedPriorCalculator()
    
    for test_case in test_features:
        print(f"\n--- {test_case['name']} ---")
        
        # é¢„æµ‹æ··åˆæ¨¡å¼
        mixture_pred = enhanced_prior_calc.predict_mixture_pattern(test_case['features'])
        
        print(f"é¢„æµ‹æ¨¡å¼: {mixture_pred['pattern_description']}")
        print(f"ä¸å¹³è¡¡è¯„åˆ†: {mixture_pred['imbalance_score']:.3f}")
        print(f"é¢„æµ‹ä¸»è¦è´¡çŒ®è€…æ¯”ä¾‹: {mixture_pred['predicted_major_ratio']:.3f}")
        print(f"é¢„æµ‹ç½®ä¿¡åº¦: {mixture_pred['confidence']:.3f}")
        
        # è®¡ç®—2äººæ··åˆçš„å…ˆéªŒå‚æ•°
        alpha_2 = enhanced_prior_calc.calculate_informative_prior_alpha(2, mixture_pred)
        print(f"2äººæ··åˆDirichletå…ˆéªŒÎ±: {alpha_2}")
        print(f"æœŸæœ›æ¯”ä¾‹: {alpha_2 / np.sum(alpha_2)}")
        
        # å¯è§†åŒ–å…ˆéªŒåˆ†å¸ƒ
        fig = enhanced_prior_calc.visualize_prior_distribution(
            alpha_2, f"{test_case['name']} - 2äººæ··åˆå…ˆéªŒåˆ†å¸ƒ")
        plt.show()

def analyze_sample_with_enhanced_priors(sample_id: str, att2_path: str, 
                                      q1_model_path: str = None) -> Dict:
    """ä½¿ç”¨å¢å¼ºå…ˆéªŒç³»ç»Ÿåˆ†ææ ·æœ¬"""
    print(f"\n{'='*80}")
    print(f"ä½¿ç”¨å¢å¼ºå…ˆéªŒç³»ç»Ÿåˆ†ææ ·æœ¬: {sample_id}")
    print(f"{'='*80}")
    
    # åˆ›å»ºå¢å¼ºæµæ°´çº¿
    enhanced_pipeline = create_enhanced_pipeline(q1_model_path)
    
    # åŠ è½½æ•°æ®
    att2_data = enhanced_pipeline.load_attachment2_data(att2_path)
    
    if sample_id not in att2_data:
        raise ValueError(f"æ ·æœ¬ {sample_id} ä¸å­˜åœ¨")
    
    # å‡†å¤‡é¢‘ç‡æ•°æ®
    att2_freq_data = enhanced_pipeline.prepare_att2_frequency_data(att2_data)
    
    # åˆ†ææ ·æœ¬
    sample_data = att2_data[sample_id]
    result = enhanced_pipeline.analyze_sample(sample_data, att2_freq_data)
    
    # ä¿å­˜ç»“æœ
    output_dir = './enhanced_mgm_rf_results'
    os.makedirs(output_dir, exist_ok=True)
    
    output_file = os.path.join(output_dir, f'{sample_id}_enhanced_result.json')
    enhanced_pipeline.save_results(result, output_file)
    
    # å¯è§†åŒ–ç»“æœ
    enhanced_pipeline.visualize_enhanced_results(result, output_dir)
    
    # æ‰“å°è¯¦ç»†ç»“æœ
    print_enhanced_analysis_summary(result)
    
    return result

def print_enhanced_analysis_summary(result: Dict):
    """æ‰“å°å¢å¼ºåˆ†ææ‘˜è¦"""
    print(f"\n{'='*60}")
    print(f"å¢å¼ºåˆ†æç»“æœæ‘˜è¦")
    print(f"{'='*60}")
    
    sample_file = result['sample_file']
    predicted_noc = result['predicted_noc']
    noc_confidence = result['noc_confidence']
    
    print(f"æ ·æœ¬ID: {sample_file}")
    print(f"é¢„æµ‹NoC: {predicted_noc} (ç½®ä¿¡åº¦: {noc_confidence:.3f})")
    
    # æ··åˆæ¨¡å¼é¢„æµ‹
    if 'mixture_prediction' in result:
        mixture_pred = result['mixture_prediction']
        print(f"\nğŸ“Š æ··åˆæ¨¡å¼é¢„æµ‹:")
        print(f"  æ¨¡å¼: {mixture_pred['pattern_description']}")
        print(f"  ä¸å¹³è¡¡è¯„åˆ†: {mixture_pred['imbalance_score']:.3f}")
        print(f"  é¢„æµ‹ä¸»è¦è´¡çŒ®è€…æ¯”ä¾‹: {mixture_pred['predicted_major_ratio']:.3f}")
        print(f"  é¢„æµ‹ç½®ä¿¡åº¦: {mixture_pred['confidence']:.3f}")
        
        print(f"\nğŸ” V5ç‰¹å¾è´¡çŒ®åˆ†æ:")
        for feature, contribution in mixture_pred['feature_contributions'].items():
            print(f"  {feature}: {contribution:.3f}")
    
    # MCMCç»“æœ
    if result['mcmc_results'] is not None:
        mcmc_results = result['mcmc_results']
        print(f"\nâš™ï¸ MCMCé‡‡æ ·ç»“æœ:")
        print(f"  æ¥å—ç‡: {mcmc_results['acceptance_rate']:.3f}")
        print(f"  æ”¶æ•›çŠ¶æ€: {'æ˜¯' if mcmc_results['converged'] else 'å¦'}")
        print(f"  æœ‰æ•ˆæ ·æœ¬æ•°: {mcmc_results['n_samples']}")
        
        if mcmc_results.get('prior_alpha') is not None:
            prior_alpha = mcmc_results['prior_alpha']
            print(f"  ä½¿ç”¨çš„å…ˆéªŒÎ±: {prior_alpha}")
        
        # åéªŒç»“æœ
        posterior_summary = result['posterior_summary']
        contributor_ranking = posterior_summary['contributor_ranking']
        
        print(f"\nğŸ¯ æ··åˆæ¯”ä¾‹åéªŒä¼°è®¡:")
        for rank, (contributor_id, mean_ratio) in enumerate(contributor_ranking, 1):
            mx_stats = posterior_summary[f'Mx_{contributor_id}']
            ci_95 = mx_stats['credible_interval_95']
            std_val = mx_stats['std']
            print(f"  è´¡çŒ®è€…{contributor_id}: {mean_ratio:.4f} Â± {std_val:.4f}")
            print(f"    95%ç½®ä¿¡åŒºé—´: [{ci_95[0]:.4f}, {ci_95[1]:.4f}]")
    
    # å¢å¼ºåˆ†æç»“æœ
    if 'enhanced_analysis' in result:
        enhanced_analysis = result['enhanced_analysis']
        
        if 'prior_effectiveness' in enhanced_analysis:
            prior_eff = enhanced_analysis['prior_effectiveness']
            print(f"\nğŸ“ˆ å…ˆéªŒæœ‰æ•ˆæ€§åˆ†æ:")
            print(f"  å…ˆéªŒå½±å“ç¨‹åº¦: {prior_eff.get('prior_influence', 'unknown')}")
            print(f"  å¹³å‡åç§»: {prior_eff.get('mean_absolute_shift', 0):.4f}")
            print(f"  KLæ•£åº¦: {prior_eff.get('kl_divergence', 0):.4f}")
        
        if 'prediction_accuracy' in enhanced_analysis:
            pred_acc = enhanced_analysis['prediction_accuracy']
            print(f"\nğŸ¯ é¢„æµ‹å‡†ç¡®æ€§:")
            print(f"  å‡†ç¡®æ€§æ°´å¹³: {pred_acc.get('accuracy_level', 'unknown')}")
            print(f"  é¢„æµ‹è¯¯å·®: {pred_acc.get('prediction_error', 0):.4f}")
    
    print(f"\nâ±ï¸ è®¡ç®—æ—¶é—´: {result['computation_time']:.1f}ç§’")
    print(f"{'='*60}")

# ä¸»ç¨‹åºæ¥å£
def main_enhanced():
    """å¢å¼ºç³»ç»Ÿä¸»ç¨‹åº"""
    print("æ³•åŒ»æ··åˆSTRå›¾è°±è´¡çŒ®è€…æ¯”ä¾‹æ¨æ–­ç³»ç»Ÿ - å¢å¼ºå…ˆéªŒç‰ˆæœ¬")
    print("åŸºäºV5ç‰¹å¾é©±åŠ¨çš„æ™ºèƒ½å…ˆéªŒ + MGM-Mæ–¹æ³•")
    print("="*80)
    
    # é€‰æ‹©è¿è¡Œæ¨¡å¼
    print("\nè¯·é€‰æ‹©è¿è¡Œæ¨¡å¼:")
    print("1. æµ‹è¯•å¢å¼ºå…ˆéªŒç³»ç»Ÿ")
    print("2. åˆ†æå•ä¸ªæ ·æœ¬ï¼ˆå¢å¼ºç‰ˆï¼‰")
    print("3. å¯¹æ¯”åˆ†æï¼ˆåŸç‰ˆ vs å¢å¼ºç‰ˆï¼‰")
    
    choice = input("è¯·è¾“å…¥é€‰æ‹© (1/2/3): ").strip()
    
    if choice == "1":
        test_enhanced_prior_system()
    
    elif choice == "2":
        att2_path = input("è¯·è¾“å…¥é™„ä»¶2è·¯å¾„ (é»˜è®¤: é™„ä»¶2ï¼šæ··åˆSTRå›¾è°±æ•°æ®.csv): ").strip()
        if not att2_path:
            att2_path = "é™„ä»¶2ï¼šæ··åˆSTRå›¾è°±æ•°æ®.csv"
        
        sample_id = input("è¯·è¾“å…¥æ ·æœ¬ID: ").strip()
        q1_model_path = input("è¯·è¾“å…¥Q1æ¨¡å‹è·¯å¾„ (å¯é€‰ï¼ŒæŒ‰Enterè·³è¿‡): ").strip()
        
        try:
            result = analyze_sample_with_enhanced_priors(
                sample_id, att2_path, q1_model_path if q1_model_path else None)
        except Exception as e:
            print(f"åˆ†æå¤±è´¥: {e}")
    
    elif choice == "3":
        print("å¯¹æ¯”åˆ†æåŠŸèƒ½å¼€å‘ä¸­...")
        # TODO: å®ç°åŸç‰ˆvså¢å¼ºç‰ˆçš„å¯¹æ¯”åˆ†æ
    
    else:
        print("æ— æ•ˆé€‰æ‹©")

if __name__ == "__main__":
    main_enhanced()