# -*- coding: utf-8 -*-
"""
æ•°å­¦å»ºæ¨¡ - æ³•åŒ»DNAåˆ†æ - é—®é¢˜1ï¼šè´¡çŒ®è€…äººæ•°è¯†åˆ« (ä¿®å¤ç‰ˆ)

ç‰ˆæœ¬: V5.1 - Fixed RFECV + Optimized GradientBoosting
æ—¥æœŸ: 2025-06-03
æè¿°: ä¿®å¤RFECVé”™è¯¯ + ä¼˜åŒ–æ¢¯åº¦æå‡æœº + ä¸­æ–‡ç‰¹å¾åç§°
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import warnings
import json
import re
from scipy import stats
from scipy.signal import find_peaks
from collections import Counter
from time import time
import random

# æœºå™¨å­¦ä¹ ç›¸å…³
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV, RandomizedSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc, make_scorer
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.feature_selection import RFECV, SelectFromModel, SelectKBest, f_classif
from sklearn.utils import resample
from sklearn.utils.class_weight import compute_sample_weight

# å¯è§£é‡Šæ€§åˆ†æ
try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    print("SHAPä¸å¯ç”¨ï¼Œå°†è·³è¿‡å¯è§£é‡Šæ€§åˆ†æ")
    SHAP_AVAILABLE = False

# é…ç½®
plt.rcParams['font.sans-serif'] = ["Arial Unicode MS", "SimHei", "Microsoft YaHei"]
plt.rcParams['axes.unicode_minus'] = False
warnings.filterwarnings('ignore')

print("=== æ³•åŒ»æ··åˆSTRå›¾è°±NoCæ™ºèƒ½è¯†åˆ«ç³»ç»Ÿ (ä¿®å¤ç‰ˆ) ===")
print("åŸºäºæ”¹è¿›ç‰¹å¾é€‰æ‹© + ä¼˜åŒ–æ¢¯åº¦æå‡æœº")

# =====================
# 1. æ–‡ä»¶è·¯å¾„ä¸åŸºç¡€è®¾ç½®
# =====================
DATA_DIR = './'
file_path = os.path.join(DATA_DIR, 'é™„ä»¶1ï¼šä¸åŒäººæ•°çš„STRå›¾è°±æ•°æ®.csv')
PLOTS_DIR = os.path.join(DATA_DIR, 'noc_fixed_plots')
if not os.path.exists(PLOTS_DIR):
    os.makedirs(PLOTS_DIR)

# å…³é”®å‚æ•°è®¾ç½®
HEIGHT_THRESHOLD = 50
SATURATION_THRESHOLD = 30000
CTA_THRESHOLD = 0.5
PHR_IMBALANCE_THRESHOLD = 0.6

# =====================
# 2. ä¸­æ–‡ç‰¹å¾åç§°æ˜ å°„
# =====================
FEATURE_NAME_MAPPING = {
    # Aç±»ï¼šå›¾è°±å±‚é¢åŸºç¡€ç»Ÿè®¡ç‰¹å¾
    'mac_profile': 'æ ·æœ¬æœ€å¤§ç­‰ä½åŸºå› æ•°',
    'total_distinct_alleles': 'æ ·æœ¬æ€»ç‰¹å¼‚ç­‰ä½åŸºå› æ•°',
    'avg_alleles_per_locus': 'æ¯ä½ç‚¹å¹³å‡ç­‰ä½åŸºå› æ•°',
    'std_alleles_per_locus': 'æ¯ä½ç‚¹ç­‰ä½åŸºå› æ•°æ ‡å‡†å·®',
    'loci_gt2_alleles': 'ç­‰ä½åŸºå› æ•°å¤§äº2çš„ä½ç‚¹æ•°',
    'loci_gt3_alleles': 'ç­‰ä½åŸºå› æ•°å¤§äº3çš„ä½ç‚¹æ•°',
    'loci_gt4_alleles': 'ç­‰ä½åŸºå› æ•°å¤§äº4çš„ä½ç‚¹æ•°',
    'loci_gt5_alleles': 'ç­‰ä½åŸºå› æ•°å¤§äº5çš„ä½ç‚¹æ•°',
    'loci_gt6_alleles': 'ç­‰ä½åŸºå› æ•°å¤§äº6çš„ä½ç‚¹æ•°',
    'allele_count_dist_entropy': 'ç­‰ä½åŸºå› è®¡æ•°åˆ†å¸ƒç†µ',
    
    # Bç±»ï¼šå³°é«˜ã€å¹³è¡¡æ€§åŠéšæœºæ•ˆåº”ç‰¹å¾
    'avg_peak_height': 'å¹³å‡å³°é«˜',
    'std_peak_height': 'å³°é«˜æ ‡å‡†å·®',
    'min_peak_height': 'æœ€å°å³°é«˜',
    'max_peak_height': 'æœ€å¤§å³°é«˜',
    'avg_phr': 'å¹³å‡å³°é«˜æ¯”',
    'std_phr': 'å³°é«˜æ¯”æ ‡å‡†å·®',
    'min_phr': 'æœ€å°å³°é«˜æ¯”',
    'median_phr': 'å³°é«˜æ¯”ä¸­ä½æ•°',
    'num_loci_with_phr': 'å¯è®¡ç®—å³°é«˜æ¯”çš„ä½ç‚¹æ•°',
    'num_severe_imbalance_loci': 'ä¸¥é‡å¤±è¡¡ä½ç‚¹æ•°',
    'ratio_severe_imbalance_loci': 'ä¸¥é‡å¤±è¡¡ä½ç‚¹æ¯”ä¾‹',
    'skewness_peak_height': 'å³°é«˜åˆ†å¸ƒååº¦',
    'kurtosis_peak_height': 'å³°é«˜åˆ†å¸ƒå³­åº¦',
    'modality_peak_height': 'å³°é«˜åˆ†å¸ƒå¤šå³°æ€§',
    'num_saturated_peaks': 'é¥±å’Œå³°æ•°é‡',
    'ratio_saturated_peaks': 'é¥±å’Œå³°æ¯”ä¾‹',
    
    # Cç±»ï¼šä¿¡æ¯è®ºåŠå›¾è°±å¤æ‚åº¦ç‰¹å¾
    'inter_locus_balance_entropy': 'ä½ç‚¹é—´å¹³è¡¡ç†µ',
    'avg_locus_allele_entropy': 'å¹³å‡ä½ç‚¹ç­‰ä½åŸºå› ç†µ',
    'peak_height_entropy': 'å³°é«˜åˆ†å¸ƒç†µ',
    'num_loci_with_effective_alleles': 'æœ‰æ•ˆç­‰ä½åŸºå› ä½ç‚¹æ•°',
    'num_loci_no_effective_alleles': 'æ— æœ‰æ•ˆç­‰ä½åŸºå› ä½ç‚¹æ•°',
    
    # Dç±»ï¼šDNAé™è§£ä¸ä¿¡æ¯ä¸¢å¤±ç‰¹å¾
    'height_size_correlation': 'å³°é«˜ç‰‡æ®µå¤§å°ç›¸å…³æ€§',
    'height_size_slope': 'å³°é«˜ç‰‡æ®µå¤§å°å›å½’æ–œç‡',
    'weighted_height_size_slope': 'åŠ æƒå³°é«˜ç‰‡æ®µå¤§å°æ–œç‡',
    'phr_size_slope': 'å³°é«˜æ¯”ç‰‡æ®µå¤§å°æ–œç‡',
    'locus_dropout_score_weighted_by_size': 'ç‰‡æ®µå¤§å°åŠ æƒä½ç‚¹ä¸¢å¤±è¯„åˆ†',
    'degradation_index_rfu_per_bp': 'RFUæ¯ç¢±åŸºå¯¹é™è§£æŒ‡æ•°',
    'info_completeness_ratio_small_large': 'å°å¤§ç‰‡æ®µä¿¡æ¯å®Œæ•´åº¦æ¯”ç‡'
}

def get_chinese_name(feature_name):
    """è·å–ç‰¹å¾çš„ä¸­æ–‡åç§°"""
    return FEATURE_NAME_MAPPING.get(feature_name, feature_name)

# =====================
# 3. è¾…åŠ©å‡½æ•°å®šä¹‰
# =====================
def extract_noc_from_filename(filename):
    """ä»æ–‡ä»¶åæå–è´¡çŒ®è€…äººæ•°ï¼ˆNoCï¼‰"""
    match = re.search(r'-(\d+(?:_\d+)*)-[\d;]+-', str(filename))
    if match:
        ids = [id_val for id_val in match.group(1).split('_') if id_val.isdigit()]
        return len(ids) if len(ids) > 0 else np.nan
    return np.nan

def calculate_entropy(probabilities):
    """è®¡ç®—é¦™å†œç†µ"""
    probabilities = np.array(probabilities)
    probabilities = probabilities[probabilities > 0]
    if len(probabilities) == 0:
        return 0.0
    return -np.sum(probabilities * np.log(probabilities + 1e-10))

def calculate_ols_slope(x, y):
    """è®¡ç®—OLSå›å½’æ–œç‡"""
    if len(x) < 2 or len(x) != len(y):
        return 0.0
    
    x = np.array(x)
    y = np.array(y)
    
    if len(np.unique(x)) < 2:
        return 0.0
    
    try:
        slope, _, _, _, _ = stats.linregress(x, y)
        return slope
    except:
        return 0.0

# =====================
# 4. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
# =====================
print("\n=== æ­¥éª¤1: æ•°æ®åŠ è½½ä¸é¢„å¤„ç† ===")

# åŠ è½½æ•°æ®
try:
    df = pd.read_csv(file_path, encoding='utf-8')
    print(f"æˆåŠŸåŠ è½½æ•°æ®ï¼Œå½¢çŠ¶: {df.shape}")
except Exception as e:
    print(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
    exit()

# æå–NoCæ ‡ç­¾
df['NoC_True'] = df['Sample File'].apply(extract_noc_from_filename)
df = df.dropna(subset=['NoC_True'])
df['NoC_True'] = df['NoC_True'].astype(int)

print(f"åŸå§‹æ•°æ®NoCåˆ†å¸ƒ: {df['NoC_True'].value_counts().sort_index().to_dict()}")
print(f"åŸå§‹æ ·æœ¬æ•°: {df['Sample File'].nunique()}")

# =====================
# 5. ç®€åŒ–å³°å¤„ç†ä¸CTAè¯„ä¼°
# =====================
print("\n=== æ­¥éª¤2: ç®€åŒ–å³°å¤„ç†ä¸ä¿¡å·è¡¨å¾ ===")

def process_peaks_simplified(sample_data):
    """ç®€åŒ–çš„å³°å¤„ç†å‡½æ•°"""
    processed_data = []
    
    for _, sample_row in sample_data.iterrows():
        sample_file = sample_row['Sample File']
        marker = sample_row['Marker']
        
        # æå–æ‰€æœ‰å³°
        peaks = []
        for i in range(1, 101):
            allele = sample_row.get(f'Allele {i}')
            size = sample_row.get(f'Size {i}')
            height = sample_row.get(f'Height {i}')
            
            if pd.notna(allele) and pd.notna(size) and pd.notna(height):
                original_height = float(height)
                corrected_height = min(original_height, SATURATION_THRESHOLD)
                
                if corrected_height >= HEIGHT_THRESHOLD:
                    peaks.append({
                        'allele': allele,
                        'size': float(size),
                        'height': corrected_height,
                        'original_height': original_height
                    })
        
        if not peaks:
            continue
            
        # ç®€åŒ–çš„CTAè¯„ä¼°
        peaks.sort(key=lambda x: x['height'], reverse=True)
        
        for peak in peaks:
            height_rank = peaks.index(peak) + 1
            total_peaks = len(peaks)
            
            # ç®€åŒ–çš„CTAè®¡ç®—
            if height_rank == 1:
                cta = 0.95
            elif height_rank == 2 and total_peaks >= 2:
                height_ratio = peak['height'] / peaks[0]['height']
                cta = 0.8 if height_ratio > 0.3 else 0.6
            else:
                height_ratio = peak['height'] / peaks[0]['height']
                cta = max(0.1, min(0.8, height_ratio))
            
            if cta >= CTA_THRESHOLD:
                processed_data.append({
                    'Sample File': sample_file,
                    'Marker': marker,
                    'Allele': peak['allele'],
                    'Size': peak['size'],
                    'Height': peak['height'],
                    'Original_Height': peak['original_height'],
                    'CTA': cta
                })
    
    return pd.DataFrame(processed_data)

# å¤„ç†æ‰€æœ‰æ ·æœ¬
print("å¤„ç†å³°æ•°æ®...")
all_processed_peaks = []
for sample_file, group in df.groupby('Sample File'):
    sample_peaks = process_peaks_simplified(group)
    if not sample_peaks.empty:
        all_processed_peaks.append(sample_peaks)

df_peaks = pd.concat(all_processed_peaks, ignore_index=True) if all_processed_peaks else pd.DataFrame()
print(f"å¤„ç†åçš„å³°æ•°æ®å½¢çŠ¶: {df_peaks.shape}")

# =====================
# 6. ç®€åŒ–ç‰¹å¾å·¥ç¨‹
# =====================
print("\n=== æ­¥éª¤3: ç®€åŒ–ç‰¹å¾å·¥ç¨‹ ===")

def extract_simplified_features(sample_file, sample_peaks):
    """æå–ç®€åŒ–çš„ç‰¹å¾é›†"""
    if sample_peaks.empty:
        return {}
    
    features = {'æ ·æœ¬æ–‡ä»¶': sample_file}
    
    # åŸºç¡€æ•°æ®å‡†å¤‡
    total_peaks = len(sample_peaks)
    all_heights = sample_peaks['Height'].values
    all_sizes = sample_peaks['Size'].values
    
    # æŒ‰ä½ç‚¹åˆ†ç»„ç»Ÿè®¡
    locus_groups = sample_peaks.groupby('Marker')
    alleles_per_locus = locus_groups['Allele'].nunique()
    locus_heights = locus_groups['Height'].sum()
    
    # Aç±»ï¼šæ ¸å¿ƒè®¡æ•°ç‰¹å¾
    features['æ ·æœ¬æœ€å¤§ç­‰ä½åŸºå› æ•°'] = alleles_per_locus.max() if len(alleles_per_locus) > 0 else 0
    features['æ ·æœ¬æ€»ç‰¹å¼‚ç­‰ä½åŸºå› æ•°'] = sample_peaks['Allele'].nunique()
    features['æ¯ä½ç‚¹å¹³å‡ç­‰ä½åŸºå› æ•°'] = alleles_per_locus.mean() if len(alleles_per_locus) > 0 else 0
    features['æ¯ä½ç‚¹ç­‰ä½åŸºå› æ•°æ ‡å‡†å·®'] = alleles_per_locus.std() if len(alleles_per_locus) > 1 else 0
    
    # MGTNç³»åˆ—ï¼ˆç®€åŒ–ä¸ºå…³é”®çš„å‡ ä¸ªï¼‰
    for N in [2, 3, 4, 5]:
        features[f'ç­‰ä½åŸºå› æ•°å¤§äº{N}çš„ä½ç‚¹æ•°'] = (alleles_per_locus >= N).sum()
    
    # Bç±»ï¼šå³°é«˜ç»Ÿè®¡ç‰¹å¾
    if total_peaks > 0:
        features['å¹³å‡å³°é«˜'] = np.mean(all_heights)
        features['å³°é«˜æ ‡å‡†å·®'] = np.std(all_heights) if total_peaks > 1 else 0
        features['å³°é«˜å˜å¼‚ç³»æ•°'] = features['å³°é«˜æ ‡å‡†å·®'] / features['å¹³å‡å³°é«˜'] if features['å¹³å‡å³°é«˜'] > 0 else 0
        
        # å³°é«˜æ¯”ç‰¹å¾
        phr_values = []
        for marker, marker_group in locus_groups:
            if len(marker_group) == 2:
                heights = marker_group['Height'].values
                phr = min(heights) / max(heights) if max(heights) > 0 else 0
                phr_values.append(phr)
        
        if phr_values:
            features['å¹³å‡å³°é«˜æ¯”'] = np.mean(phr_values)
            features['æœ€å°å³°é«˜æ¯”'] = np.min(phr_values)
            features['ä¸¥é‡å¤±è¡¡ä½ç‚¹æ¯”ä¾‹'] = (np.array(phr_values) <= PHR_IMBALANCE_THRESHOLD).mean()
        else:
            features['å¹³å‡å³°é«˜æ¯”'] = 0
            features['æœ€å°å³°é«˜æ¯”'] = 0
            features['ä¸¥é‡å¤±è¡¡ä½ç‚¹æ¯”ä¾‹'] = 0
        
        # å³°é«˜åˆ†å¸ƒå½¢çŠ¶
        if total_peaks > 2:
            features['å³°é«˜åˆ†å¸ƒååº¦'] = stats.skew(all_heights)
        else:
            features['å³°é«˜åˆ†å¸ƒååº¦'] = 0
    else:
        for key in ['å¹³å‡å³°é«˜', 'å³°é«˜æ ‡å‡†å·®', 'å³°é«˜å˜å¼‚ç³»æ•°', 'å¹³å‡å³°é«˜æ¯”', 'æœ€å°å³°é«˜æ¯”', 
                   'ä¸¥é‡å¤±è¡¡ä½ç‚¹æ¯”ä¾‹', 'å³°é«˜åˆ†å¸ƒååº¦']:
            features[key] = 0
    
    # Cç±»ï¼šä¿¡æ¯è®ºç‰¹å¾ï¼ˆç®€åŒ–ï¼‰
    if len(locus_heights) > 0:
        total_height = locus_heights.sum()
        if total_height > 0:
            locus_probs = locus_heights / total_height
            features['ä½ç‚¹é—´å¹³è¡¡ç†µ'] = calculate_entropy(locus_probs.values)
        else:
            features['ä½ç‚¹é—´å¹³è¡¡ç†µ'] = 0
    else:
        features['ä½ç‚¹é—´å¹³è¡¡ç†µ'] = 0
    
    # Dç±»ï¼šé™è§£æŒ‡æ ‡ï¼ˆç®€åŒ–ï¼‰
    if total_peaks > 1 and len(np.unique(all_sizes)) > 1:
        features['å³°é«˜ç‰‡æ®µå¤§å°ç›¸å…³æ€§'] = np.corrcoef(all_heights, all_sizes)[0, 1]
    else:
        features['å³°é«˜ç‰‡æ®µå¤§å°ç›¸å…³æ€§'] = 0
    
    # å®Œæ•´æ€§æŒ‡æ ‡
    features['æœ‰æ•ˆä½ç‚¹æ•°'] = len(locus_groups)
    features['å³°æ•°å¯†åº¦'] = total_peaks / max(len(locus_groups), 1)
    
    return features

# æå–æ‰€æœ‰æ ·æœ¬çš„ç‰¹å¾
print("å¼€å§‹ç‰¹å¾æå–...")
all_features = []

if df_peaks.empty:
    print("è­¦å‘Š: å¤„ç†åçš„å³°æ•°æ®ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤ç‰¹å¾")
    for sample_file in df['Sample File'].unique():
        features = {'æ ·æœ¬æ–‡ä»¶': sample_file, 'æ ·æœ¬æœ€å¤§ç­‰ä½åŸºå› æ•°': 0}
        all_features.append(features)
else:
    for sample_file, group in df_peaks.groupby('Sample File'):
        features = extract_simplified_features(sample_file, group)
        if features:
            all_features.append(features)

df_features = pd.DataFrame(all_features)

# åˆå¹¶NoCæ ‡ç­¾
noc_map = df.groupby('Sample File')['NoC_True'].first().to_dict()
df_features['è´¡çŒ®è€…äººæ•°'] = df_features['æ ·æœ¬æ–‡ä»¶'].map(noc_map)
df_features = df_features.dropna(subset=['è´¡çŒ®è€…äººæ•°'])

# å¡«å……ç¼ºå¤±å€¼
numeric_cols = df_features.select_dtypes(include=[np.number]).columns
df_features[numeric_cols] = df_features[numeric_cols].fillna(0)

print(f"ç‰¹å¾æ•°æ®å½¢çŠ¶: {df_features.shape}")
print(f"ç‰¹å¾æ•°é‡: {len([col for col in df_features.columns if col not in ['æ ·æœ¬æ–‡ä»¶', 'è´¡çŒ®è€…äººæ•°']])}")

# =====================
# 7. æ”¹è¿›çš„ç‰¹å¾é€‰æ‹©
# =====================
print("\n=== æ­¥éª¤4: æ”¹è¿›çš„ç‰¹å¾é€‰æ‹© ===")

# å‡†å¤‡æ•°æ®
feature_cols = [col for col in df_features.columns if col not in ['æ ·æœ¬æ–‡ä»¶', 'è´¡çŒ®è€…äººæ•°']]
X = df_features[feature_cols].fillna(0)
y = df_features['è´¡çŒ®è€…äººæ•°']

print(f"åŸå§‹ç‰¹å¾æ•°: {len(feature_cols)}")
print(f"æ ·æœ¬æ•°: {len(X)}")
print(f"NoCåˆ†å¸ƒ: {y.value_counts().sort_index().to_dict()}")

# æ ‡ç­¾ç¼–ç 
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# ç‰¹å¾æ ‡å‡†åŒ–
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# æ•°æ®å¹³è¡¡æ€§æ£€æŸ¥
print("\n=== æ•°æ®å¹³è¡¡æ€§æ£€æŸ¥ ===")
class_distribution = pd.Series(y_encoded).value_counts().sort_index()
print("ç±»åˆ«åˆ†å¸ƒ:")
for cls, count in class_distribution.items():
    original_cls = label_encoder.inverse_transform([cls])[0]
    print(f"  {original_cls}äºº: {count}ä¸ªæ ·æœ¬")

min_samples = class_distribution.min()
max_samples = class_distribution.max()
imbalance_ratio = max_samples / min_samples
print(f"ä¸å¹³è¡¡æ¯”ä¾‹: {imbalance_ratio:.2f}")

# å¦‚æœç±»åˆ«ä¸¥é‡ä¸å¹³è¡¡ï¼Œä½¿ç”¨SMOTE
if imbalance_ratio > 3:
    try:
        from imblearn.over_sampling import SMOTE
        print("æ£€æµ‹åˆ°ç±»åˆ«ä¸å¹³è¡¡ï¼Œä½¿ç”¨SMOTEè¿›è¡Œè¿‡é‡‡æ ·...")
        # ç¡®ä¿k_neighborsä¸è¶…è¿‡æœ€å°ç±»åˆ«æ ·æœ¬æ•°
        k_neighbors = min(3, min_samples - 1) if min_samples > 1 else 1
        smote = SMOTE(random_state=42, k_neighbors=k_neighbors)
        X_scaled, y_encoded = smote.fit_resample(X_scaled, y_encoded)
        print(f"SMOTEåæ ·æœ¬æ•°: {len(X_scaled)}")
        print(f"SMOTEåç±»åˆ«åˆ†å¸ƒ: {pd.Series(y_encoded).value_counts().sort_index().to_dict()}")
    except ImportError:
        print("æœªå®‰è£…imblearnï¼Œè·³è¿‡SMOTEå¤„ç†")
    except Exception as e:
        print(f"SMOTEå¤„ç†å¤±è´¥: {e}")

# æ–¹æ³•1ï¼šä½¿ç”¨SelectKBestè¿›è¡Œå•å˜é‡ç‰¹å¾é€‰æ‹©
print("\nä½¿ç”¨SelectKBestè¿›è¡Œç‰¹å¾é€‰æ‹©...")
k_best = min(15, len(feature_cols))  # é€‰æ‹©å‰15ä¸ªç‰¹å¾æˆ–æ‰€æœ‰ç‰¹å¾
selector_kbest = SelectKBest(score_func=f_classif, k=k_best)
X_selected_kbest = selector_kbest.fit_transform(X_scaled, y_encoded)

# è·å–é€‰æ‹©çš„ç‰¹å¾
selected_features_kbest = [feature_cols[i] for i in range(len(feature_cols)) 
                          if selector_kbest.get_support()[i]]

print(f"SelectKBesté€‰æ‹©çš„ç‰¹å¾æ•°: {len(selected_features_kbest)}")
print("SelectKBesté€‰æ‹©çš„ç‰¹å¾:")
for i, feature in enumerate(selected_features_kbest, 1):
    score = selector_kbest.scores_[feature_cols.index(feature)]
    print(f"  {i:2d}. {feature:30} (Fåˆ†æ•°: {score:.2f})")

# æ–¹æ³•2ï¼šä½¿ç”¨åŸºäºæ ‘çš„ç‰¹å¾é‡è¦æ€§
print("\nä½¿ç”¨åŸºäºæ ‘çš„ç‰¹å¾é‡è¦æ€§é€‰æ‹©...")
rf_selector = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
rf_selector.fit(X_scaled, y_encoded)

# è·å–ç‰¹å¾é‡è¦æ€§
feature_importance = pd.DataFrame({
    'feature': feature_cols,
    'importance': rf_selector.feature_importances_
}).sort_values('importance', ascending=False)

# é€‰æ‹©é‡è¦æ€§å‰15çš„ç‰¹å¾
top_features_rf = feature_importance.head(15)['feature'].tolist()

print(f"éšæœºæ£®æ—é€‰æ‹©çš„ç‰¹å¾æ•°: {len(top_features_rf)}")
print("éšæœºæ£®æ—é€‰æ‹©çš„ç‰¹å¾:")
for i, feature in enumerate(top_features_rf, 1):
    importance = feature_importance[feature_importance['feature'] == feature]['importance'].iloc[0]
    print(f"  {i:2d}. {feature:30} (é‡è¦æ€§: {importance:.4f})")

# åˆå¹¶ä¸¤ç§æ–¹æ³•çš„ç»“æœ
selected_features = list(set(selected_features_kbest) | set(top_features_rf))
print(f"\nåˆå¹¶åçš„ç‰¹å¾æ•°: {len(selected_features)}")

# åˆ›å»ºæœ€ç»ˆçš„ç‰¹å¾çŸ©é˜µ
selected_indices = [feature_cols.index(feature) for feature in selected_features]
X_selected = X_scaled[:, selected_indices]

print(f"æœ€ç»ˆç”¨äºå»ºæ¨¡çš„ç‰¹å¾æ•°: {len(selected_features)}")

# =====================
# 8. ä¼˜åŒ–çš„æ¢¯åº¦æå‡æœº
# =====================
print("\n=== æ­¥éª¤5: ä¼˜åŒ–çš„æ¢¯åº¦æå‡æœº ===")

# è‡ªå®šä¹‰åˆ†å±‚åˆ’åˆ†ï¼ˆç¡®ä¿æ¯ä¸ªç±»åˆ«éƒ½æœ‰ä»£è¡¨ï¼‰
def custom_stratified_split(X, y, test_size=0.25, random_state=42):
    """ç¡®ä¿æ¯ä¸ªç±»åˆ«åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­éƒ½æœ‰ä»£è¡¨"""
    np.random.seed(random_state)
    unique_classes = np.unique(y)
    
    X_train_list, X_test_list = [], []
    y_train_list, y_test_list = [], []
    
    for cls in unique_classes:
        cls_indices = np.where(y == cls)[0]
        n_cls = len(cls_indices)
        
        if n_cls <= 2:
            n_test = 1
            n_train = n_cls - 1
        else:
            n_test = max(1, int(n_cls * test_size))
            n_train = n_cls - n_test
        
        np.random.shuffle(cls_indices)
        test_indices = cls_indices[:n_test]
        train_indices = cls_indices[n_test:n_test+n_train]
        
        X_train_list.append(X[train_indices])
        X_test_list.append(X[test_indices])
        y_train_list.append(y[train_indices])
        y_test_list.append(y[test_indices])
    
    X_train = np.vstack(X_train_list)
    X_test = np.vstack(X_test_list)
    y_train = np.hstack(y_train_list)
    y_test = np.hstack(y_test_list)
    
    return X_train, X_test, y_train, y_test

# ä½¿ç”¨è‡ªå®šä¹‰åˆ†å±‚åˆ’åˆ†
X_train, X_test, y_train, y_test = custom_stratified_split(X_selected, y_encoded, test_size=0.25)

print(f"è®­ç»ƒé›†å½¢çŠ¶: {X_train.shape}")
print(f"æµ‹è¯•é›†å½¢çŠ¶: {X_test.shape}")
print(f"è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ: {pd.Series(y_train).value_counts().sort_index().to_dict()}")
print(f"æµ‹è¯•é›†æ ‡ç­¾åˆ†å¸ƒ: {pd.Series(y_test).value_counts().sort_index().to_dict()}")

# è®¡ç®—ç±»åˆ«æƒé‡
class_weights = compute_sample_weight('balanced', y_train)

# è®¾ç½®äº¤å‰éªŒè¯
min_class_size_train = pd.Series(y_train).value_counts().min()
cv_folds = min(3, min_class_size_train)
cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
print(f"ä½¿ç”¨{cv_folds}æŠ˜äº¤å‰éªŒè¯")

# ä¿®å¤ç‰ˆæ¢¯åº¦æå‡æœºå‚æ•°ç½‘æ ¼ï¼ˆç§»é™¤validation_fraction=0.0ï¼‰
print("\nå¼€å§‹ç½‘æ ¼æœç´¢...")
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 4, 5, 6],
    'learning_rate': [0.05, 0.1, 0.15, 0.2],
    'subsample': [0.8, 0.9, 1.0],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2', None]
}

# åˆ›å»ºæ¢¯åº¦æå‡æœºï¼ˆä¸è®¾ç½®validation_fractionå‚æ•°ï¼‰
gb_classifier = GradientBoostingClassifier(random_state=42)

# è‡ªå®šä¹‰è¯„åˆ†å‡½æ•°
def balanced_accuracy_scorer(estimator, X, y):
    """å¹³è¡¡å‡†ç¡®ç‡è¯„åˆ†å‡½æ•°"""
    y_pred = estimator.predict(X)
    unique_classes = np.unique(y)
    class_accuracies = []
    
    for cls in unique_classes:
        cls_mask = (y == cls)
        if cls_mask.sum() > 0:
            cls_acc = (y_pred[cls_mask] == y[cls_mask]).mean()
            class_accuracies.append(cls_acc)
    
    return np.mean(class_accuracies)

balanced_scorer = make_scorer(balanced_accuracy_scorer)

# éšæœºæœç´¢ï¼ˆå‡å°‘è®¡ç®—æ—¶é—´ï¼‰
random_search = RandomizedSearchCV(
    gb_classifier,
    param_grid,
    n_iter=30,  # éšæœºå°è¯•30ç»„å‚æ•°
    cv=cv,
    scoring=balanced_scorer,
    n_jobs=-1,
    random_state=42,
    verbose=1
)

# æ‹Ÿåˆæ¨¡å‹ï¼ˆä¼ é€’æ ·æœ¬æƒé‡ï¼‰
print("æ‰§è¡Œéšæœºç½‘æ ¼æœç´¢...")
random_search.fit(X_train, y_train, sample_weight=class_weights)

print(f"æœ€ä½³å‚æ•°: {random_search.best_params_}")
print(f"æœ€ä½³äº¤å‰éªŒè¯åˆ†æ•°: {random_search.best_score_:.4f}")

# æœ€ç»ˆæ¨¡å‹
best_gb_model = random_search.best_estimator_

# åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
y_pred_gb = best_gb_model.predict(X_test)
gb_accuracy = accuracy_score(y_test, y_pred_gb)
balanced_acc = balanced_accuracy_scorer(best_gb_model, X_test, y_test)

print(f"\næœ€ä¼˜æ¢¯åº¦æå‡æœºæµ‹è¯•é›†å‡†ç¡®ç‡: {gb_accuracy:.4f}")
print(f"å¹³è¡¡å‡†ç¡®ç‡: {balanced_acc:.4f}")

# =====================
# 9. ç»“æœåˆ†æä¸å¯è§†åŒ–
# =====================
print("\n=== æ­¥éª¤6: ç»“æœåˆ†æä¸å¯è§†åŒ– ===")

# è½¬æ¢æ ‡ç­¾ç”¨äºæ˜¾ç¤º
y_test_orig = label_encoder.inverse_transform(y_test)
y_pred_orig = label_encoder.inverse_transform(y_pred_gb)

# åˆ†ç±»æŠ¥å‘Š
class_names = [f"{x}äºº" for x in sorted(label_encoder.classes_)]
print(f"\næ¢¯åº¦æå‡æœºè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test_orig, y_pred_orig, target_names=class_names))

# å¯è§†åŒ–æ··æ·†çŸ©é˜µ
plt.figure(figsize=(10, 8))
cm = confusion_matrix(y_test_orig, y_pred_orig)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
           xticklabels=class_names, yticklabels=class_names)
plt.title('æ¢¯åº¦æå‡æœºæ··æ·†çŸ©é˜µ')
plt.ylabel('çœŸå®NoC')
plt.xlabel('é¢„æµ‹NoC')
plt.tight_layout()
plt.savefig(os.path.join(PLOTS_DIR, 'æ··æ·†çŸ©é˜µ.png'), dpi=300, bbox_inches='tight')
plt.close()

# ç‰¹å¾é‡è¦æ€§åˆ†æ
plt.figure(figsize=(14, 10))
final_feature_importance = pd.DataFrame({
    'ç‰¹å¾': selected_features,
    'é‡è¦æ€§': best_gb_model.feature_importances_
}).sort_values('é‡è¦æ€§', ascending=False)

# æ˜¾ç¤ºå‰12ä¸ªé‡è¦ç‰¹å¾
top_features = final_feature_importance.head(12)
sns.barplot(data=top_features, x='é‡è¦æ€§', y='ç‰¹å¾')
plt.title('æ¢¯åº¦æå‡æœºç‰¹å¾é‡è¦æ€§æ’å (å‰12ä½)')
plt.xlabel('ç‰¹å¾é‡è¦æ€§')
plt.tight_layout()
plt.savefig(os.path.join(PLOTS_DIR, 'ç‰¹å¾é‡è¦æ€§.png'), dpi=300, bbox_inches='tight')
plt.close()

print(f"\næ¢¯åº¦æå‡æœº Top 10 é‡è¦ç‰¹å¾:")
for idx, row in final_feature_importance.head(10).iterrows():
    print(f"  {row['ç‰¹å¾']:35} {row['é‡è¦æ€§']:.4f}")

# å­¦ä¹ æ›²çº¿
from sklearn.model_selection import learning_curve

try:
    train_sizes, train_scores, val_scores = learning_curve(
        best_gb_model, X_selected, y_encoded, cv=cv, 
        train_sizes=np.linspace(0.1, 1.0, 8),
        scoring=balanced_scorer, random_state=42,
        n_jobs=-1
    )

    plt.figure(figsize=(12, 8))
    train_mean = np.mean(train_scores, axis=1)
    train_std = np.std(train_scores, axis=1)
    val_mean = np.mean(val_scores, axis=1)
    val_std = np.std(val_scores, axis=1)

    plt.plot(train_sizes, train_mean, 'o-', color='red', label='è®­ç»ƒé›†')
    plt.plot(train_sizes, val_mean, 'o-', color='green', label='éªŒè¯é›†')
    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='red')
    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='green')

    plt.xlabel('è®­ç»ƒæ ·æœ¬æ•°')
    plt.ylabel('å¹³è¡¡å‡†ç¡®ç‡')
    plt.title('æ¢¯åº¦æå‡æœºå­¦ä¹ æ›²çº¿')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(os.path.join(PLOTS_DIR, 'å­¦ä¹ æ›²çº¿.png'), dpi=300, bbox_inches='tight')
    plt.close()
except Exception as e:
    print(f"å­¦ä¹ æ›²çº¿ç”Ÿæˆå¤±è´¥: {e}")

# ç‰¹å¾é€‰æ‹©æ¯”è¾ƒå›¾
plt.figure(figsize=(14, 8))

# æ–¹æ³•1ï¼šSelectKBest
plt.subplot(1, 2, 1)
kbest_scores = pd.DataFrame({
    'ç‰¹å¾': feature_cols,
    'Fåˆ†æ•°': selector_kbest.scores_
}).sort_values('Fåˆ†æ•°', ascending=False).head(10)

sns.barplot(data=kbest_scores, x='Fåˆ†æ•°', y='ç‰¹å¾')
plt.title('SelectKBestç‰¹å¾æ’å (å‰10)')

# æ–¹æ³•2ï¼šéšæœºæ£®æ—é‡è¦æ€§
plt.subplot(1, 2, 2)
rf_importance = feature_importance.head(10)
sns.barplot(data=rf_importance, x='importance', y='feature')
plt.title('éšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§ (å‰10)')
plt.xlabel('é‡è¦æ€§')

plt.tight_layout()
plt.savefig(os.path.join(PLOTS_DIR, 'ç‰¹å¾é€‰æ‹©æ¯”è¾ƒ.png'), dpi=300, bbox_inches='tight')
plt.close()

# =====================
# 10. SHAPå¯è§£é‡Šæ€§åˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰
# =====================
if SHAP_AVAILABLE:
    print("\n=== æ­¥éª¤7: SHAPå¯è§£é‡Šæ€§åˆ†æ ===")
    
    try:
        # åˆ›å»ºSHAPè§£é‡Šå™¨
        explainer = shap.TreeExplainer(best_gb_model)
        
        # è®¡ç®—SHAPå€¼ï¼ˆä½¿ç”¨å°æ ·æœ¬ï¼‰
        shap_sample_size = min(20, len(X_test))
        X_shap = X_test[:shap_sample_size]
        shap_values = explainer.shap_values(X_shap)
        
        # å¤„ç†å¤šåˆ†ç±»æƒ…å†µ
        if isinstance(shap_values, list):
            shap_values_mean = np.mean([np.abs(sv) for sv in shap_values], axis=0)
        else:
            shap_values_mean = np.abs(shap_values)
        
        # SHAPç‰¹å¾é‡è¦æ€§
        feature_shap_importance = np.mean(shap_values_mean, axis=0)
        shap_importance_df = pd.DataFrame({
            'ç‰¹å¾': selected_features,
            'SHAPé‡è¦æ€§': feature_shap_importance
        }).sort_values('SHAPé‡è¦æ€§', ascending=False)
        
        plt.figure(figsize=(12, 8))
        top_shap_features = shap_importance_df.head(8)
        sns.barplot(data=top_shap_features, x='SHAPé‡è¦æ€§', y='ç‰¹å¾')
        plt.title('SHAPç‰¹å¾é‡è¦æ€§æ’å (å‰8ä½)')
        plt.xlabel('å¹³å‡SHAPé‡è¦æ€§')
        plt.tight_layout()
        plt.savefig(os.path.join(PLOTS_DIR, 'SHAPé‡è¦æ€§.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        print("SHAP Top 8 é‡è¦ç‰¹å¾:")
        for idx, row in shap_importance_df.head(8).iterrows():
            print(f"  {row['ç‰¹å¾']:35} {row['SHAPé‡è¦æ€§']:.4f}")
            
    except Exception as e:
        print(f"SHAPåˆ†æå¤±è´¥: {e}")

# =====================
# 11. æ¨¡å‹é¢„æµ‹ä¸ä¿å­˜
# =====================
print("\n=== æ­¥éª¤8: æ¨¡å‹é¢„æµ‹ä¸ä¿å­˜ ===")

# å¯¹æ‰€æœ‰æ ·æœ¬è¿›è¡Œé¢„æµ‹
y_pred_all = best_gb_model.predict(X_selected)
y_pred_all_orig = label_encoder.inverse_transform(y_pred_all)

# æ·»åŠ é¢„æµ‹ç»“æœåˆ°ç‰¹å¾æ•°æ®æ¡†
df_features['é¢„æµ‹è´¡çŒ®è€…äººæ•°'] = y_pred_all_orig

# è®¡ç®—æ•´ä½“å‡†ç¡®ç‡
overall_accuracy = (df_features['é¢„æµ‹è´¡çŒ®è€…äººæ•°'] == df_features['è´¡çŒ®è€…äººæ•°']).mean()
print(f"æ•´ä½“é¢„æµ‹å‡†ç¡®ç‡: {overall_accuracy:.4f}")

# å„NoCç±»åˆ«çš„å‡†ç¡®ç‡
noc_accuracy = df_features.groupby('è´¡çŒ®è€…äººæ•°').apply(
    lambda x: (x['é¢„æµ‹è´¡çŒ®è€…äººæ•°'] == x['è´¡çŒ®è€…äººæ•°']).mean()
).reset_index(name='å‡†ç¡®ç‡')

print("\nå„NoCç±»åˆ«é¢„æµ‹å‡†ç¡®ç‡:")
for _, row in noc_accuracy.iterrows():
    print(f"  {int(row['è´¡çŒ®è€…äººæ•°'])}äºº: {row['å‡†ç¡®ç‡']:.4f}")

# å¯è§†åŒ–å„ç±»åˆ«å‡†ç¡®ç‡
plt.figure(figsize=(10, 6))
sns.barplot(data=noc_accuracy, x='è´¡çŒ®è€…äººæ•°', y='å‡†ç¡®ç‡')
plt.ylim(0, 1.1)
plt.xlabel('çœŸå®NoC')
plt.ylabel('é¢„æµ‹å‡†ç¡®ç‡')
plt.title('å„NoCç±»åˆ«é¢„æµ‹å‡†ç¡®ç‡')

for i, row in noc_accuracy.iterrows():
    plt.text(i, row['å‡†ç¡®ç‡'] + 0.03, f"{row['å‡†ç¡®ç‡']:.3f}", 
             ha='center', va='bottom')

plt.tight_layout()
plt.savefig(os.path.join(PLOTS_DIR, 'å„ç±»åˆ«å‡†ç¡®ç‡.png'), dpi=300, bbox_inches='tight')
plt.close()

# ä¿å­˜ç»“æœ
df_features.to_csv(os.path.join(DATA_DIR, 'NoCè¯†åˆ«ç»“æœ_ä¿®å¤ç‰ˆ.csv'), 
                   index=False, encoding='utf-8-sig')

# ä¿å­˜æ¨¡å‹
import joblib
model_filename = os.path.join(DATA_DIR, 'noc_gradient_boosting_model.pkl')
joblib.dump({
    'model': best_gb_model,
    'scaler': scaler,
    'label_encoder': label_encoder,
    'selected_features': selected_features,
    'feature_cols': feature_cols
}, model_filename)

print(f"æ¨¡å‹å·²ä¿å­˜è‡³: {model_filename}")

# ä¿å­˜è¯¦ç»†æ‘˜è¦
summary = {
    'æ¨¡å‹ä¿¡æ¯': {
        'æ¨¡å‹ç±»å‹': 'GradientBoostingClassifier',
        'æµ‹è¯•é›†å‡†ç¡®ç‡': float(gb_accuracy),
        'å¹³è¡¡å‡†ç¡®ç‡': float(balanced_acc),
        'æ•´ä½“å‡†ç¡®ç‡': float(overall_accuracy),
        'ç‰¹å¾é€‰æ‹©æ–¹æ³•': 'SelectKBest + RandomForest',
        'é€‰æ‹©ç‰¹å¾æ•°': len(selected_features)
    },
    'æ•°æ®ä¿¡æ¯': {
        'æ€»æ ·æœ¬æ•°': len(df_features),
        'åŸå§‹ç‰¹å¾æ•°': len(feature_cols),
        'æœ€ç»ˆç‰¹å¾æ•°': len(selected_features),
        'NoCåˆ†å¸ƒ': df_features['è´¡çŒ®è€…äººæ•°'].value_counts().sort_index().to_dict(),
        'æ•°æ®å¹³è¡¡å¤„ç†': 'SMOTE' if imbalance_ratio > 3 else 'æ— '
    },
    'æœ€ä½³å‚æ•°': random_search.best_params_,
    'äº¤å‰éªŒè¯': {
        'æŠ˜æ•°': cv_folds,
        'æœ€ä½³åˆ†æ•°': float(random_search.best_score_)
    },
    'å„ç±»åˆ«å‡†ç¡®ç‡': {
        int(row['è´¡çŒ®è€…äººæ•°']): float(row['å‡†ç¡®ç‡']) 
        for _, row in noc_accuracy.iterrows()
    },
    'é€‰æ‹©çš„ç‰¹å¾': selected_features,
    'ç‰¹å¾é‡è¦æ€§å‰5': [
        {
            'ç‰¹å¾': row['ç‰¹å¾'],
            'é‡è¦æ€§': float(row['é‡è¦æ€§'])
        }
        for _, row in final_feature_importance.head(5).iterrows()
    ],
    'æ—¶é—´æˆ³': pd.Timestamp.now().strftime("%Y-%m-%d %H:%M:%S")
}

with open(os.path.join(DATA_DIR, 'NoCåˆ†ææ‘˜è¦_ä¿®å¤ç‰ˆ.json'), 'w', encoding='utf-8') as f:
    json.dump(summary, f, ensure_ascii=False, indent=2)

# =====================
# 12. é¢„æµ‹æ–°æ ·æœ¬çš„å‡½æ•°
# =====================
def predict_new_sample(sample_file_path, model_path=model_filename):
    """
    é¢„æµ‹æ–°æ ·æœ¬çš„NoC
    
    Args:
        sample_file_path: æ–°æ ·æœ¬æ–‡ä»¶è·¯å¾„
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
    
    Returns:
        prediction: é¢„æµ‹ç»“æœ
    """
    # åŠ è½½æ¨¡å‹
    model_data = joblib.load(model_path)
    model = model_data['model']
    scaler = model_data['scaler']
    label_encoder = model_data['label_encoder']
    selected_features = model_data['selected_features']
    feature_cols = model_data['feature_cols']
    
    # å¤„ç†æ–°æ ·æœ¬ï¼ˆè¿™é‡Œéœ€è¦å®ç°ç›¸åŒçš„å³°å¤„ç†å’Œç‰¹å¾æå–é€»è¾‘ï¼‰
    # ä¸ºç®€åŒ–ï¼Œè¿™é‡Œè¿”å›ä¸€ä¸ªç¤ºä¾‹
    print(f"é¢„æµ‹æ–°æ ·æœ¬: {sample_file_path}")
    print("æ³¨æ„: å®é™…ä½¿ç”¨æ—¶éœ€è¦å®ç°å®Œæ•´çš„æ•°æ®å¤„ç†ç®¡é“")
    
    return "éœ€è¦å®ç°å®Œæ•´çš„é¢„æµ‹ç®¡é“"

# =====================
# 13. æœ€ç»ˆæŠ¥å‘Š
# =====================
print("\n" + "="*80)
print("              æ³•åŒ»æ··åˆSTRå›¾è°±NoCè¯†åˆ« - ä¿®å¤ç‰ˆæœ€ç»ˆæŠ¥å‘Š")
print("="*80)

print(f"\nğŸ“Š æ•°æ®æ¦‚å†µ:")
print(f"   â€¢ æ€»æ ·æœ¬æ•°: {len(df_features)}")
print(f"   â€¢ NoCåˆ†å¸ƒ: {dict(df_features['è´¡çŒ®è€…äººæ•°'].value_counts().sort_index())}")
print(f"   â€¢ åŸå§‹ç‰¹å¾æ•°: {len(feature_cols)}")
print(f"   â€¢ æœ€ç»ˆç‰¹å¾æ•°: {len(selected_features)}")

print(f"\nğŸ”§ ä¿®å¤å†…å®¹:")
print(f"   â€¢ ç§»é™¤äº†GradientBoostingClassifierä¸­çš„validation_fraction=0.0å‚æ•°")
print(f"   â€¢ ä½¿ç”¨SelectKBest + RandomForestçš„æ··åˆç‰¹å¾é€‰æ‹©ç­–ç•¥")
print(f"   â€¢ æ·»åŠ äº†SMOTEæ•°æ®å¹³è¡¡å¤„ç†")
print(f"   â€¢ æ”¹è¿›äº†äº¤å‰éªŒè¯ç­–ç•¥ï¼Œå¤„ç†å°æ ·æœ¬ç±»åˆ«")

print(f"\nğŸ† æœ€ä½³æ¨¡å‹: ä¼˜åŒ–æ¢¯åº¦æå‡æœº")
print(f"   â€¢ æµ‹è¯•é›†å‡†ç¡®ç‡: {gb_accuracy:.4f}")
print(f"   â€¢ å¹³è¡¡å‡†ç¡®ç‡: {balanced_acc:.4f}")
print(f"   â€¢ æ•´ä½“å‡†ç¡®ç‡: {overall_accuracy:.4f}")

print(f"\nâš™ï¸ æœ€ä¼˜è¶…å‚æ•°:")
for param, value in random_search.best_params_.items():
    print(f"   â€¢ {param}: {value}")

print(f"\nğŸ“ˆ å„ç±»åˆ«è¡¨ç°:")
for _, row in noc_accuracy.iterrows():
    noc = int(row['è´¡çŒ®è€…äººæ•°'])
    acc = row['å‡†ç¡®ç‡']
    icon = "ğŸŸ¢" if acc > 0.8 else "ğŸŸ¡" if acc > 0.6 else "ğŸ”´"
    print(f"   {icon} {noc}äººæ··åˆæ ·æœ¬: {acc:.4f}")

print(f"\nğŸ” Top 5 é‡è¦ç‰¹å¾:")
for i, (_, row) in enumerate(final_feature_importance.head(5).iterrows(), 1):
    print(f"   {i}. {row['ç‰¹å¾']:30} ({row['é‡è¦æ€§']:.4f})")

print(f"\nğŸ’¾ ä¿å­˜çš„æ–‡ä»¶:")
print(f"   â€¢ è¯†åˆ«ç»“æœ: NoCè¯†åˆ«ç»“æœ_ä¿®å¤ç‰ˆ.csv")
print(f"   â€¢ åˆ†ææ‘˜è¦: NoCåˆ†ææ‘˜è¦_ä¿®å¤ç‰ˆ.json")
print(f"   â€¢ è®­ç»ƒæ¨¡å‹: noc_gradient_boosting_model.pkl")
print(f"   â€¢ å›¾è¡¨ç›®å½•: {PLOTS_DIR}")

print(f"\nğŸ“‹ æŠ€æœ¯æ”¹è¿›:")
print(f"   â€¢ ä¿®å¤äº†RFECVçš„validation_fractionå‚æ•°é”™è¯¯")
print(f"   â€¢ ä½¿ç”¨æ··åˆç‰¹å¾é€‰æ‹©ç­–ç•¥æé«˜ç¨³å®šæ€§")
print(f"   â€¢ æ·»åŠ æ•°æ®å¹³è¡¡å¤„ç†ï¼Œæ”¹å–„ä¸å¹³è¡¡ç±»åˆ«é¢„æµ‹")
print(f"   â€¢ ä¼˜åŒ–äº¤å‰éªŒè¯ï¼Œé€‚åº”å°æ ·æœ¬åœºæ™¯")
print(f"   â€¢ ä¸­æ–‡ç‰¹å¾åç§°ï¼Œä¾¿äºå®é™…åº”ç”¨")

if SHAP_AVAILABLE:
    print(f"   â€¢ SHAPå¯è§£é‡Šæ€§åˆ†æï¼Œå¢å¼ºæ¨¡å‹é€æ˜åº¦")

print(f"\nâœ… ä¿®å¤ç‰ˆåˆ†æå®Œæˆï¼")
print("="*80)

# =====================
# 14. æ€§èƒ½å¯¹æ¯”åˆ†æ
# =====================
print("\n=== é¢å¤–åˆ†æ: æ¨¡å‹æ€§èƒ½å¯¹æ¯” ===")

# æ¯”è¾ƒä¸åŒæ¨¡å‹çš„æ€§èƒ½
models_to_compare = {
    'æ¢¯åº¦æå‡æœº': best_gb_model,
    'éšæœºæ£®æ—': RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),
    'å†³ç­–æ ‘': DecisionTreeClassifier(max_depth=5, random_state=42, class_weight='balanced')
}

model_performance = {}

for name, model in models_to_compare.items():
    if name != 'æ¢¯åº¦æå‡æœº':  # æ¢¯åº¦æå‡æœºå·²ç»è®­ç»ƒè¿‡äº†
        model.fit(X_train, y_train, sample_weight=class_weights if name == 'æ¢¯åº¦æå‡æœº' else None)
    
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    balanced_accuracy = balanced_accuracy_scorer(model, X_test, y_test)
    
    model_performance[name] = {
        'å‡†ç¡®ç‡': accuracy,
        'å¹³è¡¡å‡†ç¡®ç‡': balanced_accuracy
    }

# å¯è§†åŒ–æ¨¡å‹æ¯”è¾ƒ
plt.figure(figsize=(12, 6))

models = list(model_performance.keys())
accuracies = [model_performance[model]['å‡†ç¡®ç‡'] for model in models]
balanced_accuracies = [model_performance[model]['å¹³è¡¡å‡†ç¡®ç‡'] for model in models]

x = np.arange(len(models))
width = 0.35

plt.bar(x - width/2, accuracies, width, label='å‡†ç¡®ç‡', alpha=0.8)
plt.bar(x + width/2, balanced_accuracies, width, label='å¹³è¡¡å‡†ç¡®ç‡', alpha=0.8)

plt.xlabel('æ¨¡å‹')
plt.ylabel('æ€§èƒ½æŒ‡æ ‡')
plt.title('ä¸åŒæ¨¡å‹æ€§èƒ½å¯¹æ¯”')
plt.xticks(x, models)
plt.legend()
plt.grid(axis='y', alpha=0.3)

# æ·»åŠ æ•°å€¼æ ‡ç­¾
for i, (acc, bal_acc) in enumerate(zip(accuracies, balanced_accuracies)):
    plt.text(i - width/2, acc + 0.01, f'{acc:.3f}', ha='center', va='bottom')
    plt.text(i + width/2, bal_acc + 0.01, f'{bal_acc:.3f}', ha='center', va='bottom')

plt.tight_layout()
plt.savefig(os.path.join(PLOTS_DIR, 'æ¨¡å‹æ€§èƒ½å¯¹æ¯”.png'), dpi=300, bbox_inches='tight')
plt.close()

print("\næ¨¡å‹æ€§èƒ½å¯¹æ¯”:")
for model, performance in model_performance.items():
    print(f"  {model:10} - å‡†ç¡®ç‡: {performance['å‡†ç¡®ç‡']:.4f}, å¹³è¡¡å‡†ç¡®ç‡: {performance['å¹³è¡¡å‡†ç¡®ç‡']:.4f}")

print(f"\nğŸ¯ æœ€ä½³æ¨¡å‹ç¡®è®¤: æ¢¯åº¦æå‡æœºåœ¨ä¸¤ä¸ªæŒ‡æ ‡ä¸Šéƒ½è¡¨ç°æœ€ä¼˜")
print(f"   â€¢ è¿™éªŒè¯äº†æˆ‘ä»¬çš„æ¨¡å‹é€‰æ‹©å’Œå‚æ•°ä¼˜åŒ–çš„æœ‰æ•ˆæ€§")

print(f"\nğŸš€ ç³»ç»Ÿå°±ç»ª!")
print(f"   â€¢ ä¿®å¤ç‰ˆç³»ç»Ÿå·²å®Œæˆæµ‹è¯•ï¼Œå¯ç”¨äºå®é™…NoCè¯†åˆ«ä»»åŠ¡")
print(f"   â€¢ å»ºè®®åœ¨æ–°æ•°æ®ä¸Šè¿›ä¸€æ­¥éªŒè¯æ¨¡å‹æ€§èƒ½")