# -*- coding: utf-8 -*-
"""
æ•°å­¦å»ºæ¨¡ - æ³•åŒ»DNAåˆ†æ - é—®é¢˜1ï¼šè´¡çŒ®è€…äººæ•°è¯†åˆ« (ä¿®å¤ç‰ˆ)

ç‰ˆæœ¬: V4.1 - Fixed RFECV Issue
æ—¥æœŸ: 2025-06-03
æè¿°: ä¿®å¤RFECV sample_weightå‚æ•°é—®é¢˜ï¼Œä½¿ç”¨æ›¿ä»£æ–¹æ¡ˆè¿›è¡Œç‰¹å¾é€‰æ‹©
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import warnings
import json
import re
from scipy import stats
from scipy.signal import find_peaks
from collections import Counter
from time import time
import random

# æœºå™¨å­¦ä¹ ç›¸å…³
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, VotingClassifier, ExtraTreesClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LassoCV
from sklearn.feature_selection import SelectFromModel, SelectKBest, f_classif
from sklearn.utils import resample
from sklearn.utils.class_weight import compute_sample_weight

# å¯è§£é‡Šæ€§åˆ†æ
try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    print("SHAPä¸å¯ç”¨ï¼Œå°†è·³è¿‡å¯è§£é‡Šæ€§åˆ†æ")
    SHAP_AVAILABLE = False

# é…ç½®
plt.rcParams['font.sans-serif'] = ["Arial Unicode MS", "SimHei"]
plt.rcParams['axes.unicode_minus'] = False
warnings.filterwarnings('ignore')

print("=== æ³•åŒ»æ··åˆSTRå›¾è°±NoCæ™ºèƒ½è¯†åˆ«ç³»ç»Ÿ (ä¿®å¤ç‰ˆ) ===")
print("åŸºäºé™„ä»¶ä¸€æ•°æ®çš„å®Œæ•´å®ç°")

# =====================
# 1. æ–‡ä»¶è·¯å¾„ä¸åŸºç¡€è®¾ç½®
# =====================
DATA_DIR = './'
file_path = os.path.join(DATA_DIR, 'é™„ä»¶1ï¼šä¸åŒäººæ•°çš„STRå›¾è°±æ•°æ®.csv')
PLOTS_DIR = os.path.join(DATA_DIR, 'noc_fixed_plots')
if not os.path.exists(PLOTS_DIR):
    os.makedirs(PLOTS_DIR)

# å…³é”®å‚æ•°è®¾ç½®
HEIGHT_THRESHOLD = 50
SATURATION_THRESHOLD = 30000
CTA_THRESHOLD = 0.5
PHR_IMBALANCE_THRESHOLD = 0.6

# =====================
# 2. è¾…åŠ©å‡½æ•°å®šä¹‰
# =====================
def extract_noc_from_filename(filename):
    """ä»æ–‡ä»¶åæå–è´¡çŒ®è€…äººæ•°ï¼ˆNoCï¼‰"""
    match = re.search(r'-(\d+(?:_\d+)*)-[\d;]+-', str(filename))
    if match:
        ids = [id_val for id_val in match.group(1).split('_') if id_val.isdigit()]
        return len(ids) if len(ids) > 0 else np.nan
    return np.nan

def calculate_entropy(probabilities):
    """è®¡ç®—é¦™å†œç†µ"""
    probabilities = np.array(probabilities)
    probabilities = probabilities[probabilities > 0]
    if len(probabilities) == 0:
        return 0.0
    return -np.sum(probabilities * np.log(probabilities + 1e-10))

def calculate_ols_slope(x, y):
    """è®¡ç®—OLSå›å½’æ–œç‡"""
    if len(x) < 2 or len(x) != len(y):
        return 0.0
    
    x = np.array(x)
    y = np.array(y)
    
    if len(np.unique(x)) < 2:
        return 0.0
    
    try:
        slope, _, _, _, _ = stats.linregress(x, y)
        return slope
    except:
        return 0.0

# =====================
# 3. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
# =====================
print("\n=== æ­¥éª¤1: æ•°æ®åŠ è½½ä¸é¢„å¤„ç† ===")

# åŠ è½½æ•°æ®
try:
    df = pd.read_csv(file_path, encoding='utf-8')
    print(f"æˆåŠŸåŠ è½½æ•°æ®ï¼Œå½¢çŠ¶: {df.shape}")
except Exception as e:
    print(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
    exit()

# æå–NoCæ ‡ç­¾
df['NoC_True'] = df['Sample File'].apply(extract_noc_from_filename)
df = df.dropna(subset=['NoC_True'])
df['NoC_True'] = df['NoC_True'].astype(int)

print(f"åŸå§‹æ•°æ®NoCåˆ†å¸ƒ: {df['NoC_True'].value_counts().sort_index().to_dict()}")
print(f"åŸå§‹æ ·æœ¬æ•°: {df['Sample File'].nunique()}")

# =====================
# 4. ç®€åŒ–çš„å³°å¤„ç†
# =====================
print("\n=== æ­¥éª¤2: å³°å¤„ç†ä¸ä¿¡å·è¡¨å¾ ===")

def process_peaks_with_cta(sample_data):
    """ç®€åŒ–çš„å³°å¤„ç†ï¼ŒåŒ…å«åŸºç¡€CTAè¯„ä¼°"""
    processed_data = []
    
    for _, sample_row in sample_data.iterrows():
        sample_file = sample_row['Sample File']
        marker = sample_row['Marker']
        
        # æå–æ‰€æœ‰å³°
        peaks = []
        for i in range(1, 101):
            allele = sample_row.get(f'Allele {i}')
            size = sample_row.get(f'Size {i}')
            height = sample_row.get(f'Height {i}')
            
            if pd.notna(allele) and pd.notna(size) and pd.notna(height):
                original_height = float(height)
                corrected_height = min(original_height, SATURATION_THRESHOLD)
                
                if corrected_height >= HEIGHT_THRESHOLD:
                    peaks.append({
                        'allele': allele,
                        'size': float(size),
                        'height': corrected_height,
                        'original_height': original_height
                    })
        
        if not peaks:
            continue
            
        # ç®€åŒ–çš„CTAè¯„ä¼°
        peaks.sort(key=lambda x: x['height'], reverse=True)
        
        for peak in peaks:
            height_rank = peaks.index(peak) + 1
            total_peaks = len(peaks)
            
            if height_rank == 1:
                cta = 0.95
            elif height_rank == 2 and total_peaks >= 2:
                height_ratio = peak['height'] / peaks[0]['height']
                cta = 0.8 if height_ratio > 0.3 else 0.6
            else:
                height_ratio = peak['height'] / peaks[0]['height']
                cta = max(0.1, min(0.8, height_ratio))
            
            if cta >= CTA_THRESHOLD:
                processed_data.append({
                    'Sample File': sample_file,
                    'Marker': marker,
                    'Allele': peak['allele'],
                    'Size': peak['size'],
                    'Height': peak['height'],
                    'Original_Height': peak['original_height'],
                    'CTA': cta
                })
    
    return pd.DataFrame(processed_data)

# å¤„ç†æ‰€æœ‰æ ·æœ¬
all_processed_peaks = []
unique_samples = df['Sample File'].nunique()
processed_count = 0

print(f"å¼€å§‹å¤„ç† {unique_samples} ä¸ªæ ·æœ¬çš„å³°æ•°æ®...")

for sample_file, group in df.groupby('Sample File'):
    processed_count += 1
    if processed_count % 50 == 0 or processed_count == unique_samples:
        print(f"å¤„ç†è¿›åº¦: {processed_count}/{unique_samples} ({processed_count/unique_samples*100:.1f}%)")
    
    sample_peaks = process_peaks_with_cta(group)
    if not sample_peaks.empty:
        all_processed_peaks.append(sample_peaks)

df_peaks = pd.concat(all_processed_peaks, ignore_index=True) if all_processed_peaks else pd.DataFrame()
print(f"å¤„ç†åçš„å³°æ•°æ®å½¢çŠ¶: {df_peaks.shape}")

# =====================
# 5. ç»¼åˆç‰¹å¾å·¥ç¨‹
# =====================
print("\n=== æ­¥éª¤3: ç»¼åˆç‰¹å¾å·¥ç¨‹ ===")

def extract_comprehensive_features_v5(sample_file, sample_peaks):
    """åŸºäºæ–‡æ¡£ç¬¬4èŠ‚çš„ç»¼åˆç‰¹å¾ä½“ç³»æ„å»º"""
    if sample_peaks.empty:
        return {}
    
    features = {'Sample File': sample_file}
    
    # åŸºç¡€æ•°æ®å‡†å¤‡
    total_peaks = len(sample_peaks)
    all_heights = sample_peaks['Height'].values
    all_sizes = sample_peaks['Size'].values
    
    # æŒ‰ä½ç‚¹åˆ†ç»„ç»Ÿè®¡
    locus_groups = sample_peaks.groupby('Marker')
    alleles_per_locus = locus_groups['Allele'].nunique()
    locus_heights = locus_groups['Height'].sum()
    
    expected_autosomal_count = 23
    
    # Aç±»ï¼šè°±å›¾å±‚é¢åŸºç¡€ç»Ÿè®¡ç‰¹å¾
    features['mac_profile'] = alleles_per_locus.max() if len(alleles_per_locus) > 0 else 0
    features['total_distinct_alleles'] = sample_peaks['Allele'].nunique()
    
    if expected_autosomal_count > 0:
        all_locus_counts = np.zeros(expected_autosomal_count)
        all_locus_counts[:len(alleles_per_locus)] = alleles_per_locus.values
        features['avg_alleles_per_locus'] = np.mean(all_locus_counts)
        features['std_alleles_per_locus'] = np.std(all_locus_counts)
    else:
        features['avg_alleles_per_locus'] = alleles_per_locus.mean() if len(alleles_per_locus) > 0 else 0
        features['std_alleles_per_locus'] = alleles_per_locus.std() if len(alleles_per_locus) > 1 else 0
    
    # MGTNç³»åˆ—
    for N in [2, 3, 4, 5, 6]:
        features[f'loci_gt{N}_alleles'] = (alleles_per_locus >= N).sum()
    
    # ç­‰ä½åŸºå› è®¡æ•°çš„ç†µ
    if len(alleles_per_locus) > 0:
        counts = alleles_per_locus.value_counts(normalize=True)
        features['allele_count_dist_entropy'] = calculate_entropy(counts.values)
    else:
        features['allele_count_dist_entropy'] = 0
    
    # Bç±»ï¼šå³°é«˜ã€å¹³è¡¡æ€§åŠéšæœºæ•ˆåº”ç‰¹å¾
    if total_peaks > 0:
        features['avg_peak_height'] = np.mean(all_heights)
        features['std_peak_height'] = np.std(all_heights) if total_peaks > 1 else 0
        features['min_peak_height'] = np.min(all_heights)
        features['max_peak_height'] = np.max(all_heights)
        
        # å³°é«˜æ¯”(PHR)ç›¸å…³ç»Ÿè®¡
        phr_values = []
        for marker, marker_group in locus_groups:
            if len(marker_group) == 2:
                heights = marker_group['Height'].values
                phr = min(heights) / max(heights) if max(heights) > 0 else 0
                phr_values.append(phr)
        
        if phr_values:
            features['avg_phr'] = np.mean(phr_values)
            features['std_phr'] = np.std(phr_values) if len(phr_values) > 1 else 0
            features['min_phr'] = np.min(phr_values)
            features['median_phr'] = np.median(phr_values)
            features['num_loci_with_phr'] = len(phr_values)
            features['num_severe_imbalance_loci'] = sum(phr <= PHR_IMBALANCE_THRESHOLD for phr in phr_values)
            features['ratio_severe_imbalance_loci'] = features['num_severe_imbalance_loci'] / len(phr_values)
        else:
            for key in ['avg_phr', 'std_phr', 'min_phr', 'median_phr', 'num_loci_with_phr', 
                       'num_severe_imbalance_loci', 'ratio_severe_imbalance_loci']:
                features[key] = 0
        
        # å³°é«˜åˆ†å¸ƒç»Ÿè®¡çŸ©
        if total_peaks > 2:
            features['skewness_peak_height'] = stats.skew(all_heights)
            features['kurtosis_peak_height'] = stats.kurtosis(all_heights, fisher=False)
        else:
            features['skewness_peak_height'] = 0
            features['kurtosis_peak_height'] = 0
        
        # å³°é«˜å¤šå³°æ€§
        try:
            log_heights = np.log(all_heights + 1)
            if len(np.unique(log_heights)) > 1:
                hist, _ = np.histogram(log_heights, bins=min(10, total_peaks))
                peaks_found, _ = find_peaks(hist)
                features['modality_peak_height'] = len(peaks_found)
            else:
                features['modality_peak_height'] = 1
        except:
            features['modality_peak_height'] = 1
        
        # é¥±å’Œæ•ˆåº”
        saturated_peaks = (sample_peaks['Original_Height'] >= SATURATION_THRESHOLD).sum()
        features['num_saturated_peaks'] = saturated_peaks
        features['ratio_saturated_peaks'] = saturated_peaks / total_peaks
    else:
        for key in ['avg_peak_height', 'std_peak_height', 'min_peak_height', 'max_peak_height',
                   'avg_phr', 'std_phr', 'min_phr', 'median_phr', 'num_loci_with_phr',
                   'num_severe_imbalance_loci', 'ratio_severe_imbalance_loci',
                   'skewness_peak_height', 'kurtosis_peak_height', 'modality_peak_height',
                   'num_saturated_peaks', 'ratio_saturated_peaks']:
            features[key] = 0
    
    # Cç±»ï¼šä¿¡æ¯è®ºåŠè°±å›¾å¤æ‚åº¦ç‰¹å¾
    if len(locus_heights) > 0:
        total_height = locus_heights.sum()
        if total_height > 0:
            locus_probs = locus_heights / total_height
            features['inter_locus_balance_entropy'] = calculate_entropy(locus_probs.values)
        else:
            features['inter_locus_balance_entropy'] = 0
    else:
        features['inter_locus_balance_entropy'] = 0
    
    # å¹³å‡ä½ç‚¹ç­‰ä½åŸºå› åˆ†å¸ƒç†µ
    locus_entropies = []
    for marker, marker_group in locus_groups:
        if len(marker_group) > 1:
            heights = marker_group['Height'].values
            height_sum = heights.sum()
            if height_sum > 0:
                probs = heights / height_sum
                entropy = calculate_entropy(probs)
                locus_entropies.append(entropy)
    
    features['avg_locus_allele_entropy'] = np.mean(locus_entropies) if locus_entropies else 0
    
    # æ ·æœ¬æ•´ä½“å³°é«˜åˆ†å¸ƒç†µ
    if total_peaks > 0:
        log_heights = np.log(all_heights + 1)
        hist, _ = np.histogram(log_heights, bins=min(15, total_peaks))
        hist_probs = hist / hist.sum()
        hist_probs = hist_probs[hist_probs > 0]
        features['peak_height_entropy'] = calculate_entropy(hist_probs)
    else:
        features['peak_height_entropy'] = 0
    
    # å›¾è°±å®Œæ•´æ€§æŒ‡æ ‡
    effective_loci_count = len(locus_groups)
    features['num_loci_with_effective_alleles'] = effective_loci_count
    features['num_loci_no_effective_alleles'] = max(0, expected_autosomal_count - effective_loci_count)
    
    # Dç±»ï¼šDNAé™è§£ä¸ä¿¡æ¯ä¸¢å¤±ç‰¹å¾
    if total_peaks > 1:
        if len(np.unique(all_heights)) > 1 and len(np.unique(all_sizes)) > 1:
            features['height_size_correlation'] = np.corrcoef(all_heights, all_sizes)[0, 1]
        else:
            features['height_size_correlation'] = 0
        
        features['height_size_slope'] = calculate_ols_slope(all_sizes, all_heights)
        
        try:
            weights = all_heights / all_heights.sum()
            weighted_correlation = np.average(all_sizes, weights=weights)
            features['weighted_height_size_slope'] = calculate_ols_slope(all_sizes, all_heights)
        except:
            features['weighted_height_size_slope'] = 0
        
        if len(phr_values) > 1:
            phr_sizes = []
            for marker, marker_group in locus_groups:
                if len(marker_group) == 2:
                    avg_size = marker_group['Size'].mean()
                    phr_sizes.append(avg_size)
            
            if len(phr_sizes) == len(phr_values) and len(phr_sizes) > 1:
                features['phr_size_slope'] = calculate_ols_slope(phr_sizes, phr_values)
            else:
                features['phr_size_slope'] = 0
        else:
            features['phr_size_slope'] = 0
        
    else:
        for key in ['height_size_correlation', 'height_size_slope', 'weighted_height_size_slope', 'phr_size_slope']:
            features[key] = 0
    
    # å…¶ä»–é™è§£ç›¸å…³ç‰¹å¾
    dropout_score = features['num_loci_no_effective_alleles'] / expected_autosomal_count if expected_autosomal_count > 0 else 0
    features['locus_dropout_score_weighted_by_size'] = dropout_score
    
    if len(locus_groups) > 1:
        locus_max_heights = []
        locus_avg_sizes = []
        for marker, marker_group in locus_groups:
            max_height = marker_group['Height'].max()
            avg_size = marker_group['Size'].mean()
            locus_max_heights.append(max_height)
            locus_avg_sizes.append(avg_size)
        
        features['degradation_index_rfu_per_bp'] = calculate_ols_slope(locus_avg_sizes, locus_max_heights)
    else:
        features['degradation_index_rfu_per_bp'] = 0
    
    # ç®€åŒ–çš„ä¿¡æ¯å®Œæ•´åº¦æ¯”ç‡
    small_fragment_loci = 0
    large_fragment_loci = 0
    small_fragment_effective = 0
    large_fragment_effective = 0
    
    for marker, marker_group in locus_groups:
        avg_size = marker_group['Size'].mean()
        if avg_size < 200:
            small_fragment_loci += 1
            small_fragment_effective += 1
        else:
            large_fragment_loci += 1
            large_fragment_effective += 1
    
    total_small_expected = expected_autosomal_count // 2
    total_large_expected = expected_autosomal_count - total_small_expected
    
    small_completeness = small_fragment_effective / total_small_expected if total_small_expected > 0 else 0
    large_completeness = large_fragment_effective / total_large_expected if total_large_expected > 0 else 0
    
    if large_completeness > 0:
        features['info_completeness_ratio_small_large'] = small_completeness / large_completeness
    else:
        features['info_completeness_ratio_small_large'] = small_completeness / 0.001
    
    return features

# æå–æ‰€æœ‰æ ·æœ¬çš„ç‰¹å¾
print("å¼€å§‹ç‰¹å¾æå–...")
start_time = time()

all_features = []
unique_samples = df_peaks['Sample File'].nunique() if not df_peaks.empty else 0
processed_count = 0

if df_peaks.empty:
    print("è­¦å‘Š: å¤„ç†åçš„å³°æ•°æ®ä¸ºç©ºï¼Œä½¿ç”¨åŸå§‹æ•°æ®è¿›è¡Œç‰¹å¾æå–")
    # å¦‚æœå³°å¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°ä½¿ç”¨åŸå§‹æ•°æ®
    for sample_file in df['Sample File'].unique():
        features = {'Sample File': sample_file}
        # æ·»åŠ é»˜è®¤ç‰¹å¾å€¼
        for feature_name in ['mac_profile', 'total_distinct_alleles', 'avg_alleles_per_locus']:
            features[feature_name] = 0
        all_features.append(features)
else:
    for sample_file, group in df_peaks.groupby('Sample File'):
        processed_count += 1
        if processed_count % 100 == 0 or processed_count == unique_samples:
            print(f"ç‰¹å¾æå–è¿›åº¦: {processed_count}/{unique_samples} ({processed_count/unique_samples*100:.1f}%)")
        
        features = extract_comprehensive_features_v5(sample_file, group)
        if features:  # ç¡®ä¿ç‰¹å¾ä¸ä¸ºç©º
            all_features.append(features)

if not all_features:
    print("é”™è¯¯: æ— æ³•æå–ç‰¹å¾ï¼Œè¯·æ£€æŸ¥æ•°æ®æ ¼å¼")
    exit()

df_features = pd.DataFrame(all_features)

# åˆå¹¶NoCæ ‡ç­¾
noc_map = df.groupby('Sample File')['NoC_True'].first().to_dict()
df_features['NoC_True'] = df_features['Sample File'].map(noc_map)
df_features = df_features.dropna(subset=['NoC_True'])

# éªŒè¯æ•°æ®è´¨é‡
if df_features.empty:
    print("é”™è¯¯: ç‰¹å¾æ•°æ®ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®å¤„ç†æµç¨‹")
    exit()

# å¡«å……ç¼ºå¤±å€¼
numeric_cols = df_features.select_dtypes(include=[np.number]).columns
df_features[numeric_cols] = df_features[numeric_cols].fillna(0)

print(f"ç‰¹å¾æå–å®Œæˆï¼Œè€—æ—¶: {time() - start_time:.2f}ç§’")
print(f"ç‰¹å¾æ•°æ®å½¢çŠ¶: {df_features.shape}")

# æ£€æŸ¥ç‰¹å¾æ•°é‡
feature_cols_check = [col for col in df_features.columns if col not in ['Sample File', 'NoC_True']]
print(f"å®é™…ç‰¹å¾æ•°é‡: {len(feature_cols_check)}")

if len(feature_cols_check) == 0:
    print("é”™è¯¯: æ²¡æœ‰å¯ç”¨çš„ç‰¹å¾ï¼Œè¯·æ£€æŸ¥ç‰¹å¾æå–å‡½æ•°")
    exit()

# =====================
# 6. ä¿®å¤åçš„ç‰¹å¾é€‰æ‹©
# =====================
print("\n=== æ­¥éª¤4: æ”¹è¿›çš„ç‰¹å¾é€‰æ‹©æ–¹æ³• ===")

# å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
feature_cols = [col for col in df_features.columns if col not in ['Sample File', 'NoC_True']]
X = df_features[feature_cols].fillna(0)
y = df_features['NoC_True']

print(f"åŸå§‹ç‰¹å¾æ•°: {len(feature_cols)}")
print(f"æ ·æœ¬æ•°: {len(X)}")
print(f"NoCåˆ†å¸ƒ: {y.value_counts().sort_index().to_dict()}")

# æ ‡ç­¾ç¼–ç 
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# ç‰¹å¾æ ‡å‡†åŒ–
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# è®¡ç®—æ ·æœ¬æƒé‡ï¼ˆç”¨äºåç»­æ¨¡å‹è®­ç»ƒï¼‰
sample_weights = compute_sample_weight('balanced', y_encoded)

# å¯¹4äººå’Œ5äººæ ·æœ¬é¢å¤–åŠ æƒ
enhanced_weights = sample_weights.copy()
for i, label in enumerate(y_encoded):
    if label_encoder.inverse_transform([label])[0] == 4:
        enhanced_weights[i] *= 2.5  # 4äººæ ·æœ¬æƒé‡Ã—2.5
    elif label_encoder.inverse_transform([label])[0] == 5:
        enhanced_weights[i] *= 4.0  # 5äººæ ·æœ¬æƒé‡Ã—4.0

print(f"æ ·æœ¬æƒé‡èŒƒå›´: {enhanced_weights.min():.3f} - {enhanced_weights.max():.3f}")

# ===== æ–¹æ³•1: åŸºäºå•å˜é‡ç»Ÿè®¡çš„ç‰¹å¾é€‰æ‹© =====
print("\nä½¿ç”¨åŸºäºå•å˜é‡ç»Ÿè®¡çš„ç‰¹å¾é€‰æ‹©...")

# ä½¿ç”¨f_classifè¿›è¡Œå•å˜é‡ç‰¹å¾é€‰æ‹©
selector_univariate = SelectKBest(score_func=f_classif, k=20)  # é€‰æ‹©å‰20ä¸ªç‰¹å¾
X_selected_univariate = selector_univariate.fit_transform(X_scaled, y_encoded)

selected_features_univariate = [feature_cols[i] for i in range(len(feature_cols)) 
                               if selector_univariate.get_support()[i]]

print(f"å•å˜é‡é€‰æ‹©çš„ç‰¹å¾æ•°: {len(selected_features_univariate)}")

# ===== æ–¹æ³•2: åŸºäºLassoCVçš„ç‰¹å¾é€‰æ‹© =====
print("\nä½¿ç”¨LassoCVè¿›è¡Œç‰¹å¾é€‰æ‹©...")

lasso_cv = LassoCV(cv=5, random_state=42, max_iter=2000)
lasso_cv.fit(X_scaled, y_encoded)

# é€‰æ‹©éé›¶ç³»æ•°çš„ç‰¹å¾
selector_lasso = SelectFromModel(lasso_cv, prefit=True)
X_selected_lasso = selector_lasso.transform(X_scaled)
selected_features_lasso = [feature_cols[i] for i in range(len(feature_cols)) 
                          if selector_lasso.get_support()[i]]

print(f"LassoCVé€‰æ‹©çš„ç‰¹å¾æ•°: {len(selected_features_lasso)}")

# ===== æ–¹æ³•3: åŸºäºæ¨¡å‹é‡è¦æ€§çš„ç‰¹å¾é€‰æ‹© =====
print("\nä½¿ç”¨åŸºäºæ¨¡å‹é‡è¦æ€§çš„ç‰¹å¾é€‰æ‹©...")

# åˆ›å»ºä¸€ä¸ªæ”¯æŒæ ·æœ¬æƒé‡çš„éšæœºæ£®æ—è¿›è¡Œç‰¹å¾é€‰æ‹©
rf_selector = RandomForestClassifier(
    n_estimators=100,
    max_depth=8,
    random_state=42,
    class_weight='balanced'
)

# è®­ç»ƒæ¨¡å‹å¹¶è·å–ç‰¹å¾é‡è¦æ€§
rf_selector.fit(X_scaled, y_encoded, sample_weight=enhanced_weights)

# åŸºäºé‡è¦æ€§é€‰æ‹©ç‰¹å¾
importance_threshold = np.percentile(rf_selector.feature_importances_, 70)  # é€‰æ‹©å‰30%çš„ç‰¹å¾
important_features_mask = rf_selector.feature_importances_ >= importance_threshold
selected_features_importance = [feature_cols[i] for i in range(len(feature_cols)) 
                               if important_features_mask[i]]

print(f"åŸºäºé‡è¦æ€§é€‰æ‹©çš„ç‰¹å¾æ•°: {len(selected_features_importance)}")

# ===== ç»¼åˆç‰¹å¾é€‰æ‹©ç­–ç•¥ =====
print("\nç»¼åˆç‰¹å¾é€‰æ‹©ç­–ç•¥...")

# å–ä¸‰ç§æ–¹æ³•çš„äº¤é›†ä½œä¸ºæœ€ç»ˆç‰¹å¾
selected_features_final = list(set(selected_features_univariate) & 
                              set(selected_features_lasso) & 
                              set(selected_features_importance))

# å¦‚æœäº¤é›†å¤ªå°‘ï¼Œä½¿ç”¨å¹¶é›†çš„å‰15ä¸ªæœ€é‡è¦ç‰¹å¾
if len(selected_features_final) < 10:
    print("äº¤é›†ç‰¹å¾æ•°é‡è¾ƒå°‘ï¼Œä½¿ç”¨é‡è¦æ€§æ’åºçš„å¹¶é›†ç­–ç•¥...")
    
    # åˆå¹¶æ‰€æœ‰é€‰æ‹©çš„ç‰¹å¾
    all_selected = list(set(selected_features_univariate + selected_features_lasso + selected_features_importance))
    
    # æŒ‰éšæœºæ£®æ—é‡è¦æ€§æ’åº
    feature_importance_dict = {feature_cols[i]: rf_selector.feature_importances_[i] 
                              for i in range(len(feature_cols))}
    
    all_selected_sorted = sorted(all_selected, key=lambda x: feature_importance_dict[x], reverse=True)
    selected_features_final = all_selected_sorted[:15]  # é€‰æ‹©å‰15ä¸ª

print(f"æœ€ç»ˆé€‰æ‹©çš„ç‰¹å¾æ•°: {len(selected_features_final)}")

# æ˜¾ç¤ºé€‰æ‹©çš„ç‰¹å¾
print("\næœ€ç»ˆé€‰æ‹©çš„ç‰¹å¾:")
for i, feature in enumerate(selected_features_final, 1):
    importance = feature_importance_dict.get(feature, 0)
    print(f"  {i:2d}. {feature:35} (é‡è¦æ€§: {importance:.4f})")

# ä½¿ç”¨æœ€ç»ˆé€‰æ‹©çš„ç‰¹å¾
feature_indices = [feature_cols.index(f) for f in selected_features_final]
X_final = X_scaled[:, feature_indices]
X_final = pd.DataFrame(X_final, columns=selected_features_final)

# =====================
# 7. è‡ªå®šä¹‰åˆ†å±‚åˆ’åˆ†å‡½æ•°å’Œæ¨¡å‹è®­ç»ƒ
# =====================
def custom_train_test_split(X, y, test_size=0.3, random_state=42):
    """è‡ªå®šä¹‰è®­ç»ƒæµ‹è¯•é›†åˆ’åˆ†ï¼Œç¡®ä¿å°‘æ•°ç±»æ ·æœ¬åˆ†å¸ƒ"""
    np.random.seed(random_state)
    unique_classes, class_counts = np.unique(y, return_counts=True)
    
    X_train_list, X_test_list = [], []
    y_train_list, y_test_list = [], []
    
    for cls, count in zip(unique_classes, class_counts):
        cls_indices = np.where(y == cls)[0]
        
        # å¯¹äºæ ·æœ¬æ•°æå°‘çš„ç±»åˆ«ï¼Œç‰¹æ®Šå¤„ç†
        if count <= 3:
            # æ ·æœ¬æ•°<=3çš„ç±»åˆ«ï¼Œè‡³å°‘ä¿ç•™1ä¸ªåœ¨æµ‹è¯•é›†ä¸­
            np.random.shuffle(cls_indices)
            test_size_cls = 1
            train_size_cls = count - 1
        else:
            # æ ·æœ¬æ•°>3çš„ç±»åˆ«ï¼ŒæŒ‰æ¯”ä¾‹åˆ†é…
            test_size_cls = max(1, int(count * test_size))
            train_size_cls = count - test_size_cls
        
        # éšæœºé€‰æ‹©
        np.random.shuffle(cls_indices)
        train_indices = cls_indices[:train_size_cls]
        test_indices = cls_indices[train_size_cls:train_size_cls + test_size_cls]
        
        X_train_list.append(X.iloc[train_indices])
        X_test_list.append(X.iloc[test_indices])
        y_train_list.extend(y[train_indices])
        y_test_list.extend(y[test_indices])
    
    X_train = pd.concat(X_train_list, ignore_index=True)
    X_test = pd.concat(X_test_list, ignore_index=True)
    y_train = np.array(y_train_list)
    y_test = np.array(y_test_list)
    
    return X_train, X_test, y_train, y_test

# =====================
# 8. æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°
# =====================
print("\n=== æ­¥éª¤5: æ¨¡å‹è®­ç»ƒä¸éªŒè¯ ===")

# ä½¿ç”¨è‡ªå®šä¹‰åˆ†å±‚åˆ’åˆ†
X_train, X_test, y_train_orig, y_test_orig = custom_train_test_split(X_final, y, test_size=0.3, random_state=42)

# è½¬æ¢ä¸ºç¼–ç æ ‡ç­¾
y_train = label_encoder.transform(y_train_orig)
y_test = label_encoder.transform(y_test_orig)

print(f"è®­ç»ƒé›†ç»´åº¦: {X_train.shape}, æµ‹è¯•é›†ç»´åº¦: {X_test.shape}")
print(f"è®­ç»ƒé›†NoCåˆ†å¸ƒ: {pd.Series(y_train_orig).value_counts().sort_index().to_dict()}")
print(f"æµ‹è¯•é›†NoCåˆ†å¸ƒ: {pd.Series(y_test_orig).value_counts().sort_index().to_dict()}")

# è®¡ç®—æ ·æœ¬æƒé‡
train_sample_weights = compute_sample_weight('balanced', y_train)

# å¯¹4äººå’Œ5äººæ ·æœ¬é¢å¤–åŠ æƒ
enhanced_train_weights = train_sample_weights.copy()
for i, label in enumerate(y_train):
    original_label = label_encoder.inverse_transform([label])[0]
    if original_label == 4:
        enhanced_train_weights[i] *= 3.0  # 4äººæ ·æœ¬æƒé‡Ã—3.0
    elif original_label == 5:
        enhanced_train_weights[i] *= 5.0  # 5äººæ ·æœ¬æƒé‡Ã—5.0

# äº¤å‰éªŒè¯è®¾ç½®
cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)  # å‡å°‘æŠ˜æ•°ä»¥é€‚åº”å°æ ·æœ¬

# ===== ä¸»è¦æ¨¡å‹ï¼šGradient Boosting =====
print("\nè®­ç»ƒGradient Boostingæ¨¡å‹...")

gb_model = GradientBoostingClassifier(
    n_estimators=200,
    max_depth=5,
    learning_rate=0.1,
    subsample=0.8,
    min_samples_split=2,
    min_samples_leaf=1,
    random_state=42
)

# è®­ç»ƒæ¨¡å‹
gb_model.fit(X_train, y_train, sample_weight=enhanced_train_weights)

# åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
y_pred_gb = gb_model.predict(X_test)
gb_accuracy = accuracy_score(y_test, y_pred_gb)
print(f"Gradient Boostingæµ‹è¯•å‡†ç¡®ç‡: {gb_accuracy:.4f}")

# ===== å¯¹æ¯”æ¨¡å‹ï¼šRandom Forest =====
print("\nè®­ç»ƒRandom Forestæ¨¡å‹...")

rf_model = RandomForestClassifier(
    n_estimators=200,
    max_depth=8,
    min_samples_split=2,
    min_samples_leaf=1,
    random_state=42,
    class_weight='balanced'
)

rf_model.fit(X_train, y_train, sample_weight=enhanced_train_weights)
y_pred_rf = rf_model.predict(X_test)
rf_accuracy = accuracy_score(y_test, y_pred_rf)
print(f"Random Forestæµ‹è¯•å‡†ç¡®ç‡: {rf_accuracy:.4f}")

# ===== å¯¹æ¯”æ¨¡å‹ï¼šDecision Tree =====
print("\nè®­ç»ƒDecision Treeæ¨¡å‹...")

dt_model = DecisionTreeClassifier(
    max_depth=6,
    min_samples_split=3,
    min_samples_leaf=2,
    random_state=42,
    class_weight='balanced'
)

dt_model.fit(X_train, y_train, sample_weight=enhanced_train_weights)
y_pred_dt = dt_model.predict(X_test)
dt_accuracy = accuracy_score(y_test, y_pred_dt)
print(f"Decision Treeæµ‹è¯•å‡†ç¡®ç‡: {dt_accuracy:.4f}")

# ===== é›†æˆæ¨¡å‹ =====
print("\næ„å»ºé›†æˆæ¨¡å‹...")

ensemble_model = VotingClassifier(
    estimators=[
        ('gb', gb_model),
        ('rf', rf_model),
        ('dt', dt_model)
    ],
    voting='soft',
    weights=[3, 2, 1]  # GBæƒé‡æœ€é«˜
)

ensemble_model.fit(X_train, y_train, sample_weight=enhanced_train_weights)
y_pred_ensemble = ensemble_model.predict(X_test)
ensemble_accuracy = accuracy_score(y_test, y_pred_ensemble)
print(f"é›†æˆæ¨¡å‹æµ‹è¯•å‡†ç¡®ç‡: {ensemble_accuracy:.4f}")

# é€‰æ‹©æœ€ä½³æ¨¡å‹
models = {
    'Gradient Boosting': (gb_model, gb_accuracy, y_pred_gb),
    'Random Forest': (rf_model, rf_accuracy, y_pred_rf),
    'Decision Tree': (dt_model, dt_accuracy, y_pred_dt),
    'Ensemble': (ensemble_model, ensemble_accuracy, y_pred_ensemble)
}

best_model_name = max(models.keys(), key=lambda x: models[x][1])
best_model, best_accuracy, best_predictions = models[best_model_name]

print(f"\næœ€ä½³æ¨¡å‹: {best_model_name} (å‡†ç¡®ç‡: {best_accuracy:.4f})")

# =====================
# 9. è¯¦ç»†è¯„ä¼°ä¸å¯è§†åŒ–
# =====================
print("\n=== æ­¥éª¤6: è¯¦ç»†è¯„ä¼°ä¸å¯è§†åŒ– ===")

# è½¬æ¢æ ‡ç­¾ç”¨äºæ˜¾ç¤º
y_test_display = label_encoder.inverse_transform(y_test)
best_predictions_display = label_encoder.inverse_transform(best_predictions)

# åˆ†ç±»æŠ¥å‘Š
class_names = [str(x) for x in sorted(label_encoder.classes_)]
print(f"\n{best_model_name} è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test_display, best_predictions_display, 
                          target_names=[f"{x}äºº" for x in class_names],
                          zero_division=0))

# æ··æ·†çŸ©é˜µå¯è§†åŒ–
plt.figure(figsize=(10, 8))
cm = confusion_matrix(y_test_display, best_predictions_display)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
           xticklabels=[f"{x}äºº" for x in class_names], 
           yticklabels=[f"{x}äºº" for x in class_names])
plt.title(f'{best_model_name} æ··æ·†çŸ©é˜µ')
plt.ylabel('çœŸå®NoC')
plt.xlabel('é¢„æµ‹NoC')
plt.tight_layout()
plt.savefig(os.path.join(PLOTS_DIR, 'confusion_matrix.png'), dpi=300)
plt.close()

# æ¨¡å‹æ€§èƒ½å¯¹æ¯”
plt.figure(figsize=(12, 6))
model_names = list(models.keys())
accuracies = [models[name][1] for name in model_names]

colors = ['#d62728' if name == best_model_name else '#1f77b4' for name in model_names]
bars = plt.bar(model_names, accuracies, color=colors)
plt.ylim(0, 1.1)
plt.ylabel('æµ‹è¯•å‡†ç¡®ç‡')
plt.title('æ¨¡å‹æ€§èƒ½å¯¹æ¯”')
plt.grid(axis='y', alpha=0.3)

for bar, acc in zip(bars, accuracies):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, 
             f'{acc:.3f}', ha='center', va='bottom')

plt.xticks(rotation=15)
plt.tight_layout()
plt.savefig(os.path.join(PLOTS_DIR, 'model_comparison.png'), dpi=300)
plt.close()

# ç‰¹å¾é‡è¦æ€§åˆ†æ
if hasattr(best_model, 'feature_importances_'):
    plt.figure(figsize=(12, 8))
    
    feature_importance = pd.DataFrame({
        'feature': selected_features_final,
        'importance': best_model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    # æ˜¾ç¤ºå‰15ä¸ªé‡è¦ç‰¹å¾
    top_features = feature_importance.head(15)
    sns.barplot(data=top_features, x='importance', y='feature')
    plt.title(f'{best_model_name} ç‰¹å¾é‡è¦æ€§ (å‰15)')
    plt.xlabel('ç‰¹å¾é‡è¦æ€§')
    plt.tight_layout()
    plt.savefig(os.path.join(PLOTS_DIR, 'feature_importance.png'), dpi=300)
    plt.close()
    
    print(f"\n{best_model_name} Top 10 é‡è¦ç‰¹å¾:")
    for idx, row in feature_importance.head(10).iterrows():
        print(f"  {row['feature']:35} {row['importance']:.4f}")

# =====================
# 10. åº”ç”¨æ¨¡å‹è¿›è¡Œå…¨æ ·æœ¬é¢„æµ‹
# =====================
print("\n=== æ­¥éª¤7: åº”ç”¨æ¨¡å‹è¿›è¡Œå…¨æ ·æœ¬é¢„æµ‹ ===")

# å¯¹æ‰€æœ‰æ ·æœ¬è¿›è¡Œé¢„æµ‹
y_pred_all_encoded = best_model.predict(X_final)
y_pred_all = label_encoder.inverse_transform(y_pred_all_encoded)

# æ·»åŠ é¢„æµ‹ç»“æœåˆ°ç‰¹å¾æ•°æ®æ¡†
df_features['Predicted_NoC'] = y_pred_all

# è®¡ç®—æ•´ä½“å‡†ç¡®ç‡
overall_accuracy = (df_features['Predicted_NoC'] == df_features['NoC_True']).mean()
print(f"æ•´ä½“é¢„æµ‹å‡†ç¡®ç‡: {overall_accuracy:.4f}")

# å„NoCç±»åˆ«çš„å‡†ç¡®ç‡
noc_accuracy = df_features.groupby('NoC_True').apply(
    lambda x: (x['Predicted_NoC'] == x['NoC_True']).mean()
).reset_index(name='Accuracy')

print("\nå„NoCç±»åˆ«é¢„æµ‹å‡†ç¡®ç‡:")
for _, row in noc_accuracy.iterrows():
    print(f"  {int(row['NoC_True'])}äºº: {row['Accuracy']:.4f}")

# NoCé¢„æµ‹å‡†ç¡®ç‡å¯è§†åŒ–
plt.figure(figsize=(10, 6))
sns.barplot(data=noc_accuracy, x='NoC_True', y='Accuracy')
plt.ylim(0, 1.1)
plt.xlabel('çœŸå®NoC')
plt.ylabel('é¢„æµ‹å‡†ç¡®ç‡')
plt.title('å„NoCç±»åˆ«é¢„æµ‹å‡†ç¡®ç‡')

for i, row in noc_accuracy.iterrows():
    plt.text(i, row['Accuracy'] + 0.03, f"{row['Accuracy']:.3f}", 
             ha='center', va='bottom')

plt.tight_layout()
plt.savefig(os.path.join(PLOTS_DIR, 'noc_accuracy_by_class.png'), dpi=300)
plt.close()

# =====================
# 11. SHAPå¯è§£é‡Šæ€§åˆ†æï¼ˆå¦‚æœå¯ç”¨ï¼‰
# =====================
if SHAP_AVAILABLE and hasattr(best_model, 'feature_importances_'):
    print("\n=== æ­¥éª¤8: SHAPå¯è§£é‡Šæ€§åˆ†æ ===")
    
    try:
        # åˆ›å»ºSHAPè§£é‡Šå™¨
        explainer = shap.TreeExplainer(best_model)
        
        # è®¡ç®—SHAPå€¼ï¼ˆä½¿ç”¨å°æ ·æœ¬ï¼‰
        shap_sample_size = min(10, len(X_test))
        X_shap = X_test.iloc[:shap_sample_size]
        shap_values = explainer.shap_values(X_shap)
        
        # å¯¹äºå¤šåˆ†ç±»ï¼Œå–ç¬¬ä¸€ä¸ªç±»åˆ«çš„SHAPå€¼æˆ–è®¡ç®—å¹³å‡
        if isinstance(shap_values, list):
            shap_values_mean = np.mean([np.abs(sv) for sv in shap_values], axis=0)
        else:
            shap_values_mean = np.abs(shap_values)
        
        # SHAPç‰¹å¾é‡è¦æ€§
        feature_shap_importance = np.mean(shap_values_mean, axis=0)
        shap_importance_df = pd.DataFrame({
            'feature': selected_features_final,
            'shap_importance': feature_shap_importance
        }).sort_values('shap_importance', ascending=False)
        
        plt.figure(figsize=(12, 8))
        top_shap_features = shap_importance_df.head(10)
        sns.barplot(data=top_shap_features, x='shap_importance', y='feature')
        plt.title('SHAPç‰¹å¾é‡è¦æ€§ (å‰10)')
        plt.xlabel('å¹³å‡SHAPé‡è¦æ€§')
        plt.tight_layout()
        plt.savefig(os.path.join(PLOTS_DIR, 'shap_importance.png'), dpi=300)
        plt.close()
        
        print("SHAP Top 10 é‡è¦ç‰¹å¾:")
        for idx, row in shap_importance_df.head(10).iterrows():
            print(f"  {row['feature']:35} {row['shap_importance']:.4f}")
            
    except Exception as e:
        print(f"SHAPåˆ†æå¤±è´¥: {e}")

# =====================
# 12. ä¿å­˜ç»“æœ
# =====================
print("\n=== æ­¥éª¤9: ä¿å­˜ç»“æœ ===")

# ä¿å­˜ç‰¹å¾æ•°æ®
df_features.to_csv(os.path.join(DATA_DIR, 'noc_features_with_predictions_fixed.csv'), 
                   index=False, encoding='utf-8-sig')

# ä¿å­˜æ¨¡å‹æ€§èƒ½æ‘˜è¦
summary = {
    'model_info': {
        'best_model': best_model_name,
        'best_accuracy': float(best_accuracy),
        'overall_accuracy': float(overall_accuracy),
        'feature_selection_method': 'Combined (Univariate + LassoCV + Importance)',
        'selected_features_count': len(selected_features_final)
    },
    'data_info': {
        'total_samples': len(df_features),
        'feature_count_original': len(feature_cols),
        'feature_count_selected': len(selected_features_final),
        'noc_distribution': df_features['NoC_True'].value_counts().sort_index().to_dict()
    },
    'model_performance': {
        name: {'accuracy': float(acc)} for name, (_, acc, _) in models.items()
    },
    'noc_class_accuracy': {
        int(row['NoC_True']): float(row['Accuracy']) 
        for _, row in noc_accuracy.iterrows()
    },
    'selected_features': selected_features_final,
    'timestamp': pd.Timestamp.now().strftime("%Y-%m-%d %H:%M:%S"),
    'version': 'V4.1_Fixed'
}

with open(os.path.join(DATA_DIR, 'noc_analysis_summary_fixed.json'), 'w', encoding='utf-8') as f:
    json.dump(summary, f, ensure_ascii=False, indent=2)

# ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
try:
    import joblib
    joblib.dump(best_model, os.path.join(DATA_DIR, f'best_noc_model_fixed.pkl'))
    joblib.dump(scaler, os.path.join(DATA_DIR, 'feature_scaler_fixed.pkl'))
    joblib.dump(label_encoder, os.path.join(DATA_DIR, 'label_encoder_fixed.pkl'))
    print("æ¨¡å‹å’Œé¢„å¤„ç†å™¨å·²ä¿å­˜")
except Exception as e:
    print(f"ä¿å­˜æ¨¡å‹æ—¶å‡ºé”™: {e}")

# =====================
# 13. æœ€ç»ˆæŠ¥å‘Š
# =====================
print("\n" + "="*60)
print("           æ³•åŒ»æ··åˆSTRå›¾è°±NoCè¯†åˆ« - æœ€ç»ˆæŠ¥å‘Š (ä¿®å¤ç‰ˆ)")
print("="*60)

print(f"\nğŸ“Š æ•°æ®æ¦‚å†µ:")
print(f"   â€¢ æ€»æ ·æœ¬æ•°: {len(df_features)}")
print(f"   â€¢ NoCåˆ†å¸ƒ: {dict(df_features['NoC_True'].value_counts().sort_index())}")
print(f"   â€¢ åŸå§‹ç‰¹å¾æ•°: {len(feature_cols)}")
print(f"   â€¢ é€‰æ‹©ç‰¹å¾æ•°: {len(selected_features_final)}")

print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model_name}")
print(f"   â€¢ æµ‹è¯•é›†å‡†ç¡®ç‡: {best_accuracy:.4f}")
print(f"   â€¢ æ•´ä½“å‡†ç¡®ç‡: {overall_accuracy:.4f}")

print(f"\nğŸ“ˆ å„ç±»åˆ«è¡¨ç°:")
for _, row in noc_accuracy.iterrows():
    noc = int(row['NoC_True'])
    acc = row['Accuracy']
    print(f"   â€¢ {noc}äººæ··åˆæ ·æœ¬: {acc:.4f}")

print(f"\nğŸ” Top 5 é‡è¦ç‰¹å¾:")
if hasattr(best_model, 'feature_importances_'):
    top_5_features = feature_importance.head(5)
    for i, (_, row) in enumerate(top_5_features.iterrows(), 1):
        print(f"   {i}. {row['feature']:30} ({row['importance']:.4f})")

print(f"\nğŸ’¾ ä¿å­˜çš„æ–‡ä»¶:")
print(f"   â€¢ ç‰¹å¾æ•°æ®: noc_features_with_predictions_fixed.csv")
print(f"   â€¢ åˆ†ææ‘˜è¦: noc_analysis_summary_fixed.json")
print(f"   â€¢ æœ€ä½³æ¨¡å‹: best_noc_model_fixed.pkl")
print(f"   â€¢ å›¾è¡¨ç›®å½•: {PLOTS_DIR}")

print(f"\nğŸ“‹ æŠ€æœ¯æ”¹è¿›:")
print(f"   â€¢ ä¿®å¤äº†RFECVçš„sample_weightå‚æ•°é—®é¢˜")
print(f"   â€¢ ä½¿ç”¨å¤šç§ç‰¹å¾é€‰æ‹©æ–¹æ³•çš„ç»„åˆç­–ç•¥")
print(f"   â€¢ å¯¹å°‘æ•°ç±»åˆ«ï¼ˆ4äººã€5äººï¼‰è¿›è¡Œé¢å¤–åŠ æƒ")
print(f"   â€¢ å®ç°è‡ªå®šä¹‰åˆ†å±‚åˆ’åˆ†ç¡®ä¿æµ‹è¯•é›†ä»£è¡¨æ€§")
print(f"   â€¢ é›†æˆå¤šä¸ªæ¨¡å‹æé«˜é¢„æµ‹ç¨³å®šæ€§")

print(f"\nğŸ“‹ æ¨¡å‹è§£é‡Š:")
print(f"   â€¢ æœ¬æ¨¡å‹åŸºäº{len(selected_features_final)}ä¸ªç²¾é€‰ç‰¹å¾")
print(f"   â€¢ ç‰¹å¾æ¶µç›–å›¾è°±ç»Ÿè®¡ã€å³°é«˜åˆ†å¸ƒã€å¹³è¡¡æ€§ã€ä¿¡æ¯ç†µã€é™è§£æŒ‡æ ‡ç­‰")
print(f"   â€¢ ä½¿ç”¨{best_model_name}ç®—æ³•å®ç°NoCè‡ªåŠ¨è¯†åˆ«")
print(f"   â€¢ æ•´ä½“å‡†ç¡®ç‡è¾¾åˆ°{overall_accuracy:.1%}ï¼Œå…·æœ‰è‰¯å¥½çš„å®ç”¨ä»·å€¼")

if SHAP_AVAILABLE:
    print(f"   â€¢ å·²ç”ŸæˆSHAPå¯è§£é‡Šæ€§åˆ†æï¼Œå¢å¼ºæ¨¡å‹é€æ˜åº¦")

print("\nâœ… åˆ†æå®Œæˆï¼(V4.1 ä¿®å¤ç‰ˆ)")
print("="*60)