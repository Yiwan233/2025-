# -*- coding: utf-8 -*-
"""
æ•°å­¦å»ºæ¨¡ - æ³•åŒ»DNAåˆ†æ - é—®é¢˜1ï¼šè´¡çŒ®è€…äººæ•°è¯†åˆ« (ä¸­æ–‡ä¼˜åŒ–ç‰ˆ)

ç‰ˆæœ¬: V5.0 - Chinese Features + RFECV + Optimized GradientBoosting
æ—¥æœŸ: 2025-06-03
æè¿°: ä¸­æ–‡ç‰¹å¾åç§° + RFECVç‰¹å¾é€‰æ‹© + æ·±åº¦è°ƒå‚çš„æ¢¯åº¦æå‡æœº
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import warnings
import json
import re
from scipy import stats
from scipy.signal import find_peaks
from collections import Counter
from time import time
import random

# æœºå™¨å­¦ä¹ ç›¸å…³
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV, RandomizedSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc, make_scorer
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.feature_selection import RFECV, SelectFromModel
from sklearn.utils import resample
from sklearn.utils.class_weight import compute_sample_weight

# å¯è§£é‡Šæ€§åˆ†æ
try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    print("SHAPä¸å¯ç”¨ï¼Œå°†è·³è¿‡å¯è§£é‡Šæ€§åˆ†æ")
    SHAP_AVAILABLE = False

# é…ç½®
plt.rcParams['font.sans-serif'] = ["Arial Unicode MS", "SimHei", "Microsoft YaHei"]
plt.rcParams['axes.unicode_minus'] = False
warnings.filterwarnings('ignore')

print("=== æ³•åŒ»æ··åˆSTRå›¾è°±NoCæ™ºèƒ½è¯†åˆ«ç³»ç»Ÿ (ä¸­æ–‡ä¼˜åŒ–ç‰ˆ) ===")
print("åŸºäºRFECVç‰¹å¾é€‰æ‹© + æ·±åº¦è°ƒå‚æ¢¯åº¦æå‡æœº")

# =====================
# 1. æ–‡ä»¶è·¯å¾„ä¸åŸºç¡€è®¾ç½®
# =====================
DATA_DIR = './'
file_path = os.path.join(DATA_DIR, 'é™„ä»¶1ï¼šä¸åŒäººæ•°çš„STRå›¾è°±æ•°æ®.csv')
PLOTS_DIR = os.path.join(DATA_DIR, 'noc_chinese_plots')
if not os.path.exists(PLOTS_DIR):
    os.makedirs(PLOTS_DIR)

# å…³é”®å‚æ•°è®¾ç½®
HEIGHT_THRESHOLD = 50
SATURATION_THRESHOLD = 30000
CTA_THRESHOLD = 0.5
PHR_IMBALANCE_THRESHOLD = 0.6

# =====================
# 2. ä¸­æ–‡ç‰¹å¾åç§°æ˜ å°„
# =====================
FEATURE_NAME_MAPPING = {
    # Aç±»ï¼šå›¾è°±å±‚é¢åŸºç¡€ç»Ÿè®¡ç‰¹å¾
    'mac_profile': 'æ ·æœ¬æœ€å¤§ç­‰ä½åŸºå› æ•°',
    'total_distinct_alleles': 'æ ·æœ¬æ€»ç‰¹å¼‚ç­‰ä½åŸºå› æ•°',
    'avg_alleles_per_locus': 'æ¯ä½ç‚¹å¹³å‡ç­‰ä½åŸºå› æ•°',
    'std_alleles_per_locus': 'æ¯ä½ç‚¹ç­‰ä½åŸºå› æ•°æ ‡å‡†å·®',
    'loci_gt2_alleles': 'ç­‰ä½åŸºå› æ•°å¤§äº2çš„ä½ç‚¹æ•°',
    'loci_gt3_alleles': 'ç­‰ä½åŸºå› æ•°å¤§äº3çš„ä½ç‚¹æ•°',
    'loci_gt4_alleles': 'ç­‰ä½åŸºå› æ•°å¤§äº4çš„ä½ç‚¹æ•°',
    'loci_gt5_alleles': 'ç­‰ä½åŸºå› æ•°å¤§äº5çš„ä½ç‚¹æ•°',
    'loci_gt6_alleles': 'ç­‰ä½åŸºå› æ•°å¤§äº6çš„ä½ç‚¹æ•°',
    'allele_count_dist_entropy': 'ç­‰ä½åŸºå› è®¡æ•°åˆ†å¸ƒç†µ',
    
    # Bç±»ï¼šå³°é«˜ã€å¹³è¡¡æ€§åŠéšæœºæ•ˆåº”ç‰¹å¾
    'avg_peak_height': 'å¹³å‡å³°é«˜',
    'std_peak_height': 'å³°é«˜æ ‡å‡†å·®',
    'min_peak_height': 'æœ€å°å³°é«˜',
    'max_peak_height': 'æœ€å¤§å³°é«˜',
    'avg_phr': 'å¹³å‡å³°é«˜æ¯”',
    'std_phr': 'å³°é«˜æ¯”æ ‡å‡†å·®',
    'min_phr': 'æœ€å°å³°é«˜æ¯”',
    'median_phr': 'å³°é«˜æ¯”ä¸­ä½æ•°',
    'num_loci_with_phr': 'å¯è®¡ç®—å³°é«˜æ¯”çš„ä½ç‚¹æ•°',
    'num_severe_imbalance_loci': 'ä¸¥é‡å¤±è¡¡ä½ç‚¹æ•°',
    'ratio_severe_imbalance_loci': 'ä¸¥é‡å¤±è¡¡ä½ç‚¹æ¯”ä¾‹',
    'skewness_peak_height': 'å³°é«˜åˆ†å¸ƒååº¦',
    'kurtosis_peak_height': 'å³°é«˜åˆ†å¸ƒå³­åº¦',
    'modality_peak_height': 'å³°é«˜åˆ†å¸ƒå¤šå³°æ€§',
    'num_saturated_peaks': 'é¥±å’Œå³°æ•°é‡',
    'ratio_saturated_peaks': 'é¥±å’Œå³°æ¯”ä¾‹',
    
    # Cç±»ï¼šä¿¡æ¯è®ºåŠå›¾è°±å¤æ‚åº¦ç‰¹å¾
    'inter_locus_balance_entropy': 'ä½ç‚¹é—´å¹³è¡¡ç†µ',
    'avg_locus_allele_entropy': 'å¹³å‡ä½ç‚¹ç­‰ä½åŸºå› ç†µ',
    'peak_height_entropy': 'å³°é«˜åˆ†å¸ƒç†µ',
    'num_loci_with_effective_alleles': 'æœ‰æ•ˆç­‰ä½åŸºå› ä½ç‚¹æ•°',
    'num_loci_no_effective_alleles': 'æ— æœ‰æ•ˆç­‰ä½åŸºå› ä½ç‚¹æ•°',
    
    # Dç±»ï¼šDNAé™è§£ä¸ä¿¡æ¯ä¸¢å¤±ç‰¹å¾
    'height_size_correlation': 'å³°é«˜ç‰‡æ®µå¤§å°ç›¸å…³æ€§',
    'height_size_slope': 'å³°é«˜ç‰‡æ®µå¤§å°å›å½’æ–œç‡',
    'weighted_height_size_slope': 'åŠ æƒå³°é«˜ç‰‡æ®µå¤§å°æ–œç‡',
    'phr_size_slope': 'å³°é«˜æ¯”ç‰‡æ®µå¤§å°æ–œç‡',
    'locus_dropout_score_weighted_by_size': 'ç‰‡æ®µå¤§å°åŠ æƒä½ç‚¹ä¸¢å¤±è¯„åˆ†',
    'degradation_index_rfu_per_bp': 'RFUæ¯ç¢±åŸºå¯¹é™è§£æŒ‡æ•°',
    'info_completeness_ratio_small_large': 'å°å¤§ç‰‡æ®µä¿¡æ¯å®Œæ•´åº¦æ¯”ç‡'
}

def get_chinese_name(feature_name):
    """è·å–ç‰¹å¾çš„ä¸­æ–‡åç§°"""
    return FEATURE_NAME_MAPPING.get(feature_name, feature_name)

# =====================
# 3. è¾…åŠ©å‡½æ•°å®šä¹‰
# =====================
def extract_noc_from_filename(filename):
    """ä»æ–‡ä»¶åæå–è´¡çŒ®è€…äººæ•°ï¼ˆNoCï¼‰"""
    match = re.search(r'-(\d+(?:_\d+)*)-[\d;]+-', str(filename))
    if match:
        ids = [id_val for id_val in match.group(1).split('_') if id_val.isdigit()]
        return len(ids) if len(ids) > 0 else np.nan
    return np.nan

def calculate_entropy(probabilities):
    """è®¡ç®—é¦™å†œç†µ"""
    probabilities = np.array(probabilities)
    probabilities = probabilities[probabilities > 0]
    if len(probabilities) == 0:
        return 0.0
    return -np.sum(probabilities * np.log(probabilities + 1e-10))

def calculate_ols_slope(x, y):
    """è®¡ç®—OLSå›å½’æ–œç‡"""
    if len(x) < 2 or len(x) != len(y):
        return 0.0
    
    x = np.array(x)
    y = np.array(y)
    
    if len(np.unique(x)) < 2:
        return 0.0
    
    try:
        slope, _, _, _, _ = stats.linregress(x, y)
        return slope
    except:
        return 0.0

# =====================
# 4. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
# =====================
print("\n=== æ­¥éª¤1: æ•°æ®åŠ è½½ä¸é¢„å¤„ç† ===")

# åŠ è½½æ•°æ®
try:
    df = pd.read_csv(file_path, encoding='utf-8')
    print(f"æˆåŠŸåŠ è½½æ•°æ®ï¼Œå½¢çŠ¶: {df.shape}")
except Exception as e:
    print(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
    exit()

# æå–NoCæ ‡ç­¾
df['NoC_True'] = df['Sample File'].apply(extract_noc_from_filename)
df = df.dropna(subset=['NoC_True'])
df['NoC_True'] = df['NoC_True'].astype(int)

print(f"åŸå§‹æ•°æ®NoCåˆ†å¸ƒ: {df['NoC_True'].value_counts().sort_index().to_dict()}")
print(f"åŸå§‹æ ·æœ¬æ•°: {df['Sample File'].nunique()}")

# =====================
# 5. å³°å¤„ç†ä¸CTAè¯„ä¼°
# =====================
print("\n=== æ­¥éª¤2: å³°å¤„ç†ä¸ä¿¡å·è¡¨å¾ ===")

def process_peaks_with_cta(sample_data):
    """å³°å¤„ç†ï¼ŒåŒ…å«CTAè¯„ä¼°"""
    processed_data = []
    
    for _, sample_row in sample_data.iterrows():
        sample_file = sample_row['Sample File']
        marker = sample_row['Marker']
        
        # æå–æ‰€æœ‰å³°
        peaks = []
        for i in range(1, 101):
            allele = sample_row.get(f'Allele {i}')
            size = sample_row.get(f'Size {i}')
            height = sample_row.get(f'Height {i}')
            
            if pd.notna(allele) and pd.notna(size) and pd.notna(height):
                original_height = float(height)
                corrected_height = min(original_height, SATURATION_THRESHOLD)
                
                if corrected_height >= HEIGHT_THRESHOLD:
                    peaks.append({
                        'allele': allele,
                        'size': float(size),
                        'height': corrected_height,
                        'original_height': original_height
                    })
        
        if not peaks:
            continue
            
        # CTAè¯„ä¼°
        peaks.sort(key=lambda x: x['height'], reverse=True)
        
        for peak in peaks:
            height_rank = peaks.index(peak) + 1
            total_peaks = len(peaks)
            
            if height_rank == 1:
                cta = 0.95
            elif height_rank == 2 and total_peaks >= 2:
                height_ratio = peak['height'] / peaks[0]['height']
                cta = 0.8 if height_ratio > 0.3 else 0.6
            else:
                height_ratio = peak['height'] / peaks[0]['height']
                cta = max(0.1, min(0.8, height_ratio))
            
            if cta >= CTA_THRESHOLD:
                processed_data.append({
                    'Sample File': sample_file,
                    'Marker': marker,
                    'Allele': peak['allele'],
                    'Size': peak['size'],
                    'Height': peak['height'],
                    'Original_Height': peak['original_height'],
                    'CTA': cta
                })
    
    return pd.DataFrame(processed_data)

# å¤„ç†æ‰€æœ‰æ ·æœ¬
all_processed_peaks = []
unique_samples = df['Sample File'].nunique()
processed_count = 0

print(f"å¼€å§‹å¤„ç† {unique_samples} ä¸ªæ ·æœ¬çš„å³°æ•°æ®...")

for sample_file, group in df.groupby('Sample File'):
    processed_count += 1
    if processed_count % 50 == 0 or processed_count == unique_samples:
        print(f"å¤„ç†è¿›åº¦: {processed_count}/{unique_samples} ({processed_count/unique_samples*100:.1f}%)")
    
    sample_peaks = process_peaks_with_cta(group)
    if not sample_peaks.empty:
        all_processed_peaks.append(sample_peaks)

df_peaks = pd.concat(all_processed_peaks, ignore_index=True) if all_processed_peaks else pd.DataFrame()
print(f"å¤„ç†åçš„å³°æ•°æ®å½¢çŠ¶: {df_peaks.shape}")

# =====================
# 6. ç»¼åˆç‰¹å¾å·¥ç¨‹ï¼ˆè¿”å›ä¸­æ–‡åç§°ï¼‰
# =====================
print("\n=== æ­¥éª¤3: ç»¼åˆç‰¹å¾å·¥ç¨‹ ===")

def extract_comprehensive_features_chinese(sample_file, sample_peaks):
    """æå–ç»¼åˆç‰¹å¾ï¼Œè¿”å›ä¸­æ–‡ç‰¹å¾å"""
    if sample_peaks.empty:
        return {}
    
    features = {'æ ·æœ¬æ–‡ä»¶': sample_file}
    
    # åŸºç¡€æ•°æ®å‡†å¤‡
    total_peaks = len(sample_peaks)
    all_heights = sample_peaks['Height'].values
    all_sizes = sample_peaks['Size'].values
    
    # æŒ‰ä½ç‚¹åˆ†ç»„ç»Ÿè®¡
    locus_groups = sample_peaks.groupby('Marker')
    alleles_per_locus = locus_groups['Allele'].nunique()
    locus_heights = locus_groups['Height'].sum()
    
    expected_autosomal_count = 23
    
    # Aç±»ï¼šå›¾è°±å±‚é¢åŸºç¡€ç»Ÿè®¡ç‰¹å¾
    features['æ ·æœ¬æœ€å¤§ç­‰ä½åŸºå› æ•°'] = alleles_per_locus.max() if len(alleles_per_locus) > 0 else 0
    features['æ ·æœ¬æ€»ç‰¹å¼‚ç­‰ä½åŸºå› æ•°'] = sample_peaks['Allele'].nunique()
    
    if expected_autosomal_count > 0:
        all_locus_counts = np.zeros(expected_autosomal_count)
        all_locus_counts[:len(alleles_per_locus)] = alleles_per_locus.values
        features['æ¯ä½ç‚¹å¹³å‡ç­‰ä½åŸºå› æ•°'] = np.mean(all_locus_counts)
        features['æ¯ä½ç‚¹ç­‰ä½åŸºå› æ•°æ ‡å‡†å·®'] = np.std(all_locus_counts)
    else:
        features['æ¯ä½ç‚¹å¹³å‡ç­‰ä½åŸºå› æ•°'] = alleles_per_locus.mean() if len(alleles_per_locus) > 0 else 0
        features['æ¯ä½ç‚¹ç­‰ä½åŸºå› æ•°æ ‡å‡†å·®'] = alleles_per_locus.std() if len(alleles_per_locus) > 1 else 0
    
    # MGTNç³»åˆ—
    for N in [2, 3, 4, 5, 6]:
        features[f'ç­‰ä½åŸºå› æ•°å¤§äº{N}çš„ä½ç‚¹æ•°'] = (alleles_per_locus >= N).sum()
    
    # ç­‰ä½åŸºå› è®¡æ•°çš„ç†µ
    if len(alleles_per_locus) > 0:
        counts = alleles_per_locus.value_counts(normalize=True)
        features['ç­‰ä½åŸºå› è®¡æ•°åˆ†å¸ƒç†µ'] = calculate_entropy(counts.values)
    else:
        features['ç­‰ä½åŸºå› è®¡æ•°åˆ†å¸ƒç†µ'] = 0
    
    # Bç±»ï¼šå³°é«˜ã€å¹³è¡¡æ€§åŠéšæœºæ•ˆåº”ç‰¹å¾
    if total_peaks > 0:
        features['å¹³å‡å³°é«˜'] = np.mean(all_heights)
        features['å³°é«˜æ ‡å‡†å·®'] = np.std(all_heights) if total_peaks > 1 else 0
        features['æœ€å°å³°é«˜'] = np.min(all_heights)
        features['æœ€å¤§å³°é«˜'] = np.max(all_heights)
        
        # å³°é«˜æ¯”(PHR)ç›¸å…³ç»Ÿè®¡
        phr_values = []
        for marker, marker_group in locus_groups:
            if len(marker_group) == 2:
                heights = marker_group['Height'].values
                phr = min(heights) / max(heights) if max(heights) > 0 else 0
                phr_values.append(phr)
        
        if phr_values:
            features['å¹³å‡å³°é«˜æ¯”'] = np.mean(phr_values)
            features['å³°é«˜æ¯”æ ‡å‡†å·®'] = np.std(phr_values) if len(phr_values) > 1 else 0
            features['æœ€å°å³°é«˜æ¯”'] = np.min(phr_values)
            features['å³°é«˜æ¯”ä¸­ä½æ•°'] = np.median(phr_values)
            features['å¯è®¡ç®—å³°é«˜æ¯”çš„ä½ç‚¹æ•°'] = len(phr_values)
            features['ä¸¥é‡å¤±è¡¡ä½ç‚¹æ•°'] = sum(phr <= PHR_IMBALANCE_THRESHOLD for phr in phr_values)
            features['ä¸¥é‡å¤±è¡¡ä½ç‚¹æ¯”ä¾‹'] = features['ä¸¥é‡å¤±è¡¡ä½ç‚¹æ•°'] / len(phr_values)
        else:
            for key in ['å¹³å‡å³°é«˜æ¯”', 'å³°é«˜æ¯”æ ‡å‡†å·®', 'æœ€å°å³°é«˜æ¯”', 'å³°é«˜æ¯”ä¸­ä½æ•°', 'å¯è®¡ç®—å³°é«˜æ¯”çš„ä½ç‚¹æ•°', 
                       'ä¸¥é‡å¤±è¡¡ä½ç‚¹æ•°', 'ä¸¥é‡å¤±è¡¡ä½ç‚¹æ¯”ä¾‹']:
                features[key] = 0
        
        # å³°é«˜åˆ†å¸ƒç»Ÿè®¡çŸ©
        if total_peaks > 2:
            features['å³°é«˜åˆ†å¸ƒååº¦'] = stats.skew(all_heights)
            features['å³°é«˜åˆ†å¸ƒå³­åº¦'] = stats.kurtosis(all_heights, fisher=False)
        else:
            features['å³°é«˜åˆ†å¸ƒååº¦'] = 0
            features['å³°é«˜åˆ†å¸ƒå³­åº¦'] = 0
        
        # å³°é«˜å¤šå³°æ€§
        try:
            log_heights = np.log(all_heights + 1)
            if len(np.unique(log_heights)) > 1:
                hist, _ = np.histogram(log_heights, bins=min(10, total_peaks))
                peaks_found, _ = find_peaks(hist)
                features['å³°é«˜åˆ†å¸ƒå¤šå³°æ€§'] = len(peaks_found)
            else:
                features['å³°é«˜åˆ†å¸ƒå¤šå³°æ€§'] = 1
        except:
            features['å³°é«˜åˆ†å¸ƒå¤šå³°æ€§'] = 1
        
        # é¥±å’Œæ•ˆåº”
        saturated_peaks = (sample_peaks['Original_Height'] >= SATURATION_THRESHOLD).sum()
        features['é¥±å’Œå³°æ•°é‡'] = saturated_peaks
        features['é¥±å’Œå³°æ¯”ä¾‹'] = saturated_peaks / total_peaks
    else:
        for key in ['å¹³å‡å³°é«˜', 'å³°é«˜æ ‡å‡†å·®', 'æœ€å°å³°é«˜', 'æœ€å¤§å³°é«˜',
                   'å¹³å‡å³°é«˜æ¯”', 'å³°é«˜æ¯”æ ‡å‡†å·®', 'æœ€å°å³°é«˜æ¯”', 'å³°é«˜æ¯”ä¸­ä½æ•°', 'å¯è®¡ç®—å³°é«˜æ¯”çš„ä½ç‚¹æ•°',
                   'ä¸¥é‡å¤±è¡¡ä½ç‚¹æ•°', 'ä¸¥é‡å¤±è¡¡ä½ç‚¹æ¯”ä¾‹',
                   'å³°é«˜åˆ†å¸ƒååº¦', 'å³°é«˜åˆ†å¸ƒå³­åº¦', 'å³°é«˜åˆ†å¸ƒå¤šå³°æ€§',
                   'é¥±å’Œå³°æ•°é‡', 'é¥±å’Œå³°æ¯”ä¾‹']:
            features[key] = 0
    
    # Cç±»ï¼šä¿¡æ¯è®ºåŠå›¾è°±å¤æ‚åº¦ç‰¹å¾
    if len(locus_heights) > 0:
        total_height = locus_heights.sum()
        if total_height > 0:
            locus_probs = locus_heights / total_height
            features['ä½ç‚¹é—´å¹³è¡¡ç†µ'] = calculate_entropy(locus_probs.values)
        else:
            features['ä½ç‚¹é—´å¹³è¡¡ç†µ'] = 0
    else:
        features['ä½ç‚¹é—´å¹³è¡¡ç†µ'] = 0
    
    # å¹³å‡ä½ç‚¹ç­‰ä½åŸºå› åˆ†å¸ƒç†µ
    locus_entropies = []
    for marker, marker_group in locus_groups:
        if len(marker_group) > 1:
            heights = marker_group['Height'].values
            height_sum = heights.sum()
            if height_sum > 0:
                probs = heights / height_sum
                entropy = calculate_entropy(probs)
                locus_entropies.append(entropy)
    
    features['å¹³å‡ä½ç‚¹ç­‰ä½åŸºå› ç†µ'] = np.mean(locus_entropies) if locus_entropies else 0
    
    # æ ·æœ¬æ•´ä½“å³°é«˜åˆ†å¸ƒç†µ
    if total_peaks > 0:
        log_heights = np.log(all_heights + 1)
        hist, _ = np.histogram(log_heights, bins=min(15, total_peaks))
        hist_probs = hist / hist.sum()
        hist_probs = hist_probs[hist_probs > 0]
        features['å³°é«˜åˆ†å¸ƒç†µ'] = calculate_entropy(hist_probs)
    else:
        features['å³°é«˜åˆ†å¸ƒç†µ'] = 0
    
    # å›¾è°±å®Œæ•´æ€§æŒ‡æ ‡
    effective_loci_count = len(locus_groups)
    features['æœ‰æ•ˆç­‰ä½åŸºå› ä½ç‚¹æ•°'] = effective_loci_count
    features['æ— æœ‰æ•ˆç­‰ä½åŸºå› ä½ç‚¹æ•°'] = max(0, expected_autosomal_count - effective_loci_count)
    
    # Dç±»ï¼šDNAé™è§£ä¸ä¿¡æ¯ä¸¢å¤±ç‰¹å¾
    if total_peaks > 1:
        if len(np.unique(all_heights)) > 1 and len(np.unique(all_sizes)) > 1:
            features['å³°é«˜ç‰‡æ®µå¤§å°ç›¸å…³æ€§'] = np.corrcoef(all_heights, all_sizes)[0, 1]
        else:
            features['å³°é«˜ç‰‡æ®µå¤§å°ç›¸å…³æ€§'] = 0
        
        features['å³°é«˜ç‰‡æ®µå¤§å°å›å½’æ–œç‡'] = calculate_ols_slope(all_sizes, all_heights)
        
        try:
            weights = all_heights / all_heights.sum()
            features['åŠ æƒå³°é«˜ç‰‡æ®µå¤§å°æ–œç‡'] = calculate_ols_slope(all_sizes, all_heights)
        except:
            features['åŠ æƒå³°é«˜ç‰‡æ®µå¤§å°æ–œç‡'] = 0
        
        if len(phr_values) > 1:
            phr_sizes = []
            for marker, marker_group in locus_groups:
                if len(marker_group) == 2:
                    avg_size = marker_group['Size'].mean()
                    phr_sizes.append(avg_size)
            
            if len(phr_sizes) == len(phr_values) and len(phr_sizes) > 1:
                features['å³°é«˜æ¯”ç‰‡æ®µå¤§å°æ–œç‡'] = calculate_ols_slope(phr_sizes, phr_values)
            else:
                features['å³°é«˜æ¯”ç‰‡æ®µå¤§å°æ–œç‡'] = 0
        else:
            features['å³°é«˜æ¯”ç‰‡æ®µå¤§å°æ–œç‡'] = 0
        
    else:
        for key in ['å³°é«˜ç‰‡æ®µå¤§å°ç›¸å…³æ€§', 'å³°é«˜ç‰‡æ®µå¤§å°å›å½’æ–œç‡', 'åŠ æƒå³°é«˜ç‰‡æ®µå¤§å°æ–œç‡', 'å³°é«˜æ¯”ç‰‡æ®µå¤§å°æ–œç‡']:
            features[key] = 0
    
    # å…¶ä»–é™è§£ç›¸å…³ç‰¹å¾
    dropout_score = features['æ— æœ‰æ•ˆç­‰ä½åŸºå› ä½ç‚¹æ•°'] / expected_autosomal_count if expected_autosomal_count > 0 else 0
    features['ç‰‡æ®µå¤§å°åŠ æƒä½ç‚¹ä¸¢å¤±è¯„åˆ†'] = dropout_score
    
    if len(locus_groups) > 1:
        locus_max_heights = []
        locus_avg_sizes = []
        for marker, marker_group in locus_groups:
            max_height = marker_group['Height'].max()
            avg_size = marker_group['Size'].mean()
            locus_max_heights.append(max_height)
            locus_avg_sizes.append(avg_size)
        
        features['RFUæ¯ç¢±åŸºå¯¹é™è§£æŒ‡æ•°'] = calculate_ols_slope(locus_avg_sizes, locus_max_heights)
    else:
        features['RFUæ¯ç¢±åŸºå¯¹é™è§£æŒ‡æ•°'] = 0
    
    # ç®€åŒ–çš„ä¿¡æ¯å®Œæ•´åº¦æ¯”ç‡
    small_fragment_loci = 0
    large_fragment_loci = 0
    small_fragment_effective = 0
    large_fragment_effective = 0
    
    for marker, marker_group in locus_groups:
        avg_size = marker_group['Size'].mean()
        if avg_size < 200:
            small_fragment_loci += 1
            small_fragment_effective += 1
        else:
            large_fragment_loci += 1
            large_fragment_effective += 1
    
    total_small_expected = expected_autosomal_count // 2
    total_large_expected = expected_autosomal_count - total_small_expected
    
    small_completeness = small_fragment_effective / total_small_expected if total_small_expected > 0 else 0
    large_completeness = large_fragment_effective / total_large_expected if total_large_expected > 0 else 0
    
    if large_completeness > 0:
        features['å°å¤§ç‰‡æ®µä¿¡æ¯å®Œæ•´åº¦æ¯”ç‡'] = small_completeness / large_completeness
    else:
        features['å°å¤§ç‰‡æ®µä¿¡æ¯å®Œæ•´åº¦æ¯”ç‡'] = small_completeness / 0.001
    
    return features

# æå–æ‰€æœ‰æ ·æœ¬çš„ç‰¹å¾
print("å¼€å§‹ç‰¹å¾æå–...")
start_time = time()

all_features = []
unique_samples = df_peaks['Sample File'].nunique() if not df_peaks.empty else 0
processed_count = 0

if df_peaks.empty:
    print("è­¦å‘Š: å¤„ç†åçš„å³°æ•°æ®ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤ç‰¹å¾")
    for sample_file in df['Sample File'].unique():
        features = {'æ ·æœ¬æ–‡ä»¶': sample_file, 'æ ·æœ¬æœ€å¤§ç­‰ä½åŸºå› æ•°': 0}
        all_features.append(features)
else:
    for sample_file, group in df_peaks.groupby('Sample File'):
        processed_count += 1
        if processed_count % 100 == 0 or processed_count == unique_samples:
            print(f"ç‰¹å¾æå–è¿›åº¦: {processed_count}/{unique_samples} ({processed_count/unique_samples*100:.1f}%)")
        
        features = extract_comprehensive_features_chinese(sample_file, group)
        if features:
            all_features.append(features)

df_features = pd.DataFrame(all_features)

# åˆå¹¶NoCæ ‡ç­¾
noc_map = df.groupby('Sample File')['NoC_True'].first().to_dict()
df_features['è´¡çŒ®è€…äººæ•°'] = df_features['æ ·æœ¬æ–‡ä»¶'].map(noc_map)
df_features = df_features.dropna(subset=['è´¡çŒ®è€…äººæ•°'])

# å¡«å……ç¼ºå¤±å€¼
numeric_cols = df_features.select_dtypes(include=[np.number]).columns
df_features[numeric_cols] = df_features[numeric_cols].fillna(0)

print(f"ç‰¹å¾æå–å®Œæˆï¼Œè€—æ—¶: {time() - start_time:.2f}ç§’")
print(f"ç‰¹å¾æ•°æ®å½¢çŠ¶: {df_features.shape}")

# =====================
# 7. RFECVç‰¹å¾é€‰æ‹©ï¼ˆä¿®å¤ç‰ˆï¼‰
# =====================
print("\n=== æ­¥éª¤4: RFECVç‰¹å¾é€‰æ‹© ===")

# å‡†å¤‡æ•°æ®
feature_cols = [col for col in df_features.columns if col not in ['æ ·æœ¬æ–‡ä»¶', 'è´¡çŒ®è€…äººæ•°']]
X = df_features[feature_cols].fillna(0)
y = df_features['è´¡çŒ®è€…äººæ•°']

print(f"åŸå§‹ç‰¹å¾æ•°: {len(feature_cols)}")
print(f"æ ·æœ¬æ•°: {len(X)}")
print(f"NoCåˆ†å¸ƒ: {y.value_counts().sort_index().to_dict()}")

# æ ‡ç­¾ç¼–ç 
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# ç‰¹å¾æ ‡å‡†åŒ–
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# åˆ›å»ºç”¨äºRFECVçš„åŸºç¡€ä¼°è®¡å™¨ï¼ˆä¸ä½¿ç”¨sample_weightï¼‰
base_estimator = GradientBoostingClassifier(
    n_estimators=50,  # å‡å°‘ä¼°è®¡å™¨æ•°é‡ä»¥åŠ å¿«RFECVé€Ÿåº¦
    max_depth=4,
    learning_rate=0.1,
    random_state=42
)

# ä½¿ç”¨RFECVè¿›è¡Œç‰¹å¾é€‰æ‹©ï¼ˆä¸ä¼ é€’sample_weightï¼‰
print("ä½¿ç”¨RFECVè¿›è¡Œç‰¹å¾é€‰æ‹©...")
rfecv = RFECV(
    estimator=base_estimator,
    step=1,
    cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),  # å‡å°‘CVæŠ˜æ•°
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

# æ‰§è¡ŒRFECVï¼ˆä¸ä¼ é€’sample_weightå‚æ•°ï¼‰
rfecv.fit(X_scaled, y_encoded)

# è·å–é€‰æ‹©çš„ç‰¹å¾
selected_features_mask = rfecv.support_
selected_features = [feature_cols[i] for i in range(len(feature_cols)) if selected_features_mask[i]]
X_selected = X_scaled[:, selected_features_mask]

print(f"RFECVé€‰æ‹©çš„ç‰¹å¾æ•°: {len(selected_features)}")
print(f"æœ€ä¼˜ç‰¹å¾æ•°: {rfecv.n_features_}")

# æ˜¾ç¤ºé€‰æ‹©çš„ç‰¹å¾
print("\nRFECVé€‰æ‹©çš„ç‰¹å¾:")
for i, feature in enumerate(selected_features, 1):
    print(f"  {i:2d}. {feature}")

# =====================
# 8. æ·±åº¦è°ƒå‚çš„æ¢¯åº¦æå‡æœº
# =====================
print("\n=== æ­¥éª¤5: æ·±åº¦è°ƒå‚æ¢¯åº¦æå‡æœº ===")

# è‡ªå®šä¹‰åˆ†å±‚åˆ’åˆ†
def custom_stratified_split(X, y, test_size=0.25, random_state=42):
    """ç¡®ä¿æ¯ä¸ªç±»åˆ«åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­éƒ½æœ‰ä»£è¡¨"""
    np.random.seed(random_state)
    unique_classes = np.unique(y)
    
    X_train_list, X_test_list = [], []
    y_train_list, y_test_list = [], []
    
    for cls in unique_classes:
        cls_indices = np.where(y == cls)[0]
        n_cls = len(cls_indices)
        
        if n_cls <= 2:
            # å¯¹äºæ ·æœ¬æ•°æå°‘çš„ç±»åˆ«ï¼Œè‡³å°‘ä¿ç•™1ä¸ªåœ¨æµ‹è¯•é›†
            n_test = 1
            n_train = n_cls - 1
        else:
            n_test = max(1, int(n_cls * test_size))
            n_train = n_cls - n_test
        
        # éšæœºé€‰æ‹©è®­ç»ƒé›†å’Œæµ‹è¯•é›†ç´¢å¼•
        np.random.shuffle(cls_indices)
        test_indices = cls_indices[:n_test]
        train_indices = cls_indices[n_test:n_test+n_train]
        
        X_train_list.append(X[train_indices])
        X_test_list.append(X[test_indices])
        y_train_list.append(y[train_indices])
        y_test_list.append(y[test_indices])
    
    X_train = np.vstack(X_train_list)
    X_test = np.vstack(X_test_list)
    y_train = np.hstack(y_train_list)
    y_test = np.hstack(y_test_list)
    
    return X_train, X_test, y_train, y_test

# ä½¿ç”¨è‡ªå®šä¹‰åˆ†å±‚åˆ’åˆ†
X_train, X_test, y_train, y_test = custom_stratified_split(X_selected, y_encoded, test_size=0.25)

print(f"è®­ç»ƒé›†å½¢çŠ¶: {X_train.shape}")
print(f"æµ‹è¯•é›†å½¢çŠ¶: {X_test.shape}")
print(f"è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ: {pd.Series(y_train).value_counts().sort_index().to_dict()}")
print(f"æµ‹è¯•é›†æ ‡ç­¾åˆ†å¸ƒ: {pd.Series(y_test).value_counts().sort_index().to_dict()}")

# è®¡ç®—ç±»åˆ«æƒé‡
class_weights = compute_sample_weight('balanced', y_train)

# è®¾ç½®äº¤å‰éªŒè¯
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# ç¬¬ä¸€é˜¶æ®µï¼šç²—ç½‘æ ¼æœç´¢
print("\nç¬¬ä¸€é˜¶æ®µï¼šç²—ç½‘æ ¼æœç´¢...")
coarse_param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [4, 6, 8],
    'learning_rate': [0.05, 0.1, 0.15],
    'subsample': [0.8, 0.9, 1.0],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2', None]
}

# è‡ªå®šä¹‰è¯„åˆ†å‡½æ•°ï¼ˆå¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼‰
def balanced_accuracy_scorer(estimator, X, y):
    """å¹³è¡¡å‡†ç¡®ç‡è¯„åˆ†å‡½æ•°"""
    y_pred = estimator.predict(X)
    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
    unique_classes = np.unique(y)
    class_accuracies = []
    
    for cls in unique_classes:
        cls_mask = (y == cls)
        if cls_mask.sum() > 0:
            cls_acc = (y_pred[cls_mask] == y[cls_mask]).mean()
            class_accuracies.append(cls_acc)
    
    return np.mean(class_accuracies)

balanced_scorer = make_scorer(balanced_accuracy_scorer)

# ç²—ç½‘æ ¼æœç´¢
coarse_gb = GradientBoostingClassifier(random_state=42, validation_fraction=0.1, n_iter_no_change=10)

coarse_grid_search = RandomizedSearchCV(
    coarse_gb,
    coarse_param_grid,
    n_iter=30,  # å‡å°‘æœç´¢æ¬¡æ•°ä»¥åŠ å¿«é€Ÿåº¦
    cv=3,  # å‡å°‘CVæŠ˜æ•°
    scoring=balanced_scorer,
    n_jobs=-1,
    random_state=42,
    verbose=1
)

# æ‹Ÿåˆæ—¶ä½¿ç”¨æ ·æœ¬æƒé‡
coarse_grid_search.fit(X_train, y_train, sample_weight=class_weights)

print(f"ç²—ç½‘æ ¼æœç´¢æœ€ä½³å‚æ•°: {coarse_grid_search.best_params_}")
print(f"ç²—ç½‘æ ¼æœç´¢æœ€ä½³åˆ†æ•°: {coarse_grid_search.best_score_:.4f}")

# ç¬¬äºŒé˜¶æ®µï¼šç²¾ç»†ç½‘æ ¼æœç´¢
print("\nç¬¬äºŒé˜¶æ®µï¼šç²¾ç»†ç½‘æ ¼æœç´¢...")
best_params = coarse_grid_search.best_params_

# åŸºäºç²—æœç´¢ç»“æœæ„å»ºç²¾ç»†æœç´¢ç©ºé—´
fine_param_grid = {
    'n_estimators': [best_params['n_estimators'] - 50, best_params['n_estimators'], best_params['n_estimators'] + 50],
    'max_depth': [max(3, best_params['max_depth'] - 1), best_params['max_depth'], best_params['max_depth'] + 1],
    'learning_rate': [max(0.01, best_params['learning_rate'] - 0.02), best_params['learning_rate'], best_params['learning_rate'] + 0.02],
    'subsample': [max(0.7, best_params['subsample'] - 0.1), best_params['subsample'], min(1.0, best_params['subsample'] + 0.1)],
    'min_samples_split': [max(2, best_params['min_samples_split'] - 1), best_params['min_samples_split'], best_params['min_samples_split'] + 1],
    'min_samples_leaf': [max(1, best_params['min_samples_leaf'] - 1), best_params['min_samples_leaf'], best_params['min_samples_leaf'] + 1],
    'max_features': [best_params['max_features']]
}

# ç²¾ç»†ç½‘æ ¼æœç´¢
fine_gb = GradientBoostingClassifier(random_state=42, validation_fraction=0.1, n_iter_no_change=15)

fine_grid_search = GridSearchCV(
    fine_gb,
    fine_param_grid,
    cv=cv,
    scoring=balanced_scorer,
    n_jobs=-1,
    verbose=1
)

fine_grid_search.fit(X_train, y_train, sample_weight=class_weights)

print(f"ç²¾ç»†ç½‘æ ¼æœç´¢æœ€ä½³å‚æ•°: {fine_grid_search.best_params_}")
print(f"ç²¾ç»†ç½‘æ ¼æœç´¢æœ€ä½³åˆ†æ•°: {fine_grid_search.best_score_:.4f}")

# æœ€ç»ˆæ¨¡å‹
best_gb_model = fine_grid_search.best_estimator_

# åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
y_pred_gb = best_gb_model.predict(X_test)
gb_accuracy = accuracy_score(y_test, y_pred_gb)

print(f"\næœ€ä¼˜æ¢¯åº¦æå‡æœºæµ‹è¯•é›†å‡†ç¡®ç‡: {gb_accuracy:.4f}")

# è®¡ç®—å¹³è¡¡å‡†ç¡®ç‡
balanced_acc = balanced_accuracy_scorer(best_gb_model, X_test, y_test)
print(f"å¹³è¡¡å‡†ç¡®ç‡: {balanced_acc:.4f}")

# =====================
# 9. ç»“æœåˆ†æä¸å¯è§†åŒ–
# =====================
print("\n=== æ­¥éª¤6: ç»“æœåˆ†æä¸å¯è§†åŒ– ===")

# è½¬æ¢æ ‡ç­¾ç”¨äºæ˜¾ç¤º
y_test_orig = label_encoder.inverse_transform(y_test)
y_pred_orig = label_encoder.inverse_transform(y_pred_gb)

# æ··æ·†çŸ©é˜µ
class_names = [f"{x}äºº" for x in sorted(label_encoder.classes_)]
print(f"\næ¢¯åº¦æå‡æœºè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test_orig, y_pred_orig, target_names=class_names))

# å¯è§†åŒ–æ··æ·†çŸ©é˜µ
plt.figure(figsize=(10, 8))
cm = confusion_matrix(y_test_orig, y_pred_orig)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
           xticklabels=class_names, yticklabels=class_names)
plt.title('æ¢¯åº¦æå‡æœºæ··æ·†çŸ©é˜µ')
plt.ylabel('çœŸå®NoC')
plt.xlabel('é¢„æµ‹NoC')
plt.tight_layout()
plt.savefig(os.path.join(PLOTS_DIR, 'æ··æ·†çŸ©é˜µ.png'), dpi=300, bbox_inches='tight')
plt.close()

# ç‰¹å¾é‡è¦æ€§åˆ†æ
plt.figure(figsize=(14, 10))
feature_importance = pd.DataFrame({
    'ç‰¹å¾': selected_features,
    'é‡è¦æ€§': best_gb_model.feature_importances_
}).sort_values('é‡è¦æ€§', ascending=False)

# æ˜¾ç¤ºå‰15ä¸ªé‡è¦ç‰¹å¾
top_features = feature_importance.head(15)
sns.barplot(data=top_features, x='é‡è¦æ€§', y='ç‰¹å¾')
plt.title('æ¢¯åº¦æå‡æœºç‰¹å¾é‡è¦æ€§æ’å (å‰15ä½)')
plt.xlabel('ç‰¹å¾é‡è¦æ€§')
plt.tight_layout()
plt.savefig(os.path.join(PLOTS_DIR, 'ç‰¹å¾é‡è¦æ€§.png'), dpi=300, bbox_inches='tight')
plt.close()

print(f"\næ¢¯åº¦æå‡æœº Top 10 é‡è¦ç‰¹å¾:")
for idx, row in feature_importance.head(10).iterrows():
    print(f"  {row['ç‰¹å¾']:35} {row['é‡è¦æ€§']:.4f}")

# å­¦ä¹ æ›²çº¿
from sklearn.model_selection import learning_curve

train_sizes, train_scores, val_scores = learning_curve(
    best_gb_model, X_selected, y_encoded, cv=cv, 
    train_sizes=np.linspace(0.1, 1.0, 10),
    scoring=balanced_scorer, random_state=42,
    n_jobs=-1
)

plt.figure(figsize=(12, 8))
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
val_mean = np.mean(val_scores, axis=1)
val_std = np.std(val_scores, axis=1)

plt.plot(train_sizes, train_mean, 'o-', color='red', label='è®­ç»ƒé›†')
plt.plot(train_sizes, val_mean, 'o-', color='green', label='éªŒè¯é›†')
plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='red')
plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='green')

plt.xlabel('è®­ç»ƒæ ·æœ¬æ•°')
plt.ylabel('å¹³è¡¡å‡†ç¡®ç‡')
plt.title('æ¢¯åº¦æå‡æœºå­¦ä¹ æ›²çº¿')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig(os.path.join(PLOTS_DIR, 'å­¦ä¹ æ›²çº¿.png'), dpi=300, bbox_inches='tight')
plt.close()

# RFECVç‰¹å¾é€‰æ‹©è¿‡ç¨‹å¯è§†åŒ–
plt.figure(figsize=(12, 8))
plt.plot(range(1, len(rfecv.cv_results_['mean_test_score']) + 1), 
         rfecv.cv_results_['mean_test_score'], 'o-')
plt.axvline(x=rfecv.n_features_, color='red', linestyle='--', 
           label=f'æœ€ä¼˜ç‰¹å¾æ•°: {rfecv.n_features_}')
plt.xlabel('ç‰¹å¾æ•°é‡')
plt.ylabel('äº¤å‰éªŒè¯å‡†ç¡®ç‡')
plt.title('RFECVç‰¹å¾é€‰æ‹©è¿‡ç¨‹')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig(os.path.join(PLOTS_DIR, 'RFECVç‰¹å¾é€‰æ‹©.png'), dpi=300, bbox_inches='tight')
plt.close()

# =====================
# 10. SHAPå¯è§£é‡Šæ€§åˆ†æ
# =====================
if SHAP_AVAILABLE:
    print("\n=== æ­¥éª¤7: SHAPå¯è§£é‡Šæ€§åˆ†æ ===")
    
    try:
        # åˆ›å»ºSHAPè§£é‡Šå™¨
        explainer = shap.TreeExplainer(best_gb_model)
        
        # è®¡ç®—SHAPå€¼ï¼ˆä½¿ç”¨å°æ ·æœ¬ï¼‰
        shap_sample_size = min(30, len(X_test))
        X_shap = X_test[:shap_sample_size]
        shap_values = explainer.shap_values(X_shap)
        
        # å¯¹äºå¤šåˆ†ç±»ï¼Œå–ç¬¬ä¸€ä¸ªç±»åˆ«çš„SHAPå€¼æˆ–è®¡ç®—å¹³å‡
        if isinstance(shap_values, list):
            shap_values_mean = np.mean([np.abs(sv) for sv in shap_values], axis=0)
        else:
            shap_values_mean = np.abs(shap_values)
        
        # SHAPç‰¹å¾é‡è¦æ€§
        feature_shap_importance = np.mean(shap_values_mean, axis=0)
        shap_importance_df = pd.DataFrame({
            'ç‰¹å¾': selected_features,
            'SHAPé‡è¦æ€§': feature_shap_importance
        }).sort_values('SHAPé‡è¦æ€§', ascending=False)
        
        plt.figure(figsize=(14, 10))
        top_shap_features = shap_importance_df.head(10)
        sns.barplot(data=top_shap_features, x='SHAPé‡è¦æ€§', y='ç‰¹å¾')
        plt.title('SHAPç‰¹å¾é‡è¦æ€§æ’å (å‰10ä½)')
        plt.xlabel('å¹³å‡SHAPé‡è¦æ€§')
        plt.tight_layout()
        plt.savefig(os.path.join(PLOTS_DIR, 'SHAPé‡è¦æ€§.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        print("SHAP Top 10 é‡è¦ç‰¹å¾:")
        for idx, row in shap_importance_df.head(10).iterrows():
            print(f"  {row['ç‰¹å¾']:35} {row['SHAPé‡è¦æ€§']:.4f}")
        
        # å°è¯•ç”ŸæˆSHAPæ‘˜è¦å›¾
        try:
            if isinstance(shap_values, list):
                shap_values_plot = shap_values[0]
            else:
                shap_values_plot = shap_values
                
            plt.figure(figsize=(12, 10))
            shap.summary_plot(shap_values_plot, X_shap, feature_names=selected_features, 
                            plot_type="bar", show=False)
            plt.title("SHAPç‰¹å¾å½±å“æ‘˜è¦")
            plt.tight_layout()
            plt.savefig(os.path.join(PLOTS_DIR, 'SHAPæ‘˜è¦å›¾.png'), dpi=300, bbox_inches='tight')
            plt.close()
            
        except Exception as e:
            print(f"ç”ŸæˆSHAPæ‘˜è¦å›¾å¤±è´¥: {e}")
            
    except Exception as e:
        print(f"SHAPåˆ†æå¤±è´¥: {e}")

# =====================
# 11. æ¨¡å‹é¢„æµ‹ä¸ä¿å­˜
# =====================
print("\n=== æ­¥éª¤8: æ¨¡å‹é¢„æµ‹ä¸ä¿å­˜ ===")

# å¯¹æ‰€æœ‰æ ·æœ¬è¿›è¡Œé¢„æµ‹
y_pred_all = best_gb_model.predict(X_selected)
y_pred_all_orig = label_encoder.inverse_transform(y_pred_all)

# æ·»åŠ é¢„æµ‹ç»“æœåˆ°ç‰¹å¾æ•°æ®æ¡†
df_features['é¢„æµ‹è´¡çŒ®è€…äººæ•°'] = y_pred_all_orig

# è®¡ç®—æ•´ä½“å‡†ç¡®ç‡
overall_accuracy = (df_features['é¢„æµ‹è´¡çŒ®è€…äººæ•°'] == df_features['è´¡çŒ®è€…äººæ•°']).mean()
print(f"æ•´ä½“é¢„æµ‹å‡†ç¡®ç‡: {overall_accuracy:.4f}")

# å„NoCç±»åˆ«çš„å‡†ç¡®ç‡
noc_accuracy = df_features.groupby('è´¡çŒ®è€…äººæ•°').apply(
    lambda x: (x['é¢„æµ‹è´¡çŒ®è€…äººæ•°'] == x['è´¡çŒ®è€…äººæ•°']).mean()
).reset_index(name='å‡†ç¡®ç‡')

print("\nå„NoCç±»åˆ«é¢„æµ‹å‡†ç¡®ç‡:")
for _, row in noc_accuracy.iterrows():
    print(f"  {int(row['è´¡çŒ®è€…äººæ•°'])}äºº: {row['å‡†ç¡®ç‡']:.4f}")

# å¯è§†åŒ–å„ç±»åˆ«å‡†ç¡®ç‡
plt.figure(figsize=(10, 6))
sns.barplot(data=noc_accuracy, x='è´¡çŒ®è€…äººæ•°', y='å‡†ç¡®ç‡')
plt.ylim(0, 1.1)
plt.xlabel('çœŸå®NoC')
plt.ylabel('é¢„æµ‹å‡†ç¡®ç‡')
plt.title('å„NoCç±»åˆ«é¢„æµ‹å‡†ç¡®ç‡')

for i, row in noc_accuracy.iterrows():
    plt.text(i, row['å‡†ç¡®ç‡'] + 0.03, f"{row['å‡†ç¡®ç‡']:.3f}", 
             ha='center', va='bottom')

plt.tight_layout()
plt.savefig(os.path.join(PLOTS_DIR, 'å„ç±»åˆ«å‡†ç¡®ç‡.png'), dpi=300, bbox_inches='tight')
plt.close()

# ä¿å­˜ç»“æœ
df_features.to_csv(os.path.join(DATA_DIR, 'NoCè¯†åˆ«ç»“æœ_ä¸­æ–‡ç‰ˆ.csv'), 
                   index=False, encoding='utf-8-sig')

# ä¿å­˜æ¨¡å‹æ€§èƒ½æ‘˜è¦
summary = {
    'æ¨¡å‹ä¿¡æ¯': {
        'æœ€ä½³æ¨¡å‹': 'GradientBoostingClassifier',
        'æµ‹è¯•é›†å‡†ç¡®ç‡': float(gb_accuracy),
        'å¹³è¡¡å‡†ç¡®ç‡': float(balanced_acc),
        'æ•´ä½“å‡†ç¡®ç‡': float(overall_accuracy),
        'ç‰¹å¾é€‰æ‹©æ–¹æ³•': 'RFECV',
        'é€‰æ‹©ç‰¹å¾æ•°': len(selected_features)
    },
    'æ•°æ®ä¿¡æ¯': {
        'æ€»æ ·æœ¬æ•°': len(df_features),
        'åŸå§‹ç‰¹å¾æ•°': len(feature_cols),
        'é€‰æ‹©ç‰¹å¾æ•°': len(selected_features),
        'NoCåˆ†å¸ƒ': df_features['è´¡çŒ®è€…äººæ•°'].value_counts().sort_index().to_dict()
    },
    'æœ€ä½³å‚æ•°': fine_grid_search.best_params_,
    'å„ç±»åˆ«å‡†ç¡®ç‡': {
        int(row['è´¡çŒ®è€…äººæ•°']): float(row['å‡†ç¡®ç‡']) 
        for _, row in noc_accuracy.iterrows()
    },
    'é€‰æ‹©çš„ç‰¹å¾': selected_features,
    'æ—¶é—´æˆ³': pd.Timestamp.now().strftime("%Y-%m-%d %H:%M:%S")
}

with open(os.path.join(DATA_DIR, 'NoCåˆ†ææ‘˜è¦_ä¸­æ–‡ç‰ˆ.json'), 'w', encoding='utf-8') as f:
    json.dump(summary, f, ensure_ascii=False, indent=2)

# =====================
# 12. æœ€ç»ˆæŠ¥å‘Š
# =====================
print("\n" + "="*80)
print("              æ³•åŒ»æ··åˆSTRå›¾è°±NoCè¯†åˆ« - ä¸­æ–‡ä¼˜åŒ–ç‰ˆæœ€ç»ˆæŠ¥å‘Š")
print("="*80)

print(f"\nğŸ“Š æ•°æ®æ¦‚å†µ:")
print(f"   â€¢ æ€»æ ·æœ¬æ•°: {len(df_features)}")
print(f"   â€¢ NoCåˆ†å¸ƒ: {dict(df_features['è´¡çŒ®è€…äººæ•°'].value_counts().sort_index())}")
print(f"   â€¢ åŸå§‹ç‰¹å¾æ•°: {len(feature_cols)}")
print(f"   â€¢ RFECVé€‰æ‹©ç‰¹å¾æ•°: {len(selected_features)}")

print(f"\nğŸ† æœ€ä½³æ¨¡å‹: æ·±åº¦è°ƒå‚æ¢¯åº¦æå‡æœº")
print(f"   â€¢ æµ‹è¯•é›†å‡†ç¡®ç‡: {gb_accuracy:.4f}")
print(f"   â€¢ å¹³è¡¡å‡†ç¡®ç‡: {balanced_acc:.4f}")
print(f"   â€¢ æ•´ä½“å‡†ç¡®ç‡: {overall_accuracy:.4f}")

print(f"\nâš™ï¸ æœ€ä¼˜è¶…å‚æ•°:")
for param, value in fine_grid_search.best_params_.items():
    print(f"   â€¢ {param}: {value}")

print(f"\nğŸ“ˆ å„ç±»åˆ«è¡¨ç°:")
for _, row in noc_accuracy.iterrows():
    noc = int(row['è´¡çŒ®è€…äººæ•°'])
    acc = row['å‡†ç¡®ç‡']
    icon = "ğŸŸ¢" if acc > 0.8 else "ğŸŸ¡" if acc > 0.6 else "ğŸ”´"
    print(f"   {icon} {noc}äººæ··åˆæ ·æœ¬: {acc:.4f}")

print(f"\nğŸ” Top 5 é‡è¦ç‰¹å¾:")
for i, (_, row) in enumerate(feature_importance.head(5).iterrows(), 1):
    print(f"   {i}. {row['ç‰¹å¾']:30} ({row['é‡è¦æ€§']:.4f})")

print(f"\nğŸ’¾ ä¿å­˜çš„æ–‡ä»¶:")
print(f"   â€¢ è¯†åˆ«ç»“æœ: NoCè¯†åˆ«ç»“æœ_ä¸­æ–‡ç‰ˆ.csv")
print(f"   â€¢ åˆ†ææ‘˜è¦: NoCåˆ†ææ‘˜è¦_ä¸­æ–‡ç‰ˆ.json")
print(f"   â€¢ å›¾è¡¨ç›®å½•: {PLOTS_DIR}")

print(f"\nğŸ“‹ æŠ€æœ¯ç‰¹è‰²:")
print(f"   â€¢ å…¨ä¸­æ–‡ç‰¹å¾å‘½åï¼Œä¾¿äºç†è§£å’Œåº”ç”¨")
print(f"   â€¢ RFECVè‡ªåŠ¨ç‰¹å¾é€‰æ‹©ï¼Œé¿å…è¿‡æ‹Ÿåˆ")
print(f"   â€¢ äºŒé˜¶æ®µç½‘æ ¼æœç´¢ï¼Œæ·±åº¦è°ƒå‚æ¢¯åº¦æå‡æœº")
print(f"   â€¢ å¹³è¡¡å‡†ç¡®ç‡è¯„ä¼°ï¼Œå¤„ç†ç±»åˆ«ä¸å¹³è¡¡")
print(f"   â€¢ SHAPå¯è§£é‡Šæ€§åˆ†æï¼Œå¢å¼ºæ¨¡å‹é€æ˜åº¦")

print(f"\nâœ… ä¸­æ–‡ä¼˜åŒ–ç‰ˆåˆ†æå®Œæˆï¼")
print("="*80)