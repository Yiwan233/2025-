# -*- coding: utf-8 -*-
"""
æ•°å­¦å»ºæ¨¡ - æ³•åŒ»DNAåˆ†æ - é—®é¢˜1ï¼šè´¡çŒ®è€…äººæ•°è¯†åˆ« (RFECV + æ¢¯åº¦æå‡æœºæ·±åº¦ä¼˜åŒ–ç‰ˆ)

ç‰ˆæœ¬: V7.0 - RFECV Feature Selection + Deep Gradient Boosting Optimization
æ—¥æœŸ: 2025-06-03
æè¿°: ä¿®å¤RFECVè¯„åˆ†å™¨é”™è¯¯ + æ¢¯åº¦æå‡æœºæ·±åº¦ä¼˜åŒ– + é›†æˆå­¦ä¹ å¢å¼º
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import warnings
import json
import re
from scipy import stats
from scipy.signal import find_peaks
from collections import Counter
from time import time
import random

# æœºå™¨å­¦ä¹ ç›¸å…³
from sklearn.model_selection import (train_test_split, cross_val_score, StratifiedKFold, 
                                   GridSearchCV, RandomizedSearchCV, validation_curve,
                                   cross_validate, RepeatedStratifiedKFold)
from sklearn.metrics import (accuracy_score, confusion_matrix, classification_report, 
                           roc_curve, auc, make_scorer, f1_score, balanced_accuracy_score,
                           precision_recall_curve, average_precision_score)
from sklearn.ensemble import GradientBoostingClassifier, VotingClassifier, RandomForestClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.feature_selection import RFECV, SelectFromModel
from sklearn.utils import resample
from sklearn.utils.class_weight import compute_sample_weight
from sklearn.calibration import CalibratedClassifierCV

# å¯è§£é‡Šæ€§åˆ†æ
try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    print("SHAPä¸å¯ç”¨ï¼Œå°†è·³è¿‡å¯è§£é‡Šæ€§åˆ†æ")
    SHAP_AVAILABLE = False

# é…ç½®
plt.rcParams['font.sans-serif'] = ["Arial Unicode MS", "SimHei", "Microsoft YaHei"]
plt.rcParams['axes.unicode_minus'] = False
warnings.filterwarnings('ignore')

print("=== æ³•åŒ»æ··åˆSTRå›¾è°±NoCæ™ºèƒ½è¯†åˆ«ç³»ç»Ÿ V7.0 ===")
print("ä¿®å¤RFECVé”™è¯¯ + æ¢¯åº¦æå‡æœºæ·±åº¦ä¼˜åŒ–")

# =====================
# 1. æ–‡ä»¶è·¯å¾„ä¸åŸºç¡€è®¾ç½®
# =====================
DATA_DIR = './'
file_path = os.path.join(DATA_DIR, 'é™„ä»¶1ï¼šä¸åŒäººæ•°çš„STRå›¾è°±æ•°æ®.csv')
PLOTS_DIR = os.path.join(DATA_DIR, 'noc_gb_deep_optimization')
if not os.path.exists(PLOTS_DIR):
    os.makedirs(PLOTS_DIR)

# å…³é”®å‚æ•°è®¾ç½®
HEIGHT_THRESHOLD = 50
SATURATION_THRESHOLD = 30000
CTA_THRESHOLD = 0.5
PHR_IMBALANCE_THRESHOLD = 0.6

# =====================
# 2. ä¸­æ–‡ç‰¹å¾åç§°æ˜ å°„
# =====================
FEATURE_NAME_MAPPING = {
    # Aç±»ï¼šå›¾è°±å±‚é¢åŸºç¡€ç»Ÿè®¡ç‰¹å¾
    'mac_profile': 'æ ·æœ¬æœ€å¤§ç­‰ä½åŸºå› æ•°',
    'total_distinct_alleles': 'æ ·æœ¬æ€»ç‰¹å¼‚ç­‰ä½åŸºå› æ•°',
    'avg_alleles_per_locus': 'æ¯ä½ç‚¹å¹³å‡ç­‰ä½åŸºå› æ•°',
    'std_alleles_per_locus': 'æ¯ä½ç‚¹ç­‰ä½åŸºå› æ•°æ ‡å‡†å·®',
    'loci_gt2_alleles': 'ç­‰ä½åŸºå› æ•°å¤§äº2çš„ä½ç‚¹æ•°',
    'loci_gt3_alleles': 'ç­‰ä½åŸºå› æ•°å¤§äº3çš„ä½ç‚¹æ•°',
    'loci_gt4_alleles': 'ç­‰ä½åŸºå› æ•°å¤§äº4çš„ä½ç‚¹æ•°',
    'loci_gt5_alleles': 'ç­‰ä½åŸºå› æ•°å¤§äº5çš„ä½ç‚¹æ•°',
    'loci_gt6_alleles': 'ç­‰ä½åŸºå› æ•°å¤§äº6çš„ä½ç‚¹æ•°',
    'allele_count_dist_entropy': 'ç­‰ä½åŸºå› è®¡æ•°åˆ†å¸ƒç†µ',
    
    # Bç±»ï¼šå³°é«˜ã€å¹³è¡¡æ€§åŠéšæœºæ•ˆåº”ç‰¹å¾
    'avg_peak_height': 'å¹³å‡å³°é«˜',
    'std_peak_height': 'å³°é«˜æ ‡å‡†å·®',
    'min_peak_height': 'æœ€å°å³°é«˜',
    'max_peak_height': 'æœ€å¤§å³°é«˜',
    'peak_height_cv': 'å³°é«˜å˜å¼‚ç³»æ•°',
    'avg_phr': 'å¹³å‡å³°é«˜æ¯”',
    'std_phr': 'å³°é«˜æ¯”æ ‡å‡†å·®',
    'min_phr': 'æœ€å°å³°é«˜æ¯”',
    'median_phr': 'å³°é«˜æ¯”ä¸­ä½æ•°',
    'num_loci_with_phr': 'å¯è®¡ç®—å³°é«˜æ¯”çš„ä½ç‚¹æ•°',
    'num_severe_imbalance_loci': 'ä¸¥é‡å¤±è¡¡ä½ç‚¹æ•°',
    'ratio_severe_imbalance_loci': 'ä¸¥é‡å¤±è¡¡ä½ç‚¹æ¯”ä¾‹',
    'skewness_peak_height': 'å³°é«˜åˆ†å¸ƒååº¦',
    'kurtosis_peak_height': 'å³°é«˜åˆ†å¸ƒå³­åº¦',
    'modality_peak_height': 'å³°é«˜åˆ†å¸ƒå¤šå³°æ€§',
    'num_saturated_peaks': 'é¥±å’Œå³°æ•°é‡',
    'ratio_saturated_peaks': 'é¥±å’Œå³°æ¯”ä¾‹',
    
    # Cç±»ï¼šä¿¡æ¯è®ºåŠå›¾è°±å¤æ‚åº¦ç‰¹å¾
    'inter_locus_balance_entropy': 'ä½ç‚¹é—´å¹³è¡¡ç†µ',
    'avg_locus_allele_entropy': 'å¹³å‡ä½ç‚¹ç­‰ä½åŸºå› ç†µ',
    'peak_height_entropy': 'å³°é«˜åˆ†å¸ƒç†µ',
    'num_loci_with_effective_alleles': 'æœ‰æ•ˆç­‰ä½åŸºå› ä½ç‚¹æ•°',
    'num_loci_no_effective_alleles': 'æ— æœ‰æ•ˆç­‰ä½åŸºå› ä½ç‚¹æ•°',
    
    # Dç±»ï¼šDNAé™è§£ä¸ä¿¡æ¯ä¸¢å¤±ç‰¹å¾
    'height_size_correlation': 'å³°é«˜ç‰‡æ®µå¤§å°ç›¸å…³æ€§',
    'height_size_slope': 'å³°é«˜ç‰‡æ®µå¤§å°å›å½’æ–œç‡',
    'weighted_height_size_slope': 'åŠ æƒå³°é«˜ç‰‡æ®µå¤§å°æ–œç‡',
    'phr_size_slope': 'å³°é«˜æ¯”ç‰‡æ®µå¤§å°æ–œç‡',
    'locus_dropout_score_weighted_by_size': 'ç‰‡æ®µå¤§å°åŠ æƒä½ç‚¹ä¸¢å¤±è¯„åˆ†',
    'degradation_index_rfu_per_bp': 'RFUæ¯ç¢±åŸºå¯¹é™è§£æŒ‡æ•°',
    'info_completeness_ratio_small_large': 'å°å¤§ç‰‡æ®µä¿¡æ¯å®Œæ•´åº¦æ¯”ç‡',
    
    # æ–°å¢é«˜çº§ç‰¹å¾
    'peak_height_quantile_ratio': 'å³°é«˜åˆ†ä½æ•°æ¯”ç‡',
    'allele_diversity_index': 'ç­‰ä½åŸºå› å¤šæ ·æ€§æŒ‡æ•°',
    'peak_pattern_complexity': 'å³°æ¨¡å¼å¤æ‚åº¦',
    'heterozygosity_rate': 'æ‚åˆç‡',
    'peak_clustering_coefficient': 'å³°èšç±»ç³»æ•°'
}

def get_chinese_name(feature_name):
    """è·å–ç‰¹å¾çš„ä¸­æ–‡åç§°"""
    return FEATURE_NAME_MAPPING.get(feature_name, feature_name)

# =====================
# 3. è¾…åŠ©å‡½æ•°å®šä¹‰
# =====================
def extract_noc_from_filename(filename):
    """ä»æ–‡ä»¶åæå–è´¡çŒ®è€…äººæ•°ï¼ˆNoCï¼‰"""
    match = re.search(r'-(\d+(?:_\d+)*)-[\d;]+-', str(filename))
    if match:
        ids = [id_val for id_val in match.group(1).split('_') if id_val.isdigit()]
        return len(ids) if len(ids) > 0 else np.nan
    return np.nan

def calculate_entropy(probabilities):
    """è®¡ç®—é¦™å†œç†µ"""
    probabilities = np.array(probabilities)
    probabilities = probabilities[probabilities > 0]
    if len(probabilities) == 0:
        return 0.0
    return -np.sum(probabilities * np.log(probabilities + 1e-10))

def calculate_ols_slope(x, y):
    """è®¡ç®—OLSå›å½’æ–œç‡"""
    if len(x) < 2 or len(x) != len(y):
        return 0.0
    
    x = np.array(x)
    y = np.array(y)
    
    if len(np.unique(x)) < 2:
        return 0.0
    
    try:
        slope, _, _, _, _ = stats.linregress(x, y)
        return slope
    except:
        return 0.0

# =====================
# 4. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
# =====================
print("\n=== æ­¥éª¤1: æ•°æ®åŠ è½½ä¸é¢„å¤„ç† ===")

# åŠ è½½æ•°æ®
try:
    df = pd.read_csv(file_path, encoding='utf-8')
    print(f"æˆåŠŸåŠ è½½æ•°æ®ï¼Œå½¢çŠ¶: {df.shape}")
except Exception as e:
    print(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
    exit()

# æå–NoCæ ‡ç­¾
df['NoC_True'] = df['Sample File'].apply(extract_noc_from_filename)
df = df.dropna(subset=['NoC_True'])
df['NoC_True'] = df['NoC_True'].astype(int)

print(f"åŸå§‹æ•°æ®NoCåˆ†å¸ƒ: {df['NoC_True'].value_counts().sort_index().to_dict()}")
print(f"åŸå§‹æ ·æœ¬æ•°: {df['Sample File'].nunique()}")

# =====================
# 5. ç®€åŒ–å³°å¤„ç†ä¸CTAè¯„ä¼°
# =====================
print("\n=== æ­¥éª¤2: ç®€åŒ–å³°å¤„ç†ä¸ä¿¡å·è¡¨å¾ ===")

def process_peaks_simplified(sample_data):
    """ç®€åŒ–çš„å³°å¤„ç†å‡½æ•°"""
    processed_data = []
    
    for _, sample_row in sample_data.iterrows():
        sample_file = sample_row['Sample File']
        marker = sample_row['Marker']
        
        # æå–æ‰€æœ‰å³°
        peaks = []
        for i in range(1, 101):
            allele = sample_row.get(f'Allele {i}')
            size = sample_row.get(f'Size {i}')
            height = sample_row.get(f'Height {i}')
            
            if pd.notna(allele) and pd.notna(size) and pd.notna(height):
                original_height = float(height)
                corrected_height = min(original_height, SATURATION_THRESHOLD)
                
                if corrected_height >= HEIGHT_THRESHOLD:
                    peaks.append({
                        'allele': allele,
                        'size': float(size),
                        'height': corrected_height,
                        'original_height': original_height
                    })
        
        if not peaks:
            continue
            
        # ç®€åŒ–çš„CTAè¯„ä¼°
        peaks.sort(key=lambda x: x['height'], reverse=True)
        
        for peak in peaks:
            height_rank = peaks.index(peak) + 1
            total_peaks = len(peaks)
            
            # ç®€åŒ–çš„CTAè®¡ç®—
            if height_rank == 1:
                cta = 0.95
            elif height_rank == 2 and total_peaks >= 2:
                height_ratio = peak['height'] / peaks[0]['height']
                cta = 0.8 if height_ratio > 0.3 else 0.6
            else:
                height_ratio = peak['height'] / peaks[0]['height']
                cta = max(0.1, min(0.8, height_ratio))
            
            if cta >= CTA_THRESHOLD:
                processed_data.append({
                    'Sample File': sample_file,
                    'Marker': marker,
                    'Allele': peak['allele'],
                    'Size': peak['size'],
                    'Height': peak['height'],
                    'Original_Height': peak['original_height'],
                    'CTA': cta
                })
    
    return pd.DataFrame(processed_data)

# å¤„ç†æ‰€æœ‰æ ·æœ¬
print("å¤„ç†å³°æ•°æ®...")
all_processed_peaks = []
for sample_file, group in df.groupby('Sample File'):
    sample_peaks = process_peaks_simplified(group)
    if not sample_peaks.empty:
        all_processed_peaks.append(sample_peaks)

df_peaks = pd.concat(all_processed_peaks, ignore_index=True) if all_processed_peaks else pd.DataFrame()
print(f"å¤„ç†åçš„å³°æ•°æ®å½¢çŠ¶: {df_peaks.shape}")

# =====================
# 6. å¢å¼ºç‰ˆç‰¹å¾å·¥ç¨‹
# =====================
print("\n=== æ­¥éª¤3: å¢å¼ºç‰ˆç‰¹å¾å·¥ç¨‹ ===")

def extract_enhanced_features(sample_file, sample_peaks):
    """æå–å¢å¼ºçš„ç‰¹å¾é›†ï¼ŒåŒ…æ‹¬æ–°çš„é«˜çº§ç‰¹å¾"""
    if sample_peaks.empty:
        return {}
    
    features = {'æ ·æœ¬æ–‡ä»¶': sample_file}
    
    # åŸºç¡€æ•°æ®å‡†å¤‡
    total_peaks = len(sample_peaks)
    all_heights = sample_peaks['Height'].values
    all_sizes = sample_peaks['Size'].values
    
    # æŒ‰ä½ç‚¹åˆ†ç»„ç»Ÿè®¡
    locus_groups = sample_peaks.groupby('Marker')
    alleles_per_locus = locus_groups['Allele'].nunique()
    locus_heights = locus_groups['Height'].sum()
    
    # Aç±»ï¼šå›¾è°±å±‚é¢åŸºç¡€ç»Ÿè®¡ç‰¹å¾
    features['æ ·æœ¬æœ€å¤§ç­‰ä½åŸºå› æ•°'] = alleles_per_locus.max() if len(alleles_per_locus) > 0 else 0
    features['æ ·æœ¬æ€»ç‰¹å¼‚ç­‰ä½åŸºå› æ•°'] = sample_peaks['Allele'].nunique()
    features['æ¯ä½ç‚¹å¹³å‡ç­‰ä½åŸºå› æ•°'] = alleles_per_locus.mean() if len(alleles_per_locus) > 0 else 0
    features['æ¯ä½ç‚¹ç­‰ä½åŸºå› æ•°æ ‡å‡†å·®'] = alleles_per_locus.std() if len(alleles_per_locus) > 1 else 0
    
    # MGTNç³»åˆ—
    for N in [2, 3, 4, 5, 6]:
        features[f'ç­‰ä½åŸºå› æ•°å¤§äº{N}çš„ä½ç‚¹æ•°'] = (alleles_per_locus >= N).sum()
    
    # ç­‰ä½åŸºå› è®¡æ•°åˆ†å¸ƒçš„ç†µ
    if len(alleles_per_locus) > 0:
        counts = alleles_per_locus.value_counts(normalize=True)
        features['ç­‰ä½åŸºå› è®¡æ•°åˆ†å¸ƒç†µ'] = calculate_entropy(counts.values)
    else:
        features['ç­‰ä½åŸºå› è®¡æ•°åˆ†å¸ƒç†µ'] = 0
    
    # Bç±»ï¼šå³°é«˜ã€å¹³è¡¡æ€§åŠéšæœºæ•ˆåº”ç‰¹å¾
    if total_peaks > 0:
        # åŸºç¡€å³°é«˜ç»Ÿè®¡
        features['å¹³å‡å³°é«˜'] = np.mean(all_heights)
        features['å³°é«˜æ ‡å‡†å·®'] = np.std(all_heights) if total_peaks > 1 else 0
        features['æœ€å°å³°é«˜'] = np.min(all_heights)
        features['æœ€å¤§å³°é«˜'] = np.max(all_heights)
        features['å³°é«˜å˜å¼‚ç³»æ•°'] = features['å³°é«˜æ ‡å‡†å·®'] / features['å¹³å‡å³°é«˜'] if features['å¹³å‡å³°é«˜'] > 0 else 0
        
        # æ–°å¢ï¼šå³°é«˜åˆ†ä½æ•°æ¯”ç‡
        if total_peaks >= 4:
            q25 = np.percentile(all_heights, 25)
            q75 = np.percentile(all_heights, 75)
            features['å³°é«˜åˆ†ä½æ•°æ¯”ç‡'] = q75 / q25 if q25 > 0 else 0
        else:
            features['å³°é«˜åˆ†ä½æ•°æ¯”ç‡'] = 1.0
        
        # å³°é«˜æ¯”ç›¸å…³ç‰¹å¾
        phr_values = []
        for marker, marker_group in locus_groups:
            if len(marker_group) == 2:
                heights = marker_group['Height'].values
                phr = min(heights) / max(heights) if max(heights) > 0 else 0
                phr_values.append(phr)
        
        if phr_values:
            features['å¹³å‡å³°é«˜æ¯”'] = np.mean(phr_values)
            features['å³°é«˜æ¯”æ ‡å‡†å·®'] = np.std(phr_values) if len(phr_values) > 1 else 0
            features['æœ€å°å³°é«˜æ¯”'] = np.min(phr_values)
            features['å³°é«˜æ¯”ä¸­ä½æ•°'] = np.median(phr_values)
            features['å¯è®¡ç®—å³°é«˜æ¯”çš„ä½ç‚¹æ•°'] = len(phr_values)
            features['ä¸¥é‡å¤±è¡¡ä½ç‚¹æ•°'] = sum(phr <= PHR_IMBALANCE_THRESHOLD for phr in phr_values)
            features['ä¸¥é‡å¤±è¡¡ä½ç‚¹æ¯”ä¾‹'] = features['ä¸¥é‡å¤±è¡¡ä½ç‚¹æ•°'] / len(phr_values)
        else:
            for key in ['å¹³å‡å³°é«˜æ¯”', 'å³°é«˜æ¯”æ ‡å‡†å·®', 'æœ€å°å³°é«˜æ¯”', 'å³°é«˜æ¯”ä¸­ä½æ•°', 
                       'å¯è®¡ç®—å³°é«˜æ¯”çš„ä½ç‚¹æ•°', 'ä¸¥é‡å¤±è¡¡ä½ç‚¹æ•°', 'ä¸¥é‡å¤±è¡¡ä½ç‚¹æ¯”ä¾‹']:
                features[key] = 0
        
        # å³°é«˜åˆ†å¸ƒå½¢çŠ¶ç‰¹å¾
        if total_peaks > 2:
            features['å³°é«˜åˆ†å¸ƒååº¦'] = stats.skew(all_heights)
            features['å³°é«˜åˆ†å¸ƒå³­åº¦'] = stats.kurtosis(all_heights, fisher=False)
        else:
            features['å³°é«˜åˆ†å¸ƒååº¦'] = 0
            features['å³°é«˜åˆ†å¸ƒå³­åº¦'] = 0
        
        # å³°é«˜å¤šå³°æ€§æŒ‡æ ‡
        try:
            log_heights = np.log(all_heights + 1)
            if len(np.unique(log_heights)) > 1:
                hist, _ = np.histogram(log_heights, bins=min(10, total_peaks))
                peaks_found, _ = find_peaks(hist)
                features['å³°é«˜åˆ†å¸ƒå¤šå³°æ€§'] = len(peaks_found)
            else:
                features['å³°é«˜åˆ†å¸ƒå¤šå³°æ€§'] = 1
        except:
            features['å³°é«˜åˆ†å¸ƒå¤šå³°æ€§'] = 1
        
        # é¥±å’Œæ•ˆåº”ç‰¹å¾
        saturated_peaks = (sample_peaks['Original_Height'] >= SATURATION_THRESHOLD).sum()
        features['é¥±å’Œå³°æ•°é‡'] = saturated_peaks
        features['é¥±å’Œå³°æ¯”ä¾‹'] = saturated_peaks / total_peaks
    else:
        # ç©ºå€¼å¡«å……
        for key in ['å¹³å‡å³°é«˜', 'å³°é«˜æ ‡å‡†å·®', 'æœ€å°å³°é«˜', 'æœ€å¤§å³°é«˜', 'å³°é«˜å˜å¼‚ç³»æ•°',
                   'å³°é«˜åˆ†ä½æ•°æ¯”ç‡', 'å¹³å‡å³°é«˜æ¯”', 'å³°é«˜æ¯”æ ‡å‡†å·®', 'æœ€å°å³°é«˜æ¯”', 'å³°é«˜æ¯”ä¸­ä½æ•°',
                   'å¯è®¡ç®—å³°é«˜æ¯”çš„ä½ç‚¹æ•°', 'ä¸¥é‡å¤±è¡¡ä½ç‚¹æ•°', 'ä¸¥é‡å¤±è¡¡ä½ç‚¹æ¯”ä¾‹',
                   'å³°é«˜åˆ†å¸ƒååº¦', 'å³°é«˜åˆ†å¸ƒå³­åº¦', 'å³°é«˜åˆ†å¸ƒå¤šå³°æ€§',
                   'é¥±å’Œå³°æ•°é‡', 'é¥±å’Œå³°æ¯”ä¾‹']:
            features[key] = 0
    
    # Cç±»ï¼šä¿¡æ¯è®ºåŠå›¾è°±å¤æ‚åº¦ç‰¹å¾
    if len(locus_heights) > 0:
        total_height = locus_heights.sum()
        if total_height > 0:
            locus_probs = locus_heights / total_height
            features['ä½ç‚¹é—´å¹³è¡¡ç†µ'] = calculate_entropy(locus_probs.values)
        else:
            features['ä½ç‚¹é—´å¹³è¡¡ç†µ'] = 0
    else:
        features['ä½ç‚¹é—´å¹³è¡¡ç†µ'] = 0
    
    # å¹³å‡ä½ç‚¹ç­‰ä½åŸºå› åˆ†å¸ƒç†µ
    locus_entropies = []
    for marker, marker_group in locus_groups:
        if len(marker_group) > 1:
            heights = marker_group['Height'].values
            height_sum = heights.sum()
            if height_sum > 0:
                probs = heights / height_sum
                entropy = calculate_entropy(probs)
                locus_entropies.append(entropy)
    
    features['å¹³å‡ä½ç‚¹ç­‰ä½åŸºå› ç†µ'] = np.mean(locus_entropies) if locus_entropies else 0
    
    # æ ·æœ¬æ•´ä½“å³°é«˜åˆ†å¸ƒç†µ
    if total_peaks > 0:
        log_heights = np.log(all_heights + 1)
        hist, _ = np.histogram(log_heights, bins=min(15, total_peaks))
        hist_probs = hist / hist.sum()
        hist_probs = hist_probs[hist_probs > 0]
        features['å³°é«˜åˆ†å¸ƒç†µ'] = calculate_entropy(hist_probs)
    else:
        features['å³°é«˜åˆ†å¸ƒç†µ'] = 0
    
    # å›¾è°±å®Œæ•´æ€§æŒ‡æ ‡
    effective_loci_count = len(locus_groups)
    features['æœ‰æ•ˆç­‰ä½åŸºå› ä½ç‚¹æ•°'] = effective_loci_count
    features['æ— æœ‰æ•ˆç­‰ä½åŸºå› ä½ç‚¹æ•°'] = max(0, 20 - effective_loci_count)  # å‡è®¾20ä¸ªä½ç‚¹
    
    # Dç±»ï¼šDNAé™è§£ä¸ä¿¡æ¯ä¸¢å¤±ç‰¹å¾
    if total_peaks > 1:
        # å³°é«˜ä¸ç‰‡æ®µå¤§å°çš„ç›¸å…³æ€§
        if len(np.unique(all_heights)) > 1 and len(np.unique(all_sizes)) > 1:
            features['å³°é«˜ç‰‡æ®µå¤§å°ç›¸å…³æ€§'] = np.corrcoef(all_heights, all_sizes)[0, 1]
        else:
            features['å³°é«˜ç‰‡æ®µå¤§å°ç›¸å…³æ€§'] = 0
        
        # çº¿æ€§å›å½’æ–œç‡
        features['å³°é«˜ç‰‡æ®µå¤§å°å›å½’æ–œç‡'] = calculate_ols_slope(all_sizes, all_heights)
        
        # åŠ æƒå›å½’æ–œç‡ï¼ˆç®€åŒ–ç‰ˆï¼‰
        try:
            weights = all_heights / all_heights.sum()
            features['åŠ æƒå³°é«˜ç‰‡æ®µå¤§å°æ–œç‡'] = calculate_ols_slope(all_sizes, all_heights)
        except:
            features['åŠ æƒå³°é«˜ç‰‡æ®µå¤§å°æ–œç‡'] = 0
        
        # PHRéšç‰‡æ®µå¤§å°å˜åŒ–çš„æ–œç‡
        if len(phr_values) > 1:
            phr_sizes = []
            for marker, marker_group in locus_groups:
                if len(marker_group) == 2:
                    avg_size = marker_group['Size'].mean()
                    phr_sizes.append(avg_size)
            
            if len(phr_sizes) == len(phr_values) and len(phr_sizes) > 1:
                features['å³°é«˜æ¯”ç‰‡æ®µå¤§å°æ–œç‡'] = calculate_ols_slope(phr_sizes, phr_values)
            else:
                features['å³°é«˜æ¯”ç‰‡æ®µå¤§å°æ–œç‡'] = 0
        else:
            features['å³°é«˜æ¯”ç‰‡æ®µå¤§å°æ–œç‡'] = 0
    else:
        for key in ['å³°é«˜ç‰‡æ®µå¤§å°ç›¸å…³æ€§', 'å³°é«˜ç‰‡æ®µå¤§å°å›å½’æ–œç‡', 'åŠ æƒå³°é«˜ç‰‡æ®µå¤§å°æ–œç‡', 'å³°é«˜æ¯”ç‰‡æ®µå¤§å°æ–œç‡']:
            features[key] = 0
    
    # ä½ç‚¹ä¸¢å¤±è¯„åˆ†ï¼ˆç®€åŒ–ç‰ˆï¼‰
    dropout_score = features['æ— æœ‰æ•ˆç­‰ä½åŸºå› ä½ç‚¹æ•°'] / 20  # åŸºäº20ä¸ªä½ç‚¹çš„å‡è®¾
    features['ç‰‡æ®µå¤§å°åŠ æƒä½ç‚¹ä¸¢å¤±è¯„åˆ†'] = dropout_score
    
    # RFUæ¯ç¢±åŸºå¯¹è¡°å‡æŒ‡æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰
    if len(locus_groups) > 1:
        locus_max_heights = []
        locus_avg_sizes = []
        for marker, marker_group in locus_groups:
            max_height = marker_group['Height'].max()
            avg_size = marker_group['Size'].mean()
            locus_max_heights.append(max_height)
            locus_avg_sizes.append(avg_size)
        
        features['RFUæ¯ç¢±åŸºå¯¹é™è§£æŒ‡æ•°'] = calculate_ols_slope(locus_avg_sizes, locus_max_heights)
    else:
        features['RFUæ¯ç¢±åŸºå¯¹é™è§£æŒ‡æ•°'] = 0
    
    # å°å¤§ç‰‡æ®µä¿¡æ¯å®Œæ•´åº¦æ¯”ç‡ï¼ˆç®€åŒ–ç‰ˆï¼‰
    small_fragment_effective = sum(1 for marker, group in locus_groups if group['Size'].mean() < 200)
    large_fragment_effective = sum(1 for marker, group in locus_groups if group['Size'].mean() >= 200)
    
    if large_fragment_effective > 0:
        features['å°å¤§ç‰‡æ®µä¿¡æ¯å®Œæ•´åº¦æ¯”ç‡'] = small_fragment_effective / large_fragment_effective
    else:
        features['å°å¤§ç‰‡æ®µä¿¡æ¯å®Œæ•´åº¦æ¯”ç‡'] = small_fragment_effective / 0.001
    
    # æ–°å¢é«˜çº§ç‰¹å¾
    # 1. ç­‰ä½åŸºå› å¤šæ ·æ€§æŒ‡æ•° (Simpson's diversity index)
    if total_peaks > 0:
        allele_counts = sample_peaks['Allele'].value_counts()
        total_alleles = allele_counts.sum()
        diversity = 1 - sum((n/total_alleles)**2 for n in allele_counts.values)
        features['ç­‰ä½åŸºå› å¤šæ ·æ€§æŒ‡æ•°'] = diversity
    else:
        features['ç­‰ä½åŸºå› å¤šæ ·æ€§æŒ‡æ•°'] = 0
    
    # 2. å³°æ¨¡å¼å¤æ‚åº¦
    if len(locus_groups) > 0:
        pattern_complexity = 0
        for marker, group in locus_groups:
            n_alleles = len(group)
            if n_alleles > 2:
                # è®¡ç®—å³°é«˜çš„æ ‡å‡†å·®ä½œä¸ºå¤æ‚åº¦çš„ä¸€éƒ¨åˆ†
                height_std = group['Height'].std()
                pattern_complexity += height_std / group['Height'].mean() if group['Height'].mean() > 0 else 0
        features['å³°æ¨¡å¼å¤æ‚åº¦'] = pattern_complexity / len(locus_groups)
    else:
        features['å³°æ¨¡å¼å¤æ‚åº¦'] = 0
    
    # 3. æ‚åˆç‡
    heterozygous_loci = sum(1 for marker, group in locus_groups if len(group) == 2)
    features['æ‚åˆç‡'] = heterozygous_loci / len(locus_groups) if len(locus_groups) > 0 else 0
    
    # 4. å³°èšç±»ç³»æ•°ï¼ˆæµ‹é‡å³°çš„èšé›†ç¨‹åº¦ï¼‰
    if total_peaks > 2:
        # ä½¿ç”¨å³°é«˜çš„å˜å¼‚ç³»æ•°å’Œå³°é—´è·ç¦»
        height_cv = np.std(all_heights) / np.mean(all_heights) if np.mean(all_heights) > 0 else 0
        size_range = np.max(all_sizes) - np.min(all_sizes)
        features['å³°èšç±»ç³»æ•°'] = height_cv * (1 - size_range/1000)  # å½’ä¸€åŒ–
    else:
        features['å³°èšç±»ç³»æ•°'] = 0
    
    return features

# æå–æ‰€æœ‰æ ·æœ¬çš„ç‰¹å¾
print("å¼€å§‹å¢å¼ºç‰ˆç‰¹å¾æå–...")
all_features = []

if df_peaks.empty:
    print("è­¦å‘Š: å¤„ç†åçš„å³°æ•°æ®ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤ç‰¹å¾")
    for sample_file in df['Sample File'].unique():
        features = {'æ ·æœ¬æ–‡ä»¶': sample_file, 'æ ·æœ¬æœ€å¤§ç­‰ä½åŸºå› æ•°': 0}
        all_features.append(features)
else:
    for sample_file, group in df_peaks.groupby('Sample File'):
        features = extract_enhanced_features(sample_file, group)
        if features:
            all_features.append(features)

df_features = pd.DataFrame(all_features)

# åˆå¹¶NoCæ ‡ç­¾
noc_map = df.groupby('Sample File')['NoC_True'].first().to_dict()
df_features['è´¡çŒ®è€…äººæ•°'] = df_features['æ ·æœ¬æ–‡ä»¶'].map(noc_map)
df_features = df_features.dropna(subset=['è´¡çŒ®è€…äººæ•°'])

# å¡«å……ç¼ºå¤±å€¼
numeric_cols = df_features.select_dtypes(include=[np.number]).columns
df_features[numeric_cols] = df_features[numeric_cols].fillna(0)

print(f"ç‰¹å¾æ•°æ®å½¢çŠ¶: {df_features.shape}")
print(f"ç‰¹å¾æ•°é‡: {len([col for col in df_features.columns if col not in ['æ ·æœ¬æ–‡ä»¶', 'è´¡çŒ®è€…äººæ•°']])}")

# =====================
# 7. RFECVç‰¹å¾é€‰æ‹©
# =====================
print("\n=== æ­¥éª¤4: RFECVé€’å½’ç‰¹å¾æ¶ˆé™¤ ===")

# å‡†å¤‡æ•°æ®
feature_cols = [col for col in df_features.columns if col not in ['æ ·æœ¬æ–‡ä»¶', 'è´¡çŒ®è€…äººæ•°']]
X = df_features[feature_cols].fillna(0)
y = df_features['è´¡çŒ®è€…äººæ•°']

print(f"åŸå§‹ç‰¹å¾æ•°: {len(feature_cols)}")
print(f"æ ·æœ¬æ•°: {len(X)}")
print(f"NoCåˆ†å¸ƒ: {y.value_counts().sort_index().to_dict()}")

# æ ‡ç­¾ç¼–ç 
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# ç‰¹å¾æ ‡å‡†åŒ–
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# æ•°æ®å¹³è¡¡æ€§æ£€æŸ¥
print("\næ•°æ®å¹³è¡¡æ€§æ£€æŸ¥:")
class_distribution = pd.Series(y_encoded).value_counts().sort_index()
for cls, count in class_distribution.items():
    original_cls = label_encoder.inverse_transform([cls])[0]
    print(f"  {original_cls}äºº: {count}ä¸ªæ ·æœ¬")

min_samples = class_distribution.min()
imbalance_ratio = class_distribution.max() / min_samples
print(f"ä¸å¹³è¡¡æ¯”ä¾‹: {imbalance_ratio:.2f}")

# å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
if imbalance_ratio > 3:
    try:
        from imblearn.over_sampling import SMOTE
        print("ä½¿ç”¨SMOTEå¤„ç†ç±»åˆ«ä¸å¹³è¡¡...")
        k_neighbors = min(3, min_samples - 1) if min_samples > 1 else 1
        smote = SMOTE(random_state=42, k_neighbors=k_neighbors)
        X_scaled, y_encoded = smote.fit_resample(X_scaled, y_encoded)
        print(f"SMOTEåæ ·æœ¬æ•°: {len(X_scaled)}")
    except ImportError:
        print("æœªå®‰è£…imblearnï¼Œè·³è¿‡SMOTEå¤„ç†")
    except Exception as e:
        print(f"SMOTEå¤„ç†å¤±è´¥: {e}")

# è®¾ç½®äº¤å‰éªŒè¯
min_class_size = pd.Series(y_encoded).value_counts().min()
cv_folds = min(5, min_class_size)
cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)

print(f"\nå¼€å§‹RFECVç‰¹å¾é€‰æ‹©ï¼ˆ{cv_folds}æŠ˜äº¤å‰éªŒè¯ï¼‰...")

# åˆ›å»ºåŸºç¡€æ¢¯åº¦æå‡æœºç”¨äºRFECV
base_estimator = GradientBoostingClassifier(
    n_estimators=100,
    max_depth=4,
    learning_rate=0.1,
    random_state=42,
    warm_start=False  # ç¡®ä¿ä¸ä½¿ç”¨warm_starté¿å…å‚æ•°å†²çª
)

# è‡ªå®šä¹‰è¯„åˆ†å‡½æ•°ï¼ˆå¹³è¡¡å‡†ç¡®ç‡ï¼‰
def balanced_accuracy_scorer(estimator, X, y):
    """å¹³è¡¡å‡†ç¡®ç‡è¯„åˆ†å‡½æ•°"""
    y_pred = estimator.predict(X)
    unique_classes = np.unique(y)
    class_accuracies = []
    
    for cls in unique_classes:
        cls_mask = (y == cls)
        if cls_mask.sum() > 0:
            cls_acc = (y_pred[cls_mask] == y[cls_mask]).mean()
            class_accuracies.append(cls_acc)
    
    return np.mean(class_accuracies)

balanced_scorer = make_scorer(balanced_accuracy_scorer)

# æ‰§è¡ŒRFECV
print("æ‰§è¡Œé€’å½’ç‰¹å¾æ¶ˆé™¤äº¤å‰éªŒè¯...")
start_time = time()

rfecv = RFECV(
    estimator=base_estimator,
    step=1,
    cv=cv,
    scoring=balanced_scorer,
    n_jobs=-1,
    verbose=1
)

rfecv.fit(X_scaled, y_encoded)

elapsed_time = time() - start_time
print(f"RFECVå®Œæˆï¼Œè€—æ—¶: {elapsed_time:.1f}ç§’")

# è·å–é€‰æ‹©çš„ç‰¹å¾
selected_features = [feature_cols[i] for i in range(len(feature_cols)) if rfecv.support_[i]]
selected_indices = [i for i in range(len(feature_cols)) if rfecv.support_[i]]

print(f"\nRFECVé€‰æ‹©çš„ç‰¹å¾æ•°: {len(selected_features)}")
print(f"æœ€ä¼˜ç‰¹å¾æ•°: {rfecv.n_features_}")
print(f"æœ€ä½³äº¤å‰éªŒè¯åˆ†æ•°: {rfecv.grid_scores_[rfecv.n_features_ - 1]:.4f}")

print("\nRFECVé€‰æ‹©çš„ç‰¹å¾:")
for i, feature in enumerate(selected_features, 1):
    print(f"  {i:2d}. {feature}")

# åˆ›å»ºæœ€ç»ˆçš„ç‰¹å¾çŸ©é˜µ
X_selected = X_scaled[:, selected_indices]

# å¯è§†åŒ–RFECVç»“æœ
plt.figure(figsize=(12, 8))
plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_, 'bo-')
plt.axvline(x=rfecv.n_features_, color='red', linestyle='--', 
           label=f'æœ€ä¼˜ç‰¹å¾æ•°: {rfecv.n_features_}')
plt.xlabel('ç‰¹å¾æ•°é‡')
plt.ylabel('äº¤å‰éªŒè¯åˆ†æ•°')
plt.title('RFECVç‰¹å¾é€‰æ‹©ç»“æœ')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig(os.path.join(PLOTS_DIR, 'RFECVç»“æœ.png'), dpi=300, bbox_inches='tight')
plt.close()

# =====================
# 8. é«˜çº§æ¢¯åº¦æå‡æœºä¼˜åŒ–
# =====================
print("\n=== æ­¥éª¤5: æ¢¯åº¦æå‡æœºæ·±åº¦ä¼˜åŒ– ===")

# è‡ªå®šä¹‰åˆ†å±‚åˆ’åˆ†
def custom_stratified_split(X, y, test_size=0.25, random_state=42):
    """ç¡®ä¿æ¯ä¸ªç±»åˆ«åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­éƒ½æœ‰ä»£è¡¨"""
    np.random.seed(random_state)
    unique_classes = np.unique(y)
    
    X_train_list, X_test_list = [], []
    y_train_list, y_test_list = [], []
    
    for cls in unique_classes:
        cls_indices = np.where(y == cls)[0]
        n_cls = len(cls_indices)
        
        if n_cls <= 2:
            n_test = 1
            n_train = n_cls - 1
        else:
            n_test = max(1, int(n_cls * test_size))
            n_train = n_cls - n_test
        
        np.random.shuffle(cls_indices)
        test_indices = cls_indices[:n_test]
        train_indices = cls_indices[n_test:n_test+n_train]
        
        X_train_list.append(X[train_indices])
        X_test_list.append(X[test_indices])
        y_train_list.append(y[train_indices])
        y_test_list.append(y[test_indices])
    
    X_train = np.vstack(X_train_list)
    X_test = np.vstack(X_test_list)
    y_train = np.hstack(y_train_list)
    y_test = np.hstack(y_test_list)
    
    return X_train, X_test, y_train, y_test

# åˆ’åˆ†æ•°æ®é›†
X_train, X_test, y_train, y_test = custom_stratified_split(X_selected, y_encoded, test_size=0.25)

print(f"è®­ç»ƒé›†å½¢çŠ¶: {X_train.shape}")
print(f"æµ‹è¯•é›†å½¢çŠ¶: {X_test.shape}")
print(f"è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ: {pd.Series(y_train).value_counts().sort_index().to_dict()}")
print(f"æµ‹è¯•é›†æ ‡ç­¾åˆ†å¸ƒ: {pd.Series(y_test).value_counts().sort_index().to_dict()}")

# è®¡ç®—ç±»åˆ«æƒé‡
class_weights = compute_sample_weight('balanced', y_train)

# è®¾ç½®ä¼˜åŒ–çš„äº¤å‰éªŒè¯
min_class_size_train = pd.Series(y_train).value_counts().min()
cv_folds_opt = min(5, min_class_size_train)
cv_opt = StratifiedKFold(n_splits=cv_folds_opt, shuffle=True, random_state=42)

# === é˜¶æ®µ1: ç²—è°ƒå‚æ•° ===
print("\né˜¶æ®µ1: æ¢¯åº¦æå‡æœºç²—è°ƒå‚æ•°...")

# ç²—è°ƒå‚æ•°ç½‘æ ¼
coarse_param_grid = {
    'n_estimators': [100, 200, 300, 500],
    'max_depth': [3, 4, 5, 6, 7],
    'learning_rate': [0.05, 0.1, 0.15, 0.2],
    'subsample': [0.8, 0.9, 1.0]
}

# ç²—è°ƒæœç´¢
print("æ‰§è¡Œç²—è°ƒéšæœºæœç´¢...")
coarse_search = RandomizedSearchCV(
    GradientBoostingClassifier(random_state=42),
    coarse_param_grid,
    n_iter=50,
    cv=cv_opt,
    scoring=balanced_scorer,
    n_jobs=-1,
    random_state=42,
    verbose=1
)

coarse_search.fit(X_train, y_train, sample_weight=class_weights)

print(f"ç²—è°ƒæœ€ä½³å‚æ•°: {coarse_search.best_params_}")
print(f"ç²—è°ƒæœ€ä½³åˆ†æ•°: {coarse_search.best_score_:.4f}")

# === é˜¶æ®µ2: ç»†è°ƒå‚æ•° ===
print("\né˜¶æ®µ2: åŸºäºç²—è°ƒç»“æœè¿›è¡Œç»†è°ƒ...")

# æå–ç²—è°ƒæœ€ä½³å‚æ•°
best_coarse = coarse_search.best_params_

# ç»†è°ƒå‚æ•°ç½‘æ ¼ï¼ˆåœ¨ç²—è°ƒæœ€ä½³å‚æ•°é™„è¿‘æœç´¢ï¼‰
fine_param_grid = {}

# n_estimators ç»†è°ƒ
if best_coarse['n_estimators'] == 100:
    fine_param_grid['n_estimators'] = [80, 100, 120, 150]
elif best_coarse['n_estimators'] == 500:
    fine_param_grid['n_estimators'] = [400, 500, 600, 700]
else:
    base_n = best_coarse['n_estimators']
    fine_param_grid['n_estimators'] = [base_n - 50, base_n, base_n + 50, base_n + 100]

# max_depth ç»†è°ƒ
base_depth = best_coarse['max_depth']
fine_param_grid['max_depth'] = [max(2, base_depth - 1), base_depth, base_depth + 1]

# learning_rate ç»†è°ƒ
base_lr = best_coarse['learning_rate']
if base_lr == 0.05:
    fine_param_grid['learning_rate'] = [0.03, 0.05, 0.07, 0.08]
elif base_lr == 0.2:
    fine_param_grid['learning_rate'] = [0.15, 0.18, 0.2, 0.25]
else:
    fine_param_grid['learning_rate'] = [base_lr - 0.02, base_lr, base_lr + 0.02, base_lr + 0.05]

# subsample ç»†è°ƒ
base_sub = best_coarse['subsample']
if base_sub == 0.8:
    fine_param_grid['subsample'] = [0.75, 0.8, 0.85]
elif base_sub == 1.0:
    fine_param_grid['subsample'] = [0.9, 0.95, 1.0]
else:
    fine_param_grid['subsample'] = [base_sub - 0.1, base_sub, base_sub + 0.1]

# æ·»åŠ å…¶ä»–é‡è¦å‚æ•°
fine_param_grid.update({
    'min_samples_split': [2, 5, 10, 15],
    'min_samples_leaf': [1, 2, 4, 6],
    'max_features': ['sqrt', 'log2', None, 0.8]
})

print("æ‰§è¡Œç»†è°ƒç½‘æ ¼æœç´¢...")
fine_search = GridSearchCV(
    GradientBoostingClassifier(random_state=42),
    fine_param_grid,
    cv=cv_opt,
    scoring=balanced_scorer,
    n_jobs=-1,
    verbose=1
)

fine_search.fit(X_train, y_train, sample_weight=class_weights)

print(f"ç»†è°ƒæœ€ä½³å‚æ•°: {fine_search.best_params_}")
print(f"ç»†è°ƒæœ€ä½³åˆ†æ•°: {fine_search.best_score_:.4f}")

# === é˜¶æ®µ3: æœ€ç»ˆä¼˜åŒ– ===
print("\né˜¶æ®µ3: æœ€ç»ˆå‚æ•°å¾®è°ƒ...")

# æœ€ç»ˆå¾®è°ƒï¼ˆä¸»è¦é’ˆå¯¹æ­£åˆ™åŒ–å‚æ•°ï¼‰
final_params = fine_search.best_params_.copy()

# éªŒè¯æ­£åˆ™åŒ–å‚æ•°
validation_param_grid = {
    'min_samples_split': [max(2, final_params['min_samples_split'] - 2), 
                         final_params['min_samples_split'],
                         final_params['min_samples_split'] + 2],
    'min_samples_leaf': [max(1, final_params['min_samples_leaf'] - 1),
                        final_params['min_samples_leaf'],
                        final_params['min_samples_leaf'] + 1],
    'min_weight_fraction_leaf': [0.0, 0.01, 0.02],
    'max_leaf_nodes': [None, 20, 30, 50]
}

# å›ºå®šå…¶ä»–å‚æ•°
base_estimator_final = GradientBoostingClassifier(
    n_estimators=final_params['n_estimators'],
    max_depth=final_params['max_depth'],
    learning_rate=final_params['learning_rate'],
    subsample=final_params['subsample'],
    max_features=final_params['max_features'],
    random_state=42
)

print("æ‰§è¡Œæœ€ç»ˆå¾®è°ƒ...")
final_search = GridSearchCV(
    base_estimator_final,
    validation_param_grid,
    cv=cv_opt,
    scoring=balanced_scorer,
    n_jobs=-1,
    verbose=1
)

final_search.fit(X_train, y_train, sample_weight=class_weights)

print(f"æœ€ç»ˆæœ€ä½³å‚æ•°: {final_search.best_params_}")
print(f"æœ€ç»ˆæœ€ä½³åˆ†æ•°: {final_search.best_score_:.4f}")

# åˆ›å»ºæœ€ä¼˜æ¨¡å‹
final_params.update(final_search.best_params_)
optimal_gb_model = GradientBoostingClassifier(**final_params, random_state=42)
optimal_gb_model.fit(X_train, y_train, sample_weight=class_weights)

# åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
y_pred_gb = optimal_gb_model.predict(X_test)
gb_accuracy = accuracy_score(y_test, y_pred_gb)
balanced_acc = balanced_accuracy_scorer(optimal_gb_model, X_test, y_test)
f1_weighted = f1_score(y_test, y_pred_gb, average='weighted')

print(f"\næœ€ä¼˜æ¢¯åº¦æå‡æœºæµ‹è¯•é›†æ€§èƒ½:")
print(f"  å‡†ç¡®ç‡: {gb_accuracy:.4f}")
print(f"  å¹³è¡¡å‡†ç¡®ç‡: {balanced_acc:.4f}")
print(f"  åŠ æƒF1åˆ†æ•°: {f1_weighted:.4f}")

# =====================
# 9. éªŒè¯æ›²çº¿åˆ†æ
# =====================
print("\n=== æ­¥éª¤6: éªŒè¯æ›²çº¿åˆ†æ ===")

# åˆ†æå…³é”®è¶…å‚æ•°çš„å½±å“
key_params = ['n_estimators', 'max_depth', 'learning_rate']

fig, axes = plt.subplots(1, 3, figsize=(18, 6))

for i, param in enumerate(key_params):
    print(f"åˆ†æå‚æ•°: {param}")
    
    # è®¾ç½®å‚æ•°èŒƒå›´
    if param == 'n_estimators':
        param_range = [50, 100, 200, 300, 400, 500]
    elif param == 'max_depth':
        param_range = [2, 3, 4, 5, 6, 7, 8]
    elif param == 'learning_rate':
        param_range = [0.01, 0.05, 0.1, 0.15, 0.2, 0.3]
    
    # åˆ›å»ºåŸºç¡€æ¨¡å‹ï¼ˆä½¿ç”¨æœ€ä¼˜å‚æ•°ï¼Œä½†å˜åŒ–å½“å‰å‚æ•°ï¼‰
    base_params = final_params.copy()
    base_params.pop(param, None)  # ç§»é™¤å½“å‰è¦åˆ†æçš„å‚æ•°
    
    train_scores, validation_scores = validation_curve(
        GradientBoostingClassifier(**base_params, random_state=42),
        X_train, y_train,
        param_name=param, param_range=param_range,
        cv=cv_opt, scoring=balanced_scorer,
        n_jobs=-1
    )
    
    train_mean = np.mean(train_scores, axis=1)
    train_std = np.std(train_scores, axis=1)
    validation_mean = np.mean(validation_scores, axis=1)
    validation_std = np.std(validation_scores, axis=1)
    
    axes[i].plot(param_range, train_mean, 'o-', color='blue', label='è®­ç»ƒé›†')
    axes[i].plot(param_range, validation_mean, 'o-', color='red', label='éªŒè¯é›†')
    axes[i].fill_between(param_range, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')
    axes[i].fill_between(param_range, validation_mean - validation_std, validation_mean + validation_std, alpha=0.1, color='red')
    
    # æ ‡è®°æœ€ä¼˜å€¼
    optimal_value = final_params[param]
    if optimal_value in param_range:
        optimal_idx = param_range.index(optimal_value)
        axes[i].axvline(x=optimal_value, color='green', linestyle='--', alpha=0.7, label=f'æœ€ä¼˜å€¼: {optimal_value}')
    
    axes[i].set_xlabel(param)
    axes[i].set_ylabel('å¹³è¡¡å‡†ç¡®ç‡')
    axes[i].set_title(f'{param} éªŒè¯æ›²çº¿')
    axes[i].legend()
    axes[i].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(os.path.join(PLOTS_DIR, 'éªŒè¯æ›²çº¿åˆ†æ.png'), dpi=300, bbox_inches='tight')
plt.close()

# =====================
# 10. ç»“æœåˆ†æä¸å¯è§†åŒ–
# =====================
print("\n=== æ­¥éª¤7: ç»“æœåˆ†æä¸å¯è§†åŒ– ===")

# è½¬æ¢æ ‡ç­¾ç”¨äºæ˜¾ç¤º
y_test_orig = label_encoder.inverse_transform(y_test)
y_pred_orig = label_encoder.inverse_transform(y_pred_gb)

# åˆ†ç±»æŠ¥å‘Š
class_names = [f"{x}äºº" for x in sorted(label_encoder.classes_)]
print(f"\næ¢¯åº¦æå‡æœºè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test_orig, y_pred_orig, target_names=class_names))

# æ··æ·†çŸ©é˜µå¯è§†åŒ–
plt.figure(figsize=(10, 8))
cm = confusion_matrix(y_test_orig, y_pred_orig)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
           xticklabels=class_names, yticklabels=class_names)
plt.title('ä¼˜åŒ–æ¢¯åº¦æå‡æœºæ··æ·†çŸ©é˜µ')
plt.ylabel('çœŸå®NoC')
plt.xlabel('é¢„æµ‹NoC')
plt.tight_layout()
plt.savefig(os.path.join(PLOTS_DIR, 'æ··æ·†çŸ©é˜µ.png'), dpi=300, bbox_inches='tight')
plt.close()

# ç‰¹å¾é‡è¦æ€§åˆ†æ
plt.figure(figsize=(14, 10))
feature_importance = pd.DataFrame({
    'ç‰¹å¾': selected_features,
    'é‡è¦æ€§': optimal_gb_model.feature_importances_
}).sort_values('é‡è¦æ€§', ascending=False)

# æ˜¾ç¤ºæ‰€æœ‰é€‰æ‹©çš„ç‰¹å¾
sns.barplot(data=feature_importance, x='é‡è¦æ€§', y='ç‰¹å¾')
plt.title(f'æ¢¯åº¦æå‡æœºç‰¹å¾é‡è¦æ€§æ’å (RFECVé€‰æ‹©çš„{len(selected_features)}ä¸ªç‰¹å¾)')
plt.xlabel('ç‰¹å¾é‡è¦æ€§')
plt.tight_layout()
plt.savefig(os.path.join(PLOTS_DIR, 'ç‰¹å¾é‡è¦æ€§.png'), dpi=300, bbox_inches='tight')
plt.close()

print(f"\næ¢¯åº¦æå‡æœºç‰¹å¾é‡è¦æ€§æ’å:")
for idx, row in feature_importance.iterrows():
    print(f"  {row['ç‰¹å¾']:35} {row['é‡è¦æ€§']:.4f}")

# å­¦ä¹ æ›²çº¿
from sklearn.model_selection import learning_curve

try:
    train_sizes, train_scores, val_scores = learning_curve(
        optimal_gb_model, X_selected, y_encoded, cv=cv_opt, 
        train_sizes=np.linspace(0.1, 1.0, 10),
        scoring=balanced_scorer, random_state=42,
        n_jobs=-1
    )

    plt.figure(figsize=(12, 8))
    train_mean = np.mean(train_scores, axis=1)
    train_std = np.std(train_scores, axis=1)
    val_mean = np.mean(val_scores, axis=1)
    val_std = np.std(val_scores, axis=1)

    plt.plot(train_sizes, train_mean, 'o-', color='red', label='è®­ç»ƒé›†')
    plt.plot(train_sizes, val_mean, 'o-', color='green', label='éªŒè¯é›†')
    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='red')
    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='green')

    plt.xlabel('è®­ç»ƒæ ·æœ¬æ•°')
    plt.ylabel('å¹³è¡¡å‡†ç¡®ç‡')
    plt.title('ä¼˜åŒ–æ¢¯åº¦æå‡æœºå­¦ä¹ æ›²çº¿')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(os.path.join(PLOTS_DIR, 'å­¦ä¹ æ›²çº¿.png'), dpi=300, bbox_inches='tight')
    plt.close()
except Exception as e:
    print(f"å­¦ä¹ æ›²çº¿ç”Ÿæˆå¤±è´¥: {e}")

# ä¼˜åŒ–è¿‡ç¨‹å¯è§†åŒ–
optimization_scores = [
    coarse_search.best_score_,
    fine_search.best_score_,
    final_search.best_score_
]

optimization_stages = ['ç²—è°ƒ', 'ç»†è°ƒ', 'æœ€ç»ˆå¾®è°ƒ']

plt.figure(figsize=(10, 6))
plt.plot(optimization_stages, optimization_scores, 'bo-', linewidth=2, markersize=8)
plt.xlabel('ä¼˜åŒ–é˜¶æ®µ')
plt.ylabel('äº¤å‰éªŒè¯åˆ†æ•°')
plt.title('æ¢¯åº¦æå‡æœºå‚æ•°ä¼˜åŒ–è¿‡ç¨‹')
plt.grid(True, alpha=0.3)

for i, score in enumerate(optimization_scores):
    plt.text(i, score + 0.005, f'{score:.4f}', ha='center', va='bottom')

plt.tight_layout()
plt.savefig(os.path.join(PLOTS_DIR, 'ä¼˜åŒ–è¿‡ç¨‹.png'), dpi=300, bbox_inches='tight')
plt.close()

# =====================
# 11. SHAPå¯è§£é‡Šæ€§åˆ†æ
# =====================
if SHAP_AVAILABLE:
    print("\n=== æ­¥éª¤8: SHAPå¯è§£é‡Šæ€§åˆ†æ ===")
    
    try:
        # åˆ›å»ºSHAPè§£é‡Šå™¨
        explainer = shap.TreeExplainer(optimal_gb_model)
        
        # è®¡ç®—SHAPå€¼
        shap_sample_size = min(30, len(X_test))
        X_shap = X_test[:shap_sample_size]
        shap_values = explainer.shap_values(X_shap)
        
        # å¤„ç†å¤šåˆ†ç±»æƒ…å†µ
        if isinstance(shap_values, list):
            shap_values_mean = np.mean([np.abs(sv) for sv in shap_values], axis=0)
        else:
            shap_values_mean = np.abs(shap_values)
        
        # SHAPç‰¹å¾é‡è¦æ€§
        feature_shap_importance = np.mean(shap_values_mean, axis=0)
        shap_importance_df = pd.DataFrame({
            'ç‰¹å¾': selected_features,
            'SHAPé‡è¦æ€§': feature_shap_importance
        }).sort_values('SHAPé‡è¦æ€§', ascending=False)
        
        plt.figure(figsize=(12, 8))
        sns.barplot(data=shap_importance_df, x='SHAPé‡è¦æ€§', y='ç‰¹å¾')
        plt.title('SHAPç‰¹å¾é‡è¦æ€§æ’å')
        plt.xlabel('å¹³å‡SHAPé‡è¦æ€§')
        plt.tight_layout()
        plt.savefig(os.path.join(PLOTS_DIR, 'SHAPé‡è¦æ€§.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        print("SHAPç‰¹å¾é‡è¦æ€§æ’å:")
        for idx, row in shap_importance_df.iterrows():
            print(f"  {row['ç‰¹å¾']:35} {row['SHAPé‡è¦æ€§']:.4f}")
        
        # SHAPæ‘˜è¦å›¾
        try:
            if isinstance(shap_values, list):
                shap_values_plot = shap_values[0]
            else:
                shap_values_plot = shap_values
                
            plt.figure(figsize=(10, 8))
            shap.summary_plot(shap_values_plot, X_shap, feature_names=selected_features, 
                            plot_type="bar", show=False)
            plt.title("SHAPæ‘˜è¦å›¾")
            plt.tight_layout()
            plt.savefig(os.path.join(PLOTS_DIR, 'SHAPæ‘˜è¦å›¾.png'), dpi=300, bbox_inches='tight')
            plt.close()
        except Exception as e:
            print(f"SHAPæ‘˜è¦å›¾ç”Ÿæˆå¤±è´¥: {e}")
            
    except Exception as e:
        print(f"SHAPåˆ†æå¤±è´¥: {e}")

# =====================
# 12. æ¨¡å‹é¢„æµ‹ä¸ä¿å­˜
# =====================
print("\n=== æ­¥éª¤9: æ¨¡å‹é¢„æµ‹ä¸ä¿å­˜ ===")

# å¯¹æ‰€æœ‰æ ·æœ¬è¿›è¡Œé¢„æµ‹
y_pred_all = optimal_gb_model.predict(X_selected)
y_pred_all_orig = label_encoder.inverse_transform(y_pred_all)

# æ·»åŠ é¢„æµ‹ç»“æœåˆ°ç‰¹å¾æ•°æ®æ¡†
df_features['é¢„æµ‹è´¡çŒ®è€…äººæ•°'] = y_pred_all_orig

# è®¡ç®—æ•´ä½“å‡†ç¡®ç‡
overall_accuracy = (df_features['é¢„æµ‹è´¡çŒ®è€…äººæ•°'] == df_features['è´¡çŒ®è€…äººæ•°']).mean()
print(f"æ•´ä½“é¢„æµ‹å‡†ç¡®ç‡: {overall_accuracy:.4f}")

# å„NoCç±»åˆ«çš„å‡†ç¡®ç‡
noc_accuracy = df_features.groupby('è´¡çŒ®è€…äººæ•°').apply(
    lambda x: (x['é¢„æµ‹è´¡çŒ®è€…äººæ•°'] == x['è´¡çŒ®è€…äººæ•°']).mean()
).reset_index(name='å‡†ç¡®ç‡')

print("\nå„NoCç±»åˆ«é¢„æµ‹å‡†ç¡®ç‡:")
for _, row in noc_accuracy.iterrows():
    print(f"  {int(row['è´¡çŒ®è€…äººæ•°'])}äºº: {row['å‡†ç¡®ç‡']:.4f}")

# å¯è§†åŒ–å„ç±»åˆ«å‡†ç¡®ç‡
plt.figure(figsize=(10, 6))
sns.barplot(data=noc_accuracy, x='è´¡çŒ®è€…äººæ•°', y='å‡†ç¡®ç‡')
plt.ylim(0, 1.1)
plt.xlabel('çœŸå®NoC')
plt.ylabel('é¢„æµ‹å‡†ç¡®ç‡')
plt.title('å„NoCç±»åˆ«é¢„æµ‹å‡†ç¡®ç‡')

for i, row in noc_accuracy.iterrows():
    plt.text(i, row['å‡†ç¡®ç‡'] + 0.03, f"{row['å‡†ç¡®ç‡']:.3f}", 
             ha='center', va='bottom')

plt.tight_layout()
plt.savefig(os.path.join(PLOTS_DIR, 'å„ç±»åˆ«å‡†ç¡®ç‡.png'), dpi=300, bbox_inches='tight')
plt.close()

# ä¿å­˜ç»“æœ
df_features.to_csv(os.path.join(DATA_DIR, 'NoCè¯†åˆ«ç»“æœ_RFECV_GBä¼˜åŒ–ç‰ˆ.csv'), 
                   index=False, encoding='utf-8-sig')

# ä¿å­˜æ¨¡å‹
import joblib
model_filename = os.path.join(DATA_DIR, 'noc_optimized_gradient_boosting_model.pkl')
joblib.dump({
    'model': optimal_gb_model,
    'scaler': scaler,
    'label_encoder': label_encoder,
    'selected_features': selected_features,
    'selected_indices': selected_indices,
    'rfecv': rfecv,
    'optimization_history': {
        'coarse_best': coarse_search.best_params_,
        'fine_best': fine_search.best_params_,
        'final_best': final_search.best_params_,
        'scores': optimization_scores
    }
}, model_filename)

print(f"ä¼˜åŒ–æ¨¡å‹å·²ä¿å­˜è‡³: {model_filename}")

# ä¿å­˜è¯¦ç»†æ‘˜è¦
summary = {
    'æ¨¡å‹ä¿¡æ¯': {
        'æ¨¡å‹ç±»å‹': 'OptimizedGradientBoostingClassifier',
        'ç‰¹å¾é€‰æ‹©æ–¹æ³•': 'RFECV',
        'ä¼˜åŒ–é˜¶æ®µ': ['ç²—è°ƒ', 'ç»†è°ƒ', 'æœ€ç»ˆå¾®è°ƒ'],
        'æµ‹è¯•é›†å‡†ç¡®ç‡': float(gb_accuracy),
        'å¹³è¡¡å‡†ç¡®ç‡': float(balanced_acc),
        'åŠ æƒF1åˆ†æ•°': float(f1_weighted),
        'æ•´ä½“å‡†ç¡®ç‡': float(overall_accuracy)
    },
    'ç‰¹å¾é€‰æ‹©ç»“æœ': {
        'åŸå§‹ç‰¹å¾æ•°': len(feature_cols),
        'æœ€ç»ˆç‰¹å¾æ•°': len(selected_features),
        'RFECVæœ€ä¼˜ç‰¹å¾æ•°': int(rfecv.n_features_),
        'RFECVæœ€ä½³åˆ†æ•°': float(rfecv.grid_scores_[rfecv.n_features_ - 1]),
        'é€‰æ‹©çš„ç‰¹å¾': selected_features
    },
    'ä¼˜åŒ–è¿‡ç¨‹': {
        'ç²—è°ƒæœ€ä½³å‚æ•°': coarse_search.best_params_,
        'ç²—è°ƒæœ€ä½³åˆ†æ•°': float(coarse_search.best_score_),
        'ç»†è°ƒæœ€ä½³å‚æ•°': fine_search.best_params_,
        'ç»†è°ƒæœ€ä½³åˆ†æ•°': float(fine_search.best_score_),
        'æœ€ç»ˆæœ€ä½³å‚æ•°': final_search.best_params_,
        'æœ€ç»ˆæœ€ä½³åˆ†æ•°': float(final_search.best_score_)
    },
    'æœ€ç»ˆæ¨¡å‹å‚æ•°': final_params,
    'æ•°æ®ä¿¡æ¯': {
        'æ€»æ ·æœ¬æ•°': len(df_features),
        'NoCåˆ†å¸ƒ': df_features['è´¡çŒ®è€…äººæ•°'].value_counts().sort_index().to_dict(),
        'è®­ç»ƒé›†å¤§å°': len(X_train),
        'æµ‹è¯•é›†å¤§å°': len(X_test)
    },
    'å„ç±»åˆ«å‡†ç¡®ç‡': {
        int(row['è´¡çŒ®è€…äººæ•°']): float(row['å‡†ç¡®ç‡']) 
        for _, row in noc_accuracy.iterrows()
    },
    'ç‰¹å¾é‡è¦æ€§å‰10': [
        {
            'ç‰¹å¾': row['ç‰¹å¾'],
            'é‡è¦æ€§': float(row['é‡è¦æ€§'])
        }
        for _, row in feature_importance.head(10).iterrows()
    ],
    'æ—¶é—´æˆ³': pd.Timestamp.now().strftime("%Y-%m-%d %H:%M:%S")
}

with open(os.path.join(DATA_DIR, 'NoCåˆ†ææ‘˜è¦_RFECV_GBä¼˜åŒ–ç‰ˆ.json'), 'w', encoding='utf-8') as f:
    json.dump(summary, f, ensure_ascii=False, indent=2)

# =====================
# 13. æœ€ç»ˆæŠ¥å‘Š
# =====================
print("\n" + "="*80)
print("         æ³•åŒ»æ··åˆSTRå›¾è°±NoCè¯†åˆ« - RFECV+æ¢¯åº¦æå‡æœºä¼˜åŒ–ç‰ˆæœ€ç»ˆæŠ¥å‘Š")
print("="*80)

print(f"\nğŸ“Š æ•°æ®æ¦‚å†µ:")
print(f"   â€¢ æ€»æ ·æœ¬æ•°: {len(df_features)}")
print(f"   â€¢ NoCåˆ†å¸ƒ: {dict(df_features['è´¡çŒ®è€…äººæ•°'].value_counts().sort_index())}")
print(f"   â€¢ åŸå§‹ç‰¹å¾æ•°: {len(feature_cols)}")
print(f"   â€¢ RFECVé€‰æ‹©ç‰¹å¾æ•°: {len(selected_features)}")

print(f"\nğŸ”§ æŠ€æœ¯åˆ›æ–°:")
print(f"   â€¢ ä½¿ç”¨RFECVé€’å½’ç‰¹å¾æ¶ˆé™¤è¿›è¡Œç‰¹å¾é€‰æ‹©")
print(f"   â€¢ ä¸‰é˜¶æ®µæ¢¯åº¦æå‡æœºå‚æ•°ä¼˜åŒ–ï¼ˆç²—è°ƒâ†’ç»†è°ƒâ†’å¾®è°ƒï¼‰")
print(f"   â€¢ å¹³è¡¡å‡†ç¡®ç‡ä½œä¸ºä¼˜åŒ–ç›®æ ‡ï¼Œå¤„ç†ç±»åˆ«ä¸å¹³è¡¡")
print(f"   â€¢ éªŒè¯æ›²çº¿åˆ†æå…³é”®è¶…å‚æ•°å½±å“")

print(f"\nğŸ¯ RFECVç‰¹å¾é€‰æ‹©ç»“æœ:")
print(f"   â€¢ æœ€ä¼˜ç‰¹å¾æ•°: {rfecv.n_features_}")
print(f"   â€¢ æœ€ä½³äº¤å‰éªŒè¯åˆ†æ•°: {rfecv.grid_scores_[rfecv.n_features_ - 1]:.4f}")
print(f"   â€¢ ç‰¹å¾å‡å°‘ç‡: {(1 - len(selected_features)/len(feature_cols)):.1%}")

print(f"\nğŸš€ æ¢¯åº¦æå‡æœºä¼˜åŒ–å†ç¨‹:")
print(f"   â€¢ ç²—è°ƒé˜¶æ®µ: {coarse_search.best_score_:.4f}")
print(f"   â€¢ ç»†è°ƒé˜¶æ®µ: {fine_search.best_score_:.4f}")
print(f"   â€¢ æœ€ç»ˆå¾®è°ƒé˜¶æ®µ: {final_search.best_score_:.4f}")
print(f"   â€¢ æœ€ç»ˆæ¨¡å‹å‚æ•°: {final_params}")
print(f"   â€¢ æµ‹è¯•é›†å‡†ç¡®ç‡: {gb_accuracy:.4f}")
print(f"   â€¢ å¹³è¡¡å‡†ç¡®ç‡: {balanced_acc:.4f}")
print(f"   â€¢ åŠ æƒF1åˆ†æ•°: {f1_weighted:.4f}")
print(f"   â€¢ æ•´ä½“é¢„æµ‹å‡†ç¡®ç‡: {overall_accuracy:.4f}")