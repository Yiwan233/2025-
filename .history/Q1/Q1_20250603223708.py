# -*- coding: utf-8 -*-
"""
æ³•åŒ»æ··åˆSTRå›¾è°±è´¡çŒ®è€…äººæ•°ï¼ˆNoCï¼‰æ™ºèƒ½è¯†åˆ«æ¨¡å‹
åŸºäºæ–‡æ¡£æ€è·¯çš„å®Œæ•´å®ç° - Gradient Boostingç‰ˆæœ¬

å‚è€ƒæ–‡æ¡£ï¼šæ³•åŒ»æ··åˆSTRå›¾è°±è´¡çŒ®è€…äººæ•°ï¼ˆNoCï¼‰æ™ºèƒ½è¯†åˆ«æ¨¡å‹ç ”ç©¶
æ ¸å¿ƒæ€è·¯ï¼š
1. ç²¾ç»†åŒ–æ•°æ®é¢„å¤„ç†ä¸ä¿¡å·è¡¨å¾ï¼ˆç®€åŒ–Stutterå¤„ç†ï¼‰
2. åŸºäºæ–‡æ¡£V5æ–¹æ¡ˆçš„ç»¼åˆç‰¹å¾ä½“ç³»æ„å»º
3. LassoCVç‰¹å¾é€‰æ‹©
4. Gradient Boostingæ¨¡å‹è®­ç»ƒä¸é›†æˆ
5. SHAPå¯è§£é‡Šæ€§åˆ†æ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import warnings
import json
import re
from scipy import stats
from scipy.signal import find_peaks
from collections import Counter
from time import time

# æœºå™¨å­¦ä¹ ç›¸å…³
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, VotingClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LassoCV
from sklearn.feature_selection import SelectFromModel
plt.rcParams['font.sans-serif'] = ["Arial Unicode MS"]
plt.rcParams['axes.unicode_minus'] = False
warnings.filterwarnings('ignore')

# å¯è§£é‡Šæ€§åˆ†æ
try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    print("SHAPä¸å¯ç”¨ï¼Œå°†è·³è¿‡å¯è§£é‡Šæ€§åˆ†æ")
    SHAP_AVAILABLE = False

# é…ç½®
warnings.filterwarnings('ignore')
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 10

print("=== æ³•åŒ»æ··åˆSTRå›¾è°±NoCæ™ºèƒ½è¯†åˆ«æ¨¡å‹ ===")
print("åŸºäºGradient Boostingçš„å®Œæ•´å®ç°")

# =====================
# 1. æ–‡ä»¶è·¯å¾„ä¸åŸºç¡€è®¾ç½®
# =====================
DATA_DIR = './'
file_path = os.path.join(DATA_DIR, 'é™„ä»¶1ï¼šä¸åŒäººæ•°çš„STRå›¾è°±æ•°æ®.csv')
PLOTS_DIR = os.path.join(DATA_DIR, 'noc_analysis_plots')
if not os.path.exists(PLOTS_DIR):
    os.makedirs(PLOTS_DIR)

# å…³é”®å‚æ•°è®¾ç½®ï¼ˆåŸºäºæ–‡æ¡£ï¼‰
HEIGHT_THRESHOLD = 25  # åˆ†æè€ƒè™‘é˜ˆå€¼
SATURATION_THRESHOLD = 30000  # é¥±å’Œé˜ˆå€¼
CTA_THRESHOLD = 0.5  # çœŸå®ç­‰ä½åŸºå› ç½®ä¿¡åº¦é˜ˆå€¼
PHR_IMBALANCE_THRESHOLD = 0.6  # ä¸¥é‡ä¸å¹³è¡¡é˜ˆå€¼

# =====================
# 2. è¾…åŠ©å‡½æ•°å®šä¹‰
# =====================
def extract_noc_from_filename(filename):
    """ä»æ–‡ä»¶åæå–è´¡çŒ®è€…äººæ•°ï¼ˆNoCï¼‰"""
    match = re.search(r'-(\d+(?:_\d+)*)-[\d;]+-', str(filename))
    if match:
        ids = [id_val for id_val in match.group(1).split('_') if id_val.isdigit()]
        return len(ids) if len(ids) > 0 else np.nan
    return np.nan

def calculate_entropy(probabilities):
    """è®¡ç®—é¦™å†œç†µ"""
    probabilities = np.array(probabilities)
    probabilities = probabilities[probabilities > 0]
    if len(probabilities) == 0:
        return 0.0
    return -np.sum(probabilities * np.log(probabilities + 1e-10))

def calculate_ols_slope(x, y):
    """è®¡ç®—OLSå›å½’æ–œç‡"""
    if len(x) < 2 or len(x) != len(y):
        return 0.0
    
    x = np.array(x)
    y = np.array(y)
    
    if len(np.unique(x)) < 2:
        return 0.0
    
    try:
        slope, _, _, _, _ = stats.linregress(x, y)
        return slope
    except:
        return 0.0

# =====================
# 3. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
# =====================
print("\n=== æ­¥éª¤1: æ•°æ®åŠ è½½ä¸é¢„å¤„ç† ===")

# åŠ è½½æ•°æ®
try:
    df = pd.read_csv(file_path, encoding='utf-8')
    print(f"æˆåŠŸåŠ è½½æ•°æ®ï¼Œå½¢çŠ¶: {df.shape}")
except Exception as e:
    print(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
    exit()

# æå–NoCæ ‡ç­¾
df['NoC_True'] = df['Sample File'].apply(extract_noc_from_filename)
df = df.dropna(subset=['NoC_True'])
df['NoC_True'] = df['NoC_True'].astype(int)

print(f"NoCåˆ†å¸ƒ: {df['NoC_True'].value_counts().sort_index().to_dict()}")
print(f"æ ·æœ¬æ•°: {df['Sample File'].nunique()}")

# =====================
# 4. ç®€åŒ–çš„å³°å¤„ç†ä¸CTAè¯„ä¼°
# =====================
print("\n=== æ­¥éª¤2: å³°å¤„ç†ä¸ä¿¡å·è¡¨å¾ ===")

def process_peaks_with_cta(sample_data):
    """
    ç®€åŒ–çš„å³°å¤„ç†ï¼ŒåŒ…å«åŸºç¡€CTAè¯„ä¼°
    åŸºäºæ–‡æ¡£3.1å’Œ3.2èŠ‚çš„æ€è·¯ï¼Œä½†ç®€åŒ–Stutteræ¨¡å‹
    """
    processed_data = []
    
    for _, sample_row in sample_data.iterrows():
        sample_file = sample_row['Sample File']
        marker = sample_row['Marker']
        
        # æå–æ‰€æœ‰å³°
        peaks = []
        for i in range(1, 101):
            allele = sample_row.get(f'Allele {i}')
            size = sample_row.get(f'Size {i}')
            height = sample_row.get(f'Height {i}')
            
            if pd.notna(allele) and pd.notna(size) and pd.notna(height):
                original_height = float(height)
                # é¥±å’Œæ ¡æ­£
                corrected_height = min(original_height, SATURATION_THRESHOLD)
                
                # åˆ†æè€ƒè™‘é˜ˆå€¼è¿‡æ»¤
                if corrected_height >= HEIGHT_THRESHOLD:
                    peaks.append({
                        'allele': allele,
                        'size': float(size),
                        'height': corrected_height,
                        'original_height': original_height
                    })
        
        if not peaks:
            continue
            
        # ç®€åŒ–çš„CTAè¯„ä¼°
        # åŸºäºå³°é«˜ç›¸å¯¹å¤§å°è¿›è¡Œç®€å•çš„Stutterå¯èƒ½æ€§è¯„ä¼°
        peaks.sort(key=lambda x: x['height'], reverse=True)
        
        for peak in peaks:
            # ç®€åŒ–çš„CTAè®¡ç®—ï¼šåŸºäºå³°é«˜åœ¨ä½ç‚¹å†…çš„ç›¸å¯¹ä½ç½®
            height_rank = peaks.index(peak) + 1
            total_peaks = len(peaks)
            
            # ç®€å•çš„å¯å‘å¼CTAè¯„ä¼°
            if height_rank == 1:  # æœ€é«˜å³°
                cta = 0.95
            elif height_rank == 2 and total_peaks >= 2:  # ç¬¬äºŒé«˜å³°
                height_ratio = peak['height'] / peaks[0]['height']
                cta = 0.8 if height_ratio > 0.3 else 0.6
            else:  # å…¶ä»–å³°
                height_ratio = peak['height'] / peaks[0]['height']
                cta = max(0.1, min(0.8, height_ratio))
            
            # åº”ç”¨CTAé˜ˆå€¼è¿‡æ»¤
            if cta >= CTA_THRESHOLD:
                processed_data.append({
                    'Sample File': sample_file,
                    'Marker': marker,
                    'Allele': peak['allele'],
                    'Size': peak['size'],
                    'Height': peak['height'],
                    'Original_Height': peak['original_height'],
                    'CTA': cta
                })
    
    return pd.DataFrame(processed_data)

# å¤„ç†æ‰€æœ‰æ ·æœ¬
all_processed_peaks = []
for sample_file, group in df.groupby('Sample File'):
    sample_peaks = process_peaks_with_cta(group)
    all_processed_peaks.append(sample_peaks)

df_peaks = pd.concat(all_processed_peaks, ignore_index=True) if all_processed_peaks else pd.DataFrame()
print(f"å¤„ç†åçš„å³°æ•°æ®å½¢çŠ¶: {df_peaks.shape}")

# =====================
# 5. ç»¼åˆç‰¹å¾å·¥ç¨‹ï¼ˆåŸºäºæ–‡æ¡£V5æ–¹æ¡ˆï¼‰
# =====================
print("\n=== æ­¥éª¤3: ç»¼åˆç‰¹å¾å·¥ç¨‹ ===")

def extract_comprehensive_features_v5(sample_file, sample_peaks):
    """
    åŸºäºæ–‡æ¡£ç¬¬4èŠ‚çš„ç»¼åˆç‰¹å¾ä½“ç³»æ„å»º
    å®ç°Aã€Bã€Cã€Dç±»ç‰¹å¾
    """
    if sample_peaks.empty:
        return {}
    
    features = {'Sample File': sample_file}
    
    # åŸºç¡€æ•°æ®å‡†å¤‡
    total_peaks = len(sample_peaks)
    all_heights = sample_peaks['Height'].values
    all_sizes = sample_peaks['Size'].values
    
    # æŒ‰ä½ç‚¹åˆ†ç»„ç»Ÿè®¡
    locus_groups = sample_peaks.groupby('Marker')
    alleles_per_locus = locus_groups['Allele'].nunique()
    locus_heights = locus_groups['Height'].sum()
    
    # é¢„æœŸå¸¸æŸ“è‰²ä½“ä½ç‚¹æ•°ï¼ˆå‡è®¾å€¼ï¼Œå®é™…åº”ä»MARKER_PARAMSè·å–ï¼‰
    expected_autosomal_count = 23  # å¯æ ¹æ®å®é™…è¯•å‰‚ç›’è°ƒæ•´
    
    # ===============================
    # Aç±»ï¼šè°±å›¾å±‚é¢åŸºç¡€ç»Ÿè®¡ç‰¹å¾
    # ===============================
    
    # A.1 MACP - æ ·æœ¬æœ€å¤§ç­‰ä½åŸºå› æ•°
    features['mac_profile'] = alleles_per_locus.max() if len(alleles_per_locus) > 0 else 0
    
    # A.2 TDA - æ ·æœ¬æ€»ç‰¹å¼‚ç­‰ä½åŸºå› æ•°
    features['total_distinct_alleles'] = sample_peaks['Allele'].nunique()
    
    # A.3 AAP - æ¯ä½ç‚¹å¹³å‡ç­‰ä½åŸºå› æ•°
    if expected_autosomal_count > 0:
        # åŒ…å«ç¼ºå¤±ä½ç‚¹ï¼ˆè®¡ä¸º0ï¼‰
        all_locus_counts = np.zeros(expected_autosomal_count)
        all_locus_counts[:len(alleles_per_locus)] = alleles_per_locus.values
        features['avg_alleles_per_locus'] = np.mean(all_locus_counts)
        features['std_alleles_per_locus'] = np.std(all_locus_counts)
    else:
        features['avg_alleles_per_locus'] = alleles_per_locus.mean() if len(alleles_per_locus) > 0 else 0
        features['std_alleles_per_locus'] = alleles_per_locus.std() if len(alleles_per_locus) > 1 else 0
    
    # A.5 MGTNç³»åˆ— - ç­‰ä½åŸºå› æ•°â‰¥Nçš„ä½ç‚¹æ•°
    for N in [2, 3, 4, 5, 6]:
        features[f'loci_gt{N}_alleles'] = (alleles_per_locus >= N).sum()
    
    # A.6 ç­‰ä½åŸºå› è®¡æ•°çš„ç†µ
    if len(alleles_per_locus) > 0:
        counts = alleles_per_locus.value_counts(normalize=True)
        features['allele_count_dist_entropy'] = calculate_entropy(counts.values)
    else:
        features['allele_count_dist_entropy'] = 0
    
    # ===============================
    # Bç±»ï¼šå³°é«˜ã€å¹³è¡¡æ€§åŠéšæœºæ•ˆåº”ç‰¹å¾
    # ===============================
    
    if total_peaks > 0:
        # B.1 åŸºç¡€å³°é«˜ç»Ÿè®¡
        features['avg_peak_height'] = np.mean(all_heights)
        features['std_peak_height'] = np.std(all_heights) if total_peaks > 1 else 0
        features['min_peak_height'] = np.min(all_heights)
        features['max_peak_height'] = np.max(all_heights)
        
        # B.2 å³°é«˜æ¯”(PHR)ç›¸å…³ç»Ÿè®¡
        phr_values = []
        for marker, marker_group in locus_groups:
            if len(marker_group) == 2:  # æ°å¥½ä¸¤ä¸ªç­‰ä½åŸºå› çš„ä½ç‚¹
                heights = marker_group['Height'].values
                phr = min(heights) / max(heights) if max(heights) > 0 else 0
                phr_values.append(phr)
        
        if phr_values:
            features['avg_phr'] = np.mean(phr_values)
            features['std_phr'] = np.std(phr_values) if len(phr_values) > 1 else 0
            features['min_phr'] = np.min(phr_values)
            features['median_phr'] = np.median(phr_values)
            features['num_loci_with_phr'] = len(phr_values)
            features['num_severe_imbalance_loci'] = sum(phr <= PHR_IMBALANCE_THRESHOLD for phr in phr_values)
            features['ratio_severe_imbalance_loci'] = features['num_severe_imbalance_loci'] / len(phr_values)
        else:
            for key in ['avg_phr', 'std_phr', 'min_phr', 'median_phr', 'num_loci_with_phr', 
                       'num_severe_imbalance_loci', 'ratio_severe_imbalance_loci']:
                features[key] = 0
        
        # B.3 å³°é«˜åˆ†å¸ƒç»Ÿè®¡çŸ©
        if total_peaks > 2:
            features['skewness_peak_height'] = stats.skew(all_heights)
            features['kurtosis_peak_height'] = stats.kurtosis(all_heights, fisher=False)
        else:
            features['skewness_peak_height'] = 0
            features['kurtosis_peak_height'] = 0
        
        # B.3+ å³°é«˜å¤šå³°æ€§
        try:
            log_heights = np.log(all_heights + 1)
            if len(np.unique(log_heights)) > 1:
                hist, _ = np.histogram(log_heights, bins=min(10, total_peaks))
                peaks_found, _ = find_peaks(hist)
                features['modality_peak_height'] = len(peaks_found)
            else:
                features['modality_peak_height'] = 1
        except:
            features['modality_peak_height'] = 1
        
        # B.4 é¥±å’Œæ•ˆåº”
        saturated_peaks = (sample_peaks['Original_Height'] >= SATURATION_THRESHOLD).sum()
        features['num_saturated_peaks'] = saturated_peaks
        features['ratio_saturated_peaks'] = saturated_peaks / total_peaks
    else:
        # ç©ºå€¼å¡«å……
        for key in ['avg_peak_height', 'std_peak_height', 'min_peak_height', 'max_peak_height',
                   'avg_phr', 'std_phr', 'min_phr', 'median_phr', 'num_loci_with_phr',
                   'num_severe_imbalance_loci', 'ratio_severe_imbalance_loci',
                   'skewness_peak_height', 'kurtosis_peak_height', 'modality_peak_height',
                   'num_saturated_peaks', 'ratio_saturated_peaks']:
            features[key] = 0
    
    # ===============================
    # Cç±»ï¼šä¿¡æ¯è®ºåŠè°±å›¾å¤æ‚åº¦ç‰¹å¾
    # ===============================
    
    # C.1 ä½ç‚¹é—´å¹³è¡¡æ€§çš„é¦™å†œç†µ
    if len(locus_heights) > 0:
        total_height = locus_heights.sum()
        if total_height > 0:
            locus_probs = locus_heights / total_height
            features['inter_locus_balance_entropy'] = calculate_entropy(locus_probs.values)
        else:
            features['inter_locus_balance_entropy'] = 0
    else:
        features['inter_locus_balance_entropy'] = 0
    
    # C.2 å¹³å‡ä½ç‚¹ç­‰ä½åŸºå› åˆ†å¸ƒç†µ
    locus_entropies = []
    for marker, marker_group in locus_groups:
        if len(marker_group) > 1:
            heights = marker_group['Height'].values
            height_sum = heights.sum()
            if height_sum > 0:
                probs = heights / height_sum
                entropy = calculate_entropy(probs)
                locus_entropies.append(entropy)
    
    features['avg_locus_allele_entropy'] = np.mean(locus_entropies) if locus_entropies else 0
    
    # C.3 æ ·æœ¬æ•´ä½“å³°é«˜åˆ†å¸ƒç†µ
    if total_peaks > 0:
        log_heights = np.log(all_heights + 1)
        hist, _ = np.histogram(log_heights, bins=min(15, total_peaks))
        hist_probs = hist / hist.sum()
        hist_probs = hist_probs[hist_probs > 0]
        features['peak_height_entropy'] = calculate_entropy(hist_probs)
    else:
        features['peak_height_entropy'] = 0
    
    # C.4 å›¾è°±å®Œæ•´æ€§æŒ‡æ ‡
    effective_loci_count = len(locus_groups)
    features['num_loci_with_effective_alleles'] = effective_loci_count
    features['num_loci_no_effective_alleles'] = max(0, expected_autosomal_count - effective_loci_count)
    
    # ===============================
    # Dç±»ï¼šDNAé™è§£ä¸ä¿¡æ¯ä¸¢å¤±ç‰¹å¾
    # ===============================
    
    if total_peaks > 1:
        # D.1 å³°é«˜ä¸ç‰‡æ®µå¤§å°çš„ç›¸å…³æ€§
        if len(np.unique(all_heights)) > 1 and len(np.unique(all_sizes)) > 1:
            features['height_size_correlation'] = np.corrcoef(all_heights, all_sizes)[0, 1]
        else:
            features['height_size_correlation'] = 0
        
        # D.2 å³°é«˜ä¸ç‰‡æ®µå¤§å°çš„çº¿æ€§å›å½’æ–œç‡
        features['height_size_slope'] = calculate_ols_slope(all_sizes, all_heights)
        
        # D.3 åŠ æƒå›å½’æ–œç‡ï¼ˆæƒé‡ä¸ºå³°é«˜ï¼‰
        try:
            # ç®€åŒ–ç‰ˆåŠ æƒå›å½’
            weights = all_heights / all_heights.sum()
            weighted_correlation = np.average(all_sizes, weights=weights)
            features['weighted_height_size_slope'] = calculate_ols_slope(all_sizes, all_heights)
        except:
            features['weighted_height_size_slope'] = 0
        
        # D.4 PHRéšç‰‡æ®µå¤§å°å˜åŒ–çš„æ–œç‡
        if len(phr_values) > 1:
            phr_sizes = []
            for marker, marker_group in locus_groups:
                if len(marker_group) == 2:
                    avg_size = marker_group['Size'].mean()
                    phr_sizes.append(avg_size)
            
            if len(phr_sizes) == len(phr_values) and len(phr_sizes) > 1:
                features['phr_size_slope'] = calculate_ols_slope(phr_sizes, phr_values)
            else:
                features['phr_size_slope'] = 0
        else:
            features['phr_size_slope'] = 0
        
    else:
        for key in ['height_size_correlation', 'height_size_slope', 'weighted_height_size_slope', 'phr_size_slope']:
            features[key] = 0
    
    # D.5 ä½ç‚¹ä¸¢å¤±è¯„åˆ†ï¼ˆç®€åŒ–ç‰ˆï¼‰
    dropout_score = features['num_loci_no_effective_alleles'] / expected_autosomal_count if expected_autosomal_count > 0 else 0
    features['locus_dropout_score_weighted_by_size'] = dropout_score
    
    # D.6 RFUæ¯ç¢±åŸºå¯¹è¡°å‡æŒ‡æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰
    if len(locus_groups) > 1:
        locus_max_heights = []
        locus_avg_sizes = []
        for marker, marker_group in locus_groups:
            max_height = marker_group['Height'].max()
            avg_size = marker_group['Size'].mean()
            locus_max_heights.append(max_height)
            locus_avg_sizes.append(avg_size)
        
        features['degradation_index_rfu_per_bp'] = calculate_ols_slope(locus_avg_sizes, locus_max_heights)
    else:
        features['degradation_index_rfu_per_bp'] = 0
    
    # D.7 å°ç‰‡æ®µä¸å¤§ç‰‡æ®µä¿¡æ¯å®Œæ•´åº¦æ¯”ç‡ï¼ˆç®€åŒ–ç‰ˆï¼‰
    # å‡è®¾ç‰‡æ®µå¤§å°<200bpä¸ºå°ç‰‡æ®µï¼Œ>=200bpä¸ºå¤§ç‰‡æ®µ
    small_fragment_loci = 0
    large_fragment_loci = 0
    small_fragment_effective = 0
    large_fragment_effective = 0
    
    for marker, marker_group in locus_groups:
        avg_size = marker_group['Size'].mean()
        if avg_size < 200:
            small_fragment_loci += 1
            small_fragment_effective += 1
        else:
            large_fragment_loci += 1
            large_fragment_effective += 1
    
    # ä¼°ç®—æ€»çš„å°/å¤§ç‰‡æ®µä½ç‚¹æ•°
    total_small_expected = expected_autosomal_count // 2  # å‡è®¾ä¸€åŠæ˜¯å°ç‰‡æ®µ
    total_large_expected = expected_autosomal_count - total_small_expected
    
    small_completeness = small_fragment_effective / total_small_expected if total_small_expected > 0 else 0
    large_completeness = large_fragment_effective / total_large_expected if total_large_expected > 0 else 0
    
    if large_completeness > 0:
        features['info_completeness_ratio_small_large'] = small_completeness / large_completeness
    else:
        features['info_completeness_ratio_small_large'] = small_completeness / 0.001  # é¿å…é™¤é›¶
    
    return features

# æå–æ‰€æœ‰æ ·æœ¬çš„ç‰¹å¾
print("å¼€å§‹ç‰¹å¾æå–...")
start_time = time()

all_features = []
for sample_file, group in df_peaks.groupby('Sample File'):
    features = extract_comprehensive_features_v5(sample_file, group)
    all_features.append(features)

df_features = pd.DataFrame(all_features)

# åˆå¹¶NoCæ ‡ç­¾
noc_map = df.groupby('Sample File')['NoC_True'].first().to_dict()
df_features['NoC_True'] = df_features['Sample File'].map(noc_map)
df_features = df_features.dropna(subset=['NoC_True'])

print(f"ç‰¹å¾æå–å®Œæˆï¼Œè€—æ—¶: {time() - start_time:.2f}ç§’")
print(f"ç‰¹å¾æ•°æ®å½¢çŠ¶: {df_features.shape}")
print(f"ç‰¹å¾æ•°é‡: {len([col for col in df_features.columns if col not in ['Sample File', 'NoC_True']])}")

# =====================
# 6. ç‰¹å¾é€‰æ‹©ï¼ˆLassoCVï¼‰
# =====================
print("\n=== æ­¥éª¤4: ç‰¹å¾é€‰æ‹© ===")

# å‡†å¤‡æ•°æ®
feature_cols = [col for col in df_features.columns if col not in ['Sample File', 'NoC_True']]
X = df_features[feature_cols].fillna(0)
y = df_features['NoC_True']

print(f"åŸå§‹ç‰¹å¾æ•°: {len(feature_cols)}")
print(f"æ ·æœ¬æ•°: {len(X)}")
print(f"NoCåˆ†å¸ƒ: {y.value_counts().sort_index().to_dict()}")

# æ ‡ç­¾ç¼–ç ï¼ˆä¸ºäº†å…¼å®¹æ€§ï¼‰
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# ç‰¹å¾æ ‡å‡†åŒ–
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_scaled_df = pd.DataFrame(X_scaled, columns=feature_cols)

# LassoCVç‰¹å¾é€‰æ‹©
print("ä½¿ç”¨LassoCVè¿›è¡Œç‰¹å¾é€‰æ‹©...")
lasso_cv = LassoCV(cv=5, random_state=42, max_iter=2000)
lasso_cv.fit(X_scaled, y_encoded)

# é€‰æ‹©éé›¶ç³»æ•°çš„ç‰¹å¾
selector = SelectFromModel(lasso_cv, prefit=True)
X_selected = selector.transform(X_scaled)
selected_features = [feature_cols[i] for i in range(len(feature_cols)) if selector.get_support()[i]]

print(f"LassoCVé€‰æ‹©çš„ç‰¹å¾æ•°: {len(selected_features)}")
print("é€‰æ‹©çš„ç‰¹å¾:")
for i, feature in enumerate(selected_features, 1):
    coef = lasso_cv.coef_[feature_cols.index(feature)]
    print(f"  {i:2d}. {feature:35} (ç³»æ•°: {coef:8.4f})")

# å¦‚æœé€‰æ‹©çš„ç‰¹å¾å¤ªå°‘ï¼Œä¿ç•™é‡è¦ç‰¹å¾
if len(selected_features) < 5:
    print("è­¦å‘Š: LassoCVé€‰æ‹©çš„ç‰¹å¾å¤ªå°‘ï¼Œä½¿ç”¨åŸºäºé‡è¦æ€§çš„å¤‡é€‰æ–¹æ¡ˆ...")
    # åŸºäºç»å¯¹ç³»æ•°å€¼é€‰æ‹©å‰15ä¸ªç‰¹å¾
    feature_importance = [(i, abs(coef)) for i, coef in enumerate(lasso_cv.coef_)]
    feature_importance.sort(key=lambda x: x[1], reverse=True)
    selected_indices = [x[0] for x in feature_importance[:15]]
    selected_features = [feature_cols[i] for i in selected_indices]
    X_selected = X_scaled[:, selected_indices]
    print(f"å¤‡é€‰æ–¹æ¡ˆé€‰æ‹©äº† {len(selected_features)} ä¸ªç‰¹å¾")

# =====================
# 7. æ¨¡å‹è®­ç»ƒä¸éªŒè¯
# =====================
print("\n=== æ­¥éª¤5: æ¨¡å‹è®­ç»ƒä¸éªŒè¯ ===")

# ä½¿ç”¨é€‰æ‹©çš„ç‰¹å¾
X_final = pd.DataFrame(X_selected, columns=selected_features)

# åˆ’åˆ†æ•°æ®é›†
X_train, X_test, y_train, y_test = train_test_split(
    X_final, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded
)

# äº¤å‰éªŒè¯è®¾ç½®
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# ===== Gradient Boosting ä¸»æ¨¡å‹ =====
print("\nè®­ç»ƒGradient Boostingæ¨¡å‹...")

# è¶…å‚æ•°ç½‘æ ¼
gb_param_grid = {
    'n_estimators': [50, 100, 150, 200, 300],
    'max_depth': [3,4, 5, 7, 9],
    'learning_rate': [0.01, 0.001, 0.0005,0.00001],
    'subsample': [0.8, 0.9, 1.0],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# ç½‘æ ¼æœç´¢
gb_grid_search = GridSearchCV(
    GradientBoostingClassifier(random_state=42),
    gb_param_grid,
    cv=cv,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

print("æ‰§è¡Œç½‘æ ¼æœç´¢...")
gb_grid_search.fit(X_train, y_train)

best_gb_model = gb_grid_search.best_estimator_
print(f"æœ€ä½³å‚æ•°: {gb_grid_search.best_params_}")
print(f"æœ€ä½³CVåˆ†æ•°: {gb_grid_search.best_score_:.4f}")

# è¯„ä¼°æ¨¡å‹
y_pred_gb = best_gb_model.predict(X_test)
gb_accuracy = accuracy_score(y_test, y_pred_gb)
print(f"Gradient Boostingæµ‹è¯•å‡†ç¡®ç‡: {gb_accuracy:.4f}")

# ===== å¯¹æ¯”æ¨¡å‹ =====
print("\nè®­ç»ƒå¯¹æ¯”æ¨¡å‹...")

# Random Forest
rf_model = RandomForestClassifier(n_estimators=200, random_state=42, class_weight='balanced')
rf_scores = cross_val_score(rf_model, X_final, y_encoded, cv=cv, scoring='accuracy')
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)
rf_accuracy = accuracy_score(y_test, y_pred_rf)

print(f"Random Forest CV: {rf_scores.mean():.4f}Â±{rf_scores.std():.4f}, Test: {rf_accuracy:.4f}")

# ===== é›†æˆæ¨¡å‹ =====
print("\næ„å»ºé›†æˆæ¨¡å‹...")

ensemble_model = VotingClassifier(
    estimators=[
        ('gb', best_gb_model),
        ('rf', rf_model)
    ],
    voting='soft',
    weights=[2, 1]  # ç»™GBæ›´é«˜æƒé‡
)

ensemble_model.fit(X_train, y_train)
y_pred_ensemble = ensemble_model.predict(X_test)
ensemble_accuracy = accuracy_score(y_test, y_pred_ensemble)

print(f"é›†æˆæ¨¡å‹æµ‹è¯•å‡†ç¡®ç‡: {ensemble_accuracy:.4f}")

# é€‰æ‹©æœ€ä½³æ¨¡å‹
models = {
    'Gradient Boosting': (best_gb_model, gb_accuracy, y_pred_gb),
    'Random Forest': (rf_model, rf_accuracy, y_pred_rf),
    'Ensemble': (ensemble_model, ensemble_accuracy, y_pred_ensemble)
}

best_model_name = max(models.keys(), key=lambda x: models[x][1])
best_model, best_accuracy, best_predictions = models[best_model_name]

print(f"\næœ€ä½³æ¨¡å‹: {best_model_name} (å‡†ç¡®ç‡: {best_accuracy:.4f})")

# =====================
# 8. è¯¦ç»†è¯„ä¼°ä¸å¯è§†åŒ–
# =====================
print("\n=== æ­¥éª¤6: è¯¦ç»†è¯„ä¼°ä¸å¯è§†åŒ– ===")

# è½¬æ¢æ ‡ç­¾ç”¨äºæ˜¾ç¤º
y_test_orig = label_encoder.inverse_transform(y_test)
best_predictions_orig = label_encoder.inverse_transform(best_predictions)

# åˆ†ç±»æŠ¥å‘Š
class_names = [str(x) for x in sorted(label_encoder.classes_)]
print(f"\n{best_model_name} è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test_orig, best_predictions_orig, target_names=[f"{x}äºº" for x in class_names]))

# æ··æ·†çŸ©é˜µå¯è§†åŒ–
plt.figure(figsize=(10, 8))
cm = confusion_matrix(y_test_orig, best_predictions_orig)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
           xticklabels=[f"{x}äºº" for x in class_names], 
           yticklabels=[f"{x}äºº" for x in class_names])
plt.title(f'{best_model_name} æ··æ·†çŸ©é˜µ')
plt.ylabel('çœŸå®NoC')
plt.xlabel('é¢„æµ‹NoC')
plt.tight_layout()
plt.savefig(os.path.join(PLOTS_DIR, 'confusion_matrix.png'), dpi=300)
plt.close()

# æ¨¡å‹æ€§èƒ½å¯¹æ¯”
plt.figure(figsize=(12, 6))
model_names = list(models.keys())
accuracies = [models[name][1] for name in model_names]

colors = ['#d62728' if name == best_model_name else '#1f77b4' for name in model_names]
bars = plt.bar(model_names, accuracies, color=colors)
plt.ylim(0, 1.1)
plt.ylabel('æµ‹è¯•å‡†ç¡®ç‡')
plt.title('æ¨¡å‹æ€§èƒ½å¯¹æ¯”')
plt.grid(axis='y', alpha=0.3)

for bar, acc in zip(bars, accuracies):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, 
             f'{acc:.3f}', ha='center', va='bottom')

plt.tight_layout()
plt.savefig(os.path.join(PLOTS_DIR, 'model_comparison.png'), dpi=300)
plt.close()

# ç‰¹å¾é‡è¦æ€§åˆ†æ
if hasattr(best_model, 'feature_importances_'):
    plt.figure(figsize=(12, 8))
    
    feature_importance = pd.DataFrame({
        'feature': selected_features,
        'importance': best_model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    # æ˜¾ç¤ºå‰15ä¸ªé‡è¦ç‰¹å¾
    top_features = feature_importance.head(15)
    sns.barplot(data=top_features, x='importance', y='feature')
    plt.title(f'{best_model_name} ç‰¹å¾é‡è¦æ€§ (å‰15)')
    plt.xlabel('ç‰¹å¾é‡è¦æ€§')
    plt.tight_layout()
    plt.savefig(os.path.join(PLOTS_DIR, 'feature_importance.png'), dpi=300)
    plt.close()
    
    print(f"\n{best_model_name} Top 10 é‡è¦ç‰¹å¾:")
    for idx, row in feature_importance.head(10).iterrows():
        print(f"  {row['feature']:35} {row['importance']:.4f}")

# å­¦ä¹ æ›²çº¿
from sklearn.model_selection import learning_curve

try:
    train_sizes, train_scores, val_scores = learning_curve(
        best_model, X_final, y_encoded, cv=cv, 
        train_sizes=np.linspace(0.1, 1.0, 10),
        scoring='accuracy', random_state=42
    )

    plt.figure(figsize=(10, 6))
    train_mean = np.mean(train_scores, axis=1)
    train_std = np.std(train_scores, axis=1)
    val_mean = np.mean(val_scores, axis=1)
    val_std = np.std(val_scores, axis=1)

    plt.plot(train_sizes, train_mean, 'o-', color='r', label='è®­ç»ƒé›†')
    plt.plot(train_sizes, val_mean, 'o-', color='g', label='éªŒè¯é›†')
    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='r')
    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='g')

    plt.xlabel('è®­ç»ƒæ ·æœ¬æ•°')
    plt.ylabel('å‡†ç¡®ç‡')
    plt.title(f'{best_model_name} å­¦ä¹ æ›²çº¿')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(os.path.join(PLOTS_DIR, 'learning_curve.png'), dpi=300)
    plt.close()
except Exception as e:
    print(f"å­¦ä¹ æ›²çº¿ç”Ÿæˆå¤±è´¥: {e}")

# =====================
# 9. SHAPå¯è§£é‡Šæ€§åˆ†æ
# =====================
if SHAP_AVAILABLE and hasattr(best_model, 'feature_importances_'):
    print("\n=== æ­¥éª¤7: SHAPå¯è§£é‡Šæ€§åˆ†æ ===")
    
    try:
        # åˆ›å»ºSHAPè§£é‡Šå™¨
        explainer = shap.TreeExplainer(best_model)
        
        # è®¡ç®—SHAPå€¼ï¼ˆä½¿ç”¨å°æ ·æœ¬ï¼‰
        shap_sample_size = min(20, len(X_test))
        X_shap = X_test.iloc[:shap_sample_size]
        shap_values = explainer.shap_values(X_shap)
        
        # å¯¹äºå¤šåˆ†ç±»ï¼Œå–ç¬¬ä¸€ä¸ªç±»åˆ«çš„SHAPå€¼æˆ–è®¡ç®—å¹³å‡
        if isinstance(shap_values, list):
            shap_values_mean = np.mean([np.abs(sv) for sv in shap_values], axis=0)
        else:
            shap_values_mean = np.abs(shap_values)
        
        # SHAPç‰¹å¾é‡è¦æ€§
        feature_shap_importance = np.mean(shap_values_mean, axis=0)
        shap_importance_df = pd.DataFrame({
            'feature': selected_features,
            'shap_importance': feature_shap_importance
        }).sort_values('shap_importance', ascending=False)
        
        plt.figure(figsize=(12, 8))
        top_shap_features = shap_importance_df.head(10)
        sns.barplot(data=top_shap_features, x='shap_importance', y='feature')
        plt.title('SHAPç‰¹å¾é‡è¦æ€§ (å‰10)')
        plt.xlabel('å¹³å‡SHAPé‡è¦æ€§')
        plt.tight_layout()
        plt.savefig(os.path.join(PLOTS_DIR, 'shap_importance.png'), dpi=300)
        plt.close()
        
        print("SHAP Top 10 é‡è¦ç‰¹å¾:")
        for idx, row in shap_importance_df.head(10).iterrows():
            print(f"  {row['feature']:35} {row['shap_importance']:.4f}")
        
        # å°è¯•ç”ŸæˆSHAPæ‘˜è¦å›¾
        try:
            if isinstance(shap_values, list):
                # å¤šåˆ†ç±»æƒ…å†µï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªç±»åˆ«
                shap_values_plot = shap_values[0]
            else:
                shap_values_plot = shap_values
                
            plt.figure(figsize=(10, 8))
            shap.summary_plot(shap_values_plot, X_shap, feature_names=selected_features, 
                            plot_type="bar", show=False)
            plt.title("SHAPæ‘˜è¦å›¾")
            plt.tight_layout()
            plt.savefig(os.path.join(PLOTS_DIR, 'shap_summary.png'), dpi=300, bbox_inches='tight')
            plt.close()
            
        except Exception as e:
            print(f"ç”ŸæˆSHAPæ‘˜è¦å›¾å¤±è´¥: {e}")
            
    except Exception as e:
        print(f"SHAPåˆ†æå¤±è´¥: {e}")

# =====================
# 10. åº”ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹
# =====================
print("\n=== æ­¥éª¤8: åº”ç”¨æ¨¡å‹è¿›è¡Œå…¨æ ·æœ¬é¢„æµ‹ ===")

# å¯¹æ‰€æœ‰æ ·æœ¬è¿›è¡Œé¢„æµ‹
y_pred_all_encoded = best_model.predict(X_final)
y_pred_all = label_encoder.inverse_transform(y_pred_all_encoded)

# æ·»åŠ é¢„æµ‹ç»“æœåˆ°ç‰¹å¾æ•°æ®æ¡†
df_features['Predicted_NoC'] = y_pred_all

# è®¡ç®—æ•´ä½“å‡†ç¡®ç‡
overall_accuracy = (df_features['Predicted_NoC'] == df_features['NoC_True']).mean()
print(f"æ•´ä½“é¢„æµ‹å‡†ç¡®ç‡: {overall_accuracy:.4f}")

# å„NoCç±»åˆ«çš„å‡†ç¡®ç‡
noc_accuracy = df_features.groupby('NoC_True').apply(
    lambda x: (x['Predicted_NoC'] == x['NoC_True']).mean()
).reset_index(name='Accuracy')

print("\nå„NoCç±»åˆ«é¢„æµ‹å‡†ç¡®ç‡:")
for _, row in noc_accuracy.iterrows():
    print(f"  {int(row['NoC_True'])}äºº: {row['Accuracy']:.4f}")

# NoCé¢„æµ‹å‡†ç¡®ç‡å¯è§†åŒ–
plt.figure(figsize=(10, 6))
sns.barplot(data=noc_accuracy, x='NoC_True', y='Accuracy')
plt.ylim(0, 1.1)
plt.xlabel('çœŸå®NoC')
plt.ylabel('é¢„æµ‹å‡†ç¡®ç‡')
plt.title('å„NoCç±»åˆ«é¢„æµ‹å‡†ç¡®ç‡')

for i, row in noc_accuracy.iterrows():
    plt.text(i, row['Accuracy'] + 0.03, f"{row['Accuracy']:.3f}", 
             ha='center', va='bottom')

plt.tight_layout()
plt.savefig(os.path.join(PLOTS_DIR, 'noc_accuracy_by_class.png'), dpi=300)
plt.close()

# =====================
# 11. ä¿å­˜ç»“æœ
# =====================
print("\n=== æ­¥éª¤9: ä¿å­˜ç»“æœ ===")

# ä¿å­˜ç‰¹å¾æ•°æ®
df_features.to_csv(os.path.join(DATA_DIR, 'noc_features_with_predictions.csv'), 
                   index=False, encoding='utf-8-sig')

# ä¿å­˜æ¨¡å‹æ€§èƒ½æ‘˜è¦
summary = {
    'model_info': {
        'best_model': best_model_name,
        'best_accuracy': float(best_accuracy),
        'overall_accuracy': float(overall_accuracy),
        'feature_selection_method': 'LassoCV',
        'selected_features_count': len(selected_features)
    },
    'data_info': {
        'total_samples': len(df_features),
        'feature_count_original': len(feature_cols),
        'feature_count_selected': len(selected_features),
        'noc_distribution': df_features['NoC_True'].value_counts().sort_index().to_dict()
    },
    'model_performance': {
        name: {'accuracy': float(acc)} for name, (_, acc, _) in models.items()
    },
    'noc_class_accuracy': {
        int(row['NoC_True']): float(row['Accuracy']) 
        for _, row in noc_accuracy.iterrows()
    },
    'selected_features': selected_features,
    'hyperparameters': gb_grid_search.best_params_ if best_model_name == 'Gradient Boosting' else {},
    'timestamp': pd.Timestamp.now().strftime("%Y-%m-%d %H:%M:%S")
}

with open(os.path.join(DATA_DIR, 'noc_analysis_summary.json'), 'w', encoding='utf-8') as f:
    json.dump(summary, f, ensure_ascii=False, indent=2)

# ä¿å­˜æœ€ä½³æ¨¡å‹
import joblib
try:
    joblib.dump(best_model, os.path.join(DATA_DIR, f'best_noc_model_{best_model_name.lower().replace(" ", "_")}.pkl'))
    joblib.dump(scaler, os.path.join(DATA_DIR, 'feature_scaler.pkl'))
    joblib.dump(label_encoder, os.path.join(DATA_DIR, 'label_encoder.pkl'))
    print("æ¨¡å‹å’Œé¢„å¤„ç†å™¨å·²ä¿å­˜")
except Exception as e:
    print(f"ä¿å­˜æ¨¡å‹æ—¶å‡ºé”™: {e}")

# =====================
# 12. æœ€ç»ˆæŠ¥å‘Š
# =====================
print("\n" + "="*60)
print("           æ³•åŒ»æ··åˆSTRå›¾è°±NoCè¯†åˆ« - æœ€ç»ˆæŠ¥å‘Š")
print("="*60)

print(f"\nğŸ“Š æ•°æ®æ¦‚å†µ:")
print(f"   â€¢ æ€»æ ·æœ¬æ•°: {len(df_features)}")
print(f"   â€¢ NoCåˆ†å¸ƒ: {dict(df_features['NoC_True'].value_counts().sort_index())}")
print(f"   â€¢ åŸå§‹ç‰¹å¾æ•°: {len(feature_cols)}")
print(f"   â€¢ é€‰æ‹©ç‰¹å¾æ•°: {len(selected_features)}")

print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model_name}")
print(f"   â€¢ æµ‹è¯•é›†å‡†ç¡®ç‡: {best_accuracy:.4f}")
print(f"   â€¢ æ•´ä½“å‡†ç¡®ç‡: {overall_accuracy:.4f}")

if best_model_name == 'Gradient Boosting':
    print(f"   â€¢ æœ€ä½³è¶…å‚æ•°: {gb_grid_search.best_params_}")

print(f"\nğŸ“ˆ å„ç±»åˆ«è¡¨ç°:")
for _, row in noc_accuracy.iterrows():
    noc = int(row['NoC_True'])
    acc = row['Accuracy']
    print(f"   â€¢ {noc}äººæ··åˆæ ·æœ¬: {acc:.4f}")

print(f"\nğŸ” Top 5 é‡è¦ç‰¹å¾:")
if hasattr(best_model, 'feature_importances_'):
    top_5_features = feature_importance.head(5)
    for i, (_, row) in enumerate(top_5_features.iterrows(), 1):
        print(f"   {i}. {row['feature']:30} ({row['importance']:.4f})")

print(f"\nğŸ’¾ ä¿å­˜çš„æ–‡ä»¶:")
print(f"   â€¢ ç‰¹å¾æ•°æ®: noc_features_with_predictions.csv")
print(f"   â€¢ åˆ†ææ‘˜è¦: noc_analysis_summary.json")
print(f"   â€¢ æœ€ä½³æ¨¡å‹: best_noc_model_{best_model_name.lower().replace(' ', '_')}.pkl")
print(f"   â€¢ å›¾è¡¨ç›®å½•: {PLOTS_DIR}")

print(f"\nğŸ“‹ æ¨¡å‹è§£é‡Š:")
print(f"   â€¢ æœ¬æ¨¡å‹åŸºäº{len(selected_features)}ä¸ªç²¾é€‰ç‰¹å¾")
print(f"   â€¢ ç‰¹å¾æ¶µç›–å›¾è°±ç»Ÿè®¡ã€å³°é«˜åˆ†å¸ƒã€å¹³è¡¡æ€§ã€ä¿¡æ¯ç†µã€é™è§£æŒ‡æ ‡ç­‰")
print(f"   â€¢ ä½¿ç”¨{best_model_name}ç®—æ³•å®ç°NoCè‡ªåŠ¨è¯†åˆ«")
print(f"   â€¢ æ•´ä½“å‡†ç¡®ç‡è¾¾åˆ°{overall_accuracy:.1%}ï¼Œå…·æœ‰è‰¯å¥½çš„å®ç”¨ä»·å€¼")

if SHAP_AVAILABLE:
    print(f"   â€¢ å·²ç”ŸæˆSHAPå¯è§£é‡Šæ€§åˆ†æï¼Œå¢å¼ºæ¨¡å‹é€æ˜åº¦")

print("\nâœ… åˆ†æå®Œæˆï¼")
print("="*60)