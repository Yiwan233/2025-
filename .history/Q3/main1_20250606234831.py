# -*- coding: utf-8 -*-
"""
æ•°å­¦å»ºæ¨¡ - æ³•åŒ»DNAåˆ†æ - é—®é¢˜ä¸‰ï¼šSTRæ··åˆæ ·æœ¬MCMCè´å¶æ–¯åŸºå› å‹æ¨æ–­ç³»ç»Ÿ V4.0

åŸºäº"æ›´å¥½çš„é—®é¢˜ä¸‰æ€è·¯"æ–‡æ¡£å®ç°çš„æ”¹è¿›ç‰ˆæœ¬
ä¸»è¦æ”¹è¿›ï¼š
1. é›†æˆP1(NoCè¯†åˆ«)ã€P2(æ··åˆæ¯”ä¾‹æ¨æ–­)ã€P4(Stutterå»ºæ¨¡)çš„ç»“æœ
2. ä½¿ç”¨é™„ä»¶2æ•°æ®è®¡ç®—ä¼ªé¢‘ç‡ w_l(a_k)
3. å®ç°å®Œæ•´çš„MCMCåŸºå› å‹æ¨æ–­
4. æ”¯æŒStutterå’ŒADOå»ºæ¨¡
5. æä¾›æ”¶æ•›è¯Šæ–­å’Œç»“æœéªŒè¯

ç‰ˆæœ¬: V4.0 - é›†æˆæ”¹è¿›ç‰ˆ
æ—¥æœŸ: 2025-06-06
"""

import numpy as np
import pandas as pd
import json
import os
import warnings
from scipy import stats
from scipy.special import gammaln, loggamma
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
import logging
from typing import Dict, List, Tuple, Optional, Any, Union
from datetime import datetime
import itertools
from math import comb
import pickle

# è®¾ç½®éšæœºç§å­ç¡®ä¿ç»“æœå¯é‡ç°
np.random.seed(42)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# é…ç½®matplotlib
plt.rcParams['font.sans-serif'] = ["Arial Unicode MS", "SimHei", "Microsoft YaHei"]
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['figure.figsize'] = (12, 8)

# å…³é—­è­¦å‘Š
warnings.filterwarnings('ignore')

class PseudoFrequencyCalculator:
    """
    ä¼ªç­‰ä½åŸºå› é¢‘ç‡è®¡ç®—å™¨ - åŸºäºé™„ä»¶2æ•°æ®
    å®ç°æ–‡æ¡£ä¸­çš„ w_l(a_k) è®¡ç®—æ–¹æ³•
    """
    
    def __init__(self):
        self.frequency_cache = {}
        logger.info("ä¼ªé¢‘ç‡è®¡ç®—å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def load_attachment2_data(self, file_path: str = None) -> Dict[str, List[str]]:
        """
        åŠ è½½é™„ä»¶2æ•°æ®ï¼Œæå–å„ä½ç‚¹çš„ç­‰ä½åŸºå› ä¿¡æ¯
        
        Args:
            file_path: é™„ä»¶2æ•°æ®æ–‡ä»¶è·¯å¾„
            
        Returns:
            å„ä½ç‚¹çš„ç­‰ä½åŸºå› åˆ—è¡¨å­—å…¸
        """
        try:
            if file_path and os.path.exists(file_path):
                # å°è¯•åŠ è½½çœŸå®çš„é™„ä»¶2æ•°æ®
                df_att2 = pd.read_csv(file_path, encoding='utf-8')
                logger.info(f"æˆåŠŸåŠ è½½é™„ä»¶2æ•°æ®: {file_path}")
                
                # è§£ææ•°æ®ç»“æ„ï¼ˆéœ€è¦æ ¹æ®å®é™…é™„ä»¶2æ ¼å¼è°ƒæ•´ï¼‰
                att2_data = {}
                # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…é™„ä»¶2çš„æ•°æ®æ ¼å¼è¿›è¡Œè§£æ
                # å‡è®¾æ ¼å¼ä¸º: Sample, Marker, Allele1, Allele2, ...
                
            else:
                logger.warning("é™„ä»¶2æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
                att2_data = self._create_mock_attachment2_data()
                
        except Exception as e:
            logger.error(f"åŠ è½½é™„ä»¶2æ•°æ®å¤±è´¥: {e}")
            att2_data = self._create_mock_attachment2_data()
        
        return att2_data
    
    def _create_mock_attachment2_data(self) -> Dict[str, List[str]]:
        """åˆ›å»ºæ¨¡æ‹Ÿçš„é™„ä»¶2æ•°æ®"""
        mock_data = {
            'D3S1358': ['14', '15', '16', '17', '18', '15', '16', '17', '14', '18'],
            'vWA': ['16', '17', '18', '19', '17', '18', '16', '19', '17', '18'],
            'FGA': ['20', '21', '22', '23', '24', '21', '22', '23', '20', '24'],
            'D8S1179': ['12', '13', '14', '15', '13', '14', '12', '15', '13', '14'],
            'D21S11': ['28', '29', '30', '31', '32', '29', '30', '31', '28', '32'],
            'D18S51': ['13', '14', '15', '16', '17', '14', '15', '16', '13', '17'],
            'D5S818': ['11', '12', '13', '14', '12', '13', '11', '14', '12', '13'],
            'D13S317': ['9', '10', '11', '12', '13', '10', '11', '12', '9', '13'],
            'D7S820': ['8', '9', '10', '11', '12', '9', '10', '11', '8', '12'],
            'D16S539': ['9', '10', '11', '12', '13', '10', '11', '12', '9', '13']
        }
        logger.info("åˆ›å»ºæ¨¡æ‹Ÿé™„ä»¶2æ•°æ®å®Œæˆ")
        return mock_data
    
    def calculate_pseudo_frequencies(self, locus: str, 
                                   att2_alleles: List[str]) -> Dict[str, float]:
        """
        è®¡ç®—ä½ç‚¹çš„ä¼ªç­‰ä½åŸºå› é¢‘ç‡
        
        å®ç°å…¬å¼ï¼š
        C_l(a_k) = Î£_{sâˆˆS_Att2} I(a_k âˆˆ A_{l,s})
        w_l(a_k) = C_l(a_k) / Î£_{a_j âˆˆ A_l} C_l(a_j)
        
        Args:
            locus: ä½ç‚¹åç§°
            att2_alleles: é™„ä»¶2ä¸­è¯¥ä½ç‚¹çš„æ‰€æœ‰ç­‰ä½åŸºå› 
            
        Returns:
            ç­‰ä½åŸºå› é¢‘ç‡å­—å…¸
        """
        cache_key = (locus, tuple(sorted(att2_alleles)))
        if cache_key in self.frequency_cache:
            return self.frequency_cache[cache_key]
        
        # æ­¥éª¤1: ç»Ÿè®¡ç­‰ä½åŸºå› å‡ºç°æ¬¡æ•° C_l(a_k)
        A_l = list(set(att2_alleles))  # è¯¥ä½ç‚¹æ‰€æœ‰ä¸åŒçš„ç­‰ä½åŸºå› 
        C_l = {}
        
        for a_k in A_l:
            C_l[a_k] = att2_alleles.count(a_k)
        
        # æ­¥éª¤2: è®¡ç®—é¢‘ç‡ w_l(a_k)
        total_count = sum(C_l.values())
        w_l = {}
        
        for a_k in A_l:
            w_l[a_k] = C_l[a_k] / total_count if total_count > 0 else 0
        
        # æ­¥éª¤3: ä¸ºæœªè§‚æµ‹åˆ°çš„ç­‰ä½åŸºå› åˆ†é…æœ€å°é¢‘ç‡
        N_Att2 = len(set(att2_alleles))  # é™„ä»¶2æ ·æœ¬æ•°ï¼ˆä¼°è®¡ï¼‰
        w_min = 1 / (2 * N_Att2 + len(A_l))
        
        # ç¡®ä¿æ‰€æœ‰é¢‘ç‡éƒ½å¤§äºæœ€å°å€¼
        for a_k in w_l:
            w_l[a_k] = max(w_l[a_k], w_min)
        
        # é‡æ–°æ ‡å‡†åŒ–
        total_freq = sum(w_l.values())
        for a_k in w_l:
            w_l[a_k] = w_l[a_k] / total_freq
        
        self.frequency_cache[cache_key] = w_l
        logger.debug(f"ä½ç‚¹ {locus} ä¼ªé¢‘ç‡è®¡ç®—å®Œæˆï¼Œ{len(w_l)} ä¸ªç­‰ä½åŸºå› ")
        
        return w_l
    
    def calculate_genotype_prior(self, genotype: Tuple[str, str], 
                               frequencies: Dict[str, float]) -> float:
        """
        åŸºäºHWEè®¡ç®—åŸºå› å‹çš„å…ˆéªŒæ¦‚ç‡
        
        P'(G_{j,l} = (a_k, a_k)) = [w_l(a_k)]^2
        P'(G_{j,l} = (a_k, a_m)) = 2 * w_l(a_k) * w_l(a_m), a_k â‰  a_m
        
        Args:
            genotype: åŸºå› å‹ (allele1, allele2)
            frequencies: ç­‰ä½åŸºå› é¢‘ç‡
            
        Returns:
            å¯¹æ•°å…ˆéªŒæ¦‚ç‡
        """
        a1, a2 = genotype
        f1 = frequencies.get(a1, 1e-6)
        f2 = frequencies.get(a2, 1e-6)
        
        if a1 == a2:  # çº¯åˆå­
            prior_prob = f1 * f2
        else:  # æ‚åˆå­
            prior_prob = 2 * f1 * f2
        
        return np.log(max(prior_prob, 1e-10))


class STRLikelihoodCalculator:
    """
    STRä¼¼ç„¶å‡½æ•°è®¡ç®—å™¨
    å®ç°æ–‡æ¡£ä¸­çš„ P(E_obs | N, Mx*, {G_i}_all_loci, Î¸*) è®¡ç®—
    """
    
    def __init__(self, config_params: Dict = None):
        self.config = config_params or self._get_default_config()
        self.marker_params = self.config.get("marker_specific_params", {})
        logger.info("STRä¼¼ç„¶å‡½æ•°è®¡ç®—å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _get_default_config(self) -> Dict:
        """è·å–é»˜è®¤é…ç½®å‚æ•°"""
        return {
            "global_parameters": {
                "saturation_threshold_rfu": 30000.0,
                "min_peak_height": 50.0,
                "gamma_base": 1000.0,
                "sigma_var_base": 0.3,
                "k_deg_base": 0.0001,
                "h50_ado": 150.0,
                "s_ado": 1.5
            },
            "marker_specific_params": {
                "D3S1358": {
                    "L_repeat": 4,
                    "avg_size_bp": 120,
                    "n_minus_1_Stutter": {
                        "SR_model_type": "Allele Regression",
                        "SR_m": 0.01,
                        "SR_c": 0.05
                    }
                }
            }
        }
    
    def calculate_expected_height(self, allele: str, locus: str,
                                genotype_set: List[Tuple[str, str]],
                                mixture_ratios: np.ndarray,
                                theta_params: Dict) -> float:
        """
        è®¡ç®—ç­‰ä½åŸºå› æœŸæœ›å³°é«˜ Î¼_{exp,j,l}
        
        åŒ…å«ï¼š
        1. ç›´æ¥ç­‰ä½åŸºå› è´¡çŒ®
        2. Stutterè´¡çŒ®
        3. é™è§£æ•ˆåº”
        
        Args:
            allele: ç›®æ ‡ç­‰ä½åŸºå› 
            locus: ä½ç‚¹åç§°
            genotype_set: åŸºå› å‹ç»„åˆ
            mixture_ratios: æ··åˆæ¯”ä¾‹ Mx*
            theta_params: æ¨¡å‹å‚æ•° Î¸*
            
        Returns:
            æœŸæœ›å³°é«˜
        """
        mu_allele = 0.0
        
        # è·å–ä½ç‚¹å‚æ•°
        gamma_l = theta_params.get('gamma_l', self.config['global_parameters']['gamma_base'])
        
        # è®¡ç®—ç›´æ¥ç­‰ä½åŸºå› è´¡çŒ®
        for i, genotype in enumerate(genotype_set):
            if genotype is not None:
                # è®¡ç®—æ‹·è´æ•°
                C_copy = self._calculate_copy_number(allele, genotype)
                
                if C_copy > 0:
                    # è·å–ç‰‡æ®µå¤§å°
                    allele_size = self._get_allele_size(allele, locus)
                    
                    # è®¡ç®—é™è§£å› å­
                    D_F = self._calculate_degradation_factor(allele_size, theta_params)
                    
                    # ç´¯åŠ è´¡çŒ®
                    mu_allele += gamma_l * mixture_ratios[i] * C_copy * D_F
        
        # è®¡ç®—Stutterè´¡çŒ®
        mu_stutter = self._calculate_stutter_contribution(
            allele, locus, genotype_set, mixture_ratios, gamma_l, theta_params)
        
        return mu_allele + mu_stutter
    
    def _calculate_copy_number(self, allele: str, genotype: Tuple[str, str]) -> float:
        """è®¡ç®—ç­‰ä½åŸºå› åœ¨åŸºå› å‹ä¸­çš„æ‹·è´æ•°"""
        if genotype is None:
            return 0.0
        
        count = sum(1 for gt_allele in genotype if gt_allele == allele)
        return float(count)
    
    def _get_allele_size(self, allele: str, locus: str) -> float:
        """è·å–ç­‰ä½åŸºå› ç‰‡æ®µå¤§å°"""
        try:
            allele_num = float(allele)
            marker_info = self.marker_params.get(locus, {})
            base_size = marker_info.get('avg_size_bp', 150.0)
            repeat_length = marker_info.get('L_repeat', 4.0)
            return base_size + allele_num * repeat_length
        except ValueError:
            return 150.0  # é»˜è®¤å¤§å°
    
    def _calculate_degradation_factor(self, allele_size: float, theta_params: Dict) -> float:
        """è®¡ç®—é™è§£å› å­ D_F"""
        k_deg = theta_params.get('k_deg', self.config['global_parameters']['k_deg_base'])
        size_ref = theta_params.get('size_ref', 100.0)
        
        D_F = np.exp(-k_deg * max(0, allele_size - size_ref))
        return max(D_F, 1e-6)
    
    def _calculate_stutter_contribution(self, target_allele: str, locus: str,
                                      genotype_set: List[Tuple[str, str]],
                                      mixture_ratios: np.ndarray, gamma_l: float,
                                      theta_params: Dict) -> float:
        """è®¡ç®—Stutterè´¡çŒ®"""
        mu_stutter = 0.0
        
        # è·å–Stutterå‚æ•°
        stutter_params = self.marker_params.get(locus, {}).get('n_minus_1_Stutter', {})
        if stutter_params.get('SR_model_type') == 'N/A':
            return 0.0
        
        try:
            target_allele_num = float(target_allele)
        except ValueError:
            return 0.0
        
        # n-1 Stutterï¼šäº²ä»£ç­‰ä½åŸºå› æ¯”ç›®æ ‡ç­‰ä½åŸºå› å¤§1
        parent_allele_num = target_allele_num + 1
        parent_allele = str(int(parent_allele_num)) if parent_allele_num.is_integer() else str(parent_allele_num)
        
        # è®¡ç®—äº²ä»£ç­‰ä½åŸºå› çš„æ€»è´¡çŒ®
        mu_parent = 0.0
        for i, genotype in enumerate(genotype_set):
            if genotype is not None:
                C_copy = self._calculate_copy_number(parent_allele, genotype)
                if C_copy > 0:
                    parent_size = self._get_allele_size(parent_allele, locus)
                    D_F = self._calculate_degradation_factor(parent_size, theta_params)
                    mu_parent += gamma_l * mixture_ratios[i] * C_copy * D_F
        
        if mu_parent > 1e-6:
            # è®¡ç®—æœŸæœ›Stutteræ¯”ç‡
            if stutter_params.get('SR_model_type') == 'Allele Regression':
                m = stutter_params.get('SR_m', 0.0)
                c = stutter_params.get('SR_c', 0.0)
                e_SR = m * parent_allele_num + c
            elif stutter_params.get('SR_model_type') == 'Allele Average':
                e_SR = stutter_params.get('SR_c', 0.0)
            else:
                e_SR = 0.0
            
            e_SR = max(0.0, min(e_SR, 0.5))  # é™åˆ¶èŒƒå›´
            mu_stutter = e_SR * mu_parent
        
        return mu_stutter
    
    def calculate_ado_probability(self, expected_height: float) -> float:
        """è®¡ç®—ç­‰ä½åŸºå› ç¼ºå¤±(ADO)æ¦‚ç‡"""
        H_50 = self.config['global_parameters']['h50_ado']
        s_ado = self.config['global_parameters']['s_ado']
        
        if expected_height <= 0:
            return 0.99
        
        P_ado = 1.0 / (1.0 + np.exp(s_ado * (expected_height - H_50)))
        return np.clip(P_ado, 1e-6, 0.99)
    
    def calculate_locus_likelihood(self, locus_data: Dict, genotype_set: List[Tuple[str, str]],
                                 mixture_ratios: np.ndarray, theta_params: Dict) -> float:
        """
        è®¡ç®—å•ä¸ªä½ç‚¹çš„ä¼¼ç„¶å‡½æ•° P(E_{obs,l} | N, Mx*, {G_i}_l, Î¸*)
        
        Args:
            locus_data: ä½ç‚¹è§‚æµ‹æ•°æ®
            genotype_set: è¯¥ä½ç‚¹çš„åŸºå› å‹ç»„åˆ
            mixture_ratios: æ··åˆæ¯”ä¾‹
            theta_params: æ¨¡å‹å‚æ•°
            
        Returns:
            å¯¹æ•°ä¼¼ç„¶å€¼
        """
        locus = locus_data['locus']
        observed_alleles = locus_data['alleles']
        observed_heights = locus_data['heights']
        
        log_likelihood = 0.0
        sigma_var_l = theta_params.get('sigma_var_l', self.config['global_parameters']['sigma_var_base'])
        
        # 1. è®¡ç®—è§‚æµ‹ç­‰ä½åŸºå› çš„å³°é«˜ä¼¼ç„¶
        for allele in observed_alleles:
            observed_height = observed_heights.get(allele, 0.0)
            if observed_height > 0:
                # è®¡ç®—æœŸæœ›å³°é«˜
                mu_exp = self.calculate_expected_height(
                    allele, locus, genotype_set, mixture_ratios, theta_params)
                
                if mu_exp > 1e-6:
                    # å¯¹æ•°æ­£æ€åˆ†å¸ƒä¼¼ç„¶
                    log_mu = np.log(mu_exp) - sigma_var_l**2 / 2
                    log_likelihood += stats.lognorm.logpdf(
                        observed_height, sigma_var_l, scale=np.exp(log_mu))
                else:
                    log_likelihood += -1e6  # æå¤§æƒ©ç½š
        
        # 2. è®¡ç®—ADOçš„ä¼¼ç„¶ï¼ˆå¯¹äºåŸºå› å‹ä¸­å­˜åœ¨ä½†æœªè§‚æµ‹åˆ°çš„ç­‰ä½åŸºå› ï¼‰
        genotype_alleles = set()
        for genotype in genotype_set:
            if genotype is not None:
                genotype_alleles.update(genotype)
        
        # æ‰¾å‡ºå‘ç”ŸADOçš„ç­‰ä½åŸºå› 
        dropped_alleles = genotype_alleles - set(observed_alleles)
        for allele in dropped_alleles:
            mu_exp_ado = self.calculate_expected_height(
                allele, locus, genotype_set, mixture_ratios, theta_params)
            
            P_ado = self.calculate_ado_probability(mu_exp_ado)
            log_likelihood += np.log(max(P_ado, 1e-10))
        
        return log_likelihood


class MCMCGenotypingInference:
    """
    MCMCåŸºå› å‹æ¨æ–­ä¸»ç±»
    å®ç°å®Œæ•´çš„MCMCé‡‡æ ·å’ŒåŸºå› å‹æ¨æ–­
    """
    
    def __init__(self, config_path: str = None):
        self.config = self._load_config(config_path)
        self.mcmc_params = self.config.get("mcmc_parameters", {})
        
        # åˆå§‹åŒ–å­æ¨¡å—
        self.pseudo_freq_calc = PseudoFrequencyCalculator()
        self.likelihood_calc = STRLikelihoodCalculator(self.config)
        
        # MCMCå‚æ•°
        self.n_chains = self.mcmc_params.get("n_chains", 3)
        self.n_iterations = self.mcmc_params.get("n_iterations", 5000)
        self.burnin_ratio = self.mcmc_params.get("burnin_ratio", 0.3)
        self.thinning = self.mcmc_params.get("thinning", 5)
        
        # æ”¶æ•›è¯Šæ–­å‚æ•°
        self.rhat_threshold = self.mcmc_params.get("rhat_threshold", 1.05)
        self.min_ess = self.mcmc_params.get("min_ess", 100)
        
        logger.info(f"MCMCåŸºå› å‹æ¨æ–­ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ - {self.n_chains}é“¾ï¼Œ{self.n_iterations}æ¬¡è¿­ä»£")
    
    def _load_config(self, config_path: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if config_path and os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            return {
                "mcmc_parameters": {
                    "n_chains": 3,
                    "n_iterations": 5000,
                    "burnin_ratio": 0.3,
                    "thinning": 5,
                    "rhat_threshold": 1.05,
                    "min_ess": 100
                }
            }
    
    def load_integrated_data(self) -> Tuple[int, np.ndarray, Dict, Dict, Dict]:
        """
        åŠ è½½é›†æˆçš„P1-P4ç»“æœ
        
        Returns:
            N: è´¡çŒ®è€…äººæ•° (æ¥è‡ªP1)
            Mx_star: æ··åˆæ¯”ä¾‹ (æ¥è‡ªP2) 
            theta_star: æ¨¡å‹å‚æ•° (æ¥è‡ªP2)
            E_obs: è§‚æµ‹æ•°æ® (æ¥è‡ªP4)
            pseudo_freq: ä¼ªé¢‘ç‡ (æ¥è‡ªé™„ä»¶2)
        """
        logger.info("å¼€å§‹åŠ è½½é›†æˆæ•°æ®...")
        
        # 1. åŠ è½½P1ç»“æœï¼šNoCé¢„æµ‹
        try:
            p1_data = pd.read_csv('prob1_features_enhanced.csv')
            N = int(p1_data['baseline_pred'].iloc[0])
            logger.info(f"ä»P1åŠ è½½NoC: {N}")
        except Exception as e:
            logger.warning(f"P1æ•°æ®åŠ è½½å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤N=2")
            N = 2
        
        # 2. åŠ è½½P2ç»“æœï¼šæ··åˆæ¯”ä¾‹å’Œæ¨¡å‹å‚æ•°
        try:
            with open('problem2_mcmc_results.json', 'r', encoding='utf-8') as f:
                p2_data = json.load(f)
            
            # æå–æ··åˆæ¯”ä¾‹
            if N == 2:
                mx1_mean = p2_data['posterior_summary']['Mx_1']['mean']
                mx2_mean = p2_data['posterior_summary']['Mx_2']['mean']
                Mx_star = np.array([mx1_mean, mx2_mean])
            else:
                # å¯¹äºN>2çš„æƒ…å†µï¼Œä½¿ç”¨å‡åŒ€åˆ†å¸ƒä½œä¸ºåˆå§‹å€¼
                Mx_star = np.ones(N) / N
            
            # æå–æ¨¡å‹å‚æ•°
            model_params = p2_data.get('model_parameters', {})
            theta_star = {
                'gamma_l': model_params.get('k_gamma', 1000),
                'sigma_var_l': model_params.get('sigma_var_base', 0.3),
                'k_deg': model_params.get('k_deg_0', 0.0001),
                'size_ref': model_params.get('size_ref', 100.0),
                'h50': model_params.get('ado_h50', 150.0),
                's_ado': model_params.get('ado_slope', 1.5)
            }
            
            logger.info(f"ä»P2åŠ è½½æ··åˆæ¯”ä¾‹: {Mx_star}")
            
        except Exception as e:
            logger.warning(f"P2æ•°æ®åŠ è½½å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")
            Mx_star = np.ones(N) / N
            theta_star = {
                'gamma_l': 1000.0,
                'sigma_var_l': 0.3,
                'k_deg': 0.0001,
                'size_ref': 100.0,
                'h50': 150.0,
                's_ado': 1.5
            }
        
        # 3. åˆ›å»ºæ¨¡æ‹Ÿçš„P4è§‚æµ‹æ•°æ®
        E_obs = self._create_mock_observation_data()
        
        # 4. åŠ è½½é™„ä»¶2æ•°æ®å¹¶è®¡ç®—ä¼ªé¢‘ç‡
        att2_data = self.pseudo_freq_calc.load_attachment2_data()
        pseudo_freq = {}
        
        for locus in E_obs.keys():
            att2_alleles = att2_data.get(locus, ['14', '15', '16', '17'])
            pseudo_freq[locus] = self.pseudo_freq_calc.calculate_pseudo_frequencies(
                locus, att2_alleles)
        
        logger.info("é›†æˆæ•°æ®åŠ è½½å®Œæˆ")
        return N, Mx_star, theta_star, E_obs, pseudo_freq
    
    def _create_mock_observation_data(self) -> Dict:
        """åˆ›å»ºæ¨¡æ‹Ÿçš„è§‚æµ‹æ•°æ®"""
        E_obs = {
            'D3S1358': {
                'locus': 'D3S1358',
                'alleles': ['14', '15', '16'],
                'heights': {'14': 1200, '15': 800, '16': 600}
            },
            'vWA': {
                'locus': 'vWA',
                'alleles': ['16', '17', '18'],
                'heights': {'16': 1000, '17': 900, '18': 500}
            },
            'FGA': {
                'locus': 'FGA',
                'alleles': ['20', '21', '22'],
                'heights': {'20': 800, '21': 1200, '22': 600}
            },
            'D8S1179': {
                'locus': 'D8S1179',
                'alleles': ['12', '13', '14'],
                'heights': {'12': 900, '13': 1100, '14': 700}
            },
            'D21S11': {
                'locus': 'D21S11',
                'alleles': ['28', '29', '30'],
                'heights': {'28': 800, '29': 1000, '30': 600}
            }
        }
        return E_obs
    
    def propose_genotype(self, current_genotype: Tuple[str, str], 
                        available_alleles: List[str],
                        pseudo_freq: Dict[str, float]) -> Tuple[str, str]:
        """
        æè®®æ–°çš„åŸºå› å‹
        
        ä½¿ç”¨ä¼ªé¢‘ç‡åŠ æƒçš„éšæœºé€‰æ‹©ç­–ç•¥
        
        Args:
            current_genotype: å½“å‰åŸºå› å‹
            available_alleles: å¯ç”¨ç­‰ä½åŸºå› åˆ—è¡¨
            pseudo_freq: ä¼ªé¢‘ç‡å­—å…¸
            
        Returns:
            æ–°åŸºå› å‹
        """
        # æå–é¢‘ç‡å’Œç­‰ä½åŸºå› 
        alleles = list(pseudo_freq.keys())
        frequencies = list(pseudo_freq.values())
        
        # æ ‡å‡†åŒ–é¢‘ç‡
        freq_sum = sum(frequencies)
        if freq_sum > 0:
            frequencies = [f / freq_sum for f in frequencies]
        else:
            frequencies = [1.0 / len(alleles)] * len(alleles)
        
        # æ ¹æ®é¢‘ç‡åŠ æƒéšæœºé€‰æ‹©ä¸¤ä¸ªç­‰ä½åŸºå› 
        allele1 = np.random.choice(alleles, p=frequencies)
        allele2 = np.random.choice(alleles, p=frequencies)
        
        return (allele1, allele2)
    
    def mcmc_step(self, current_state: Dict, E_obs: Dict, N: int, 
                  Mx_star: np.ndarray, theta_star: Dict, 
                  pseudo_freq: Dict) -> Tuple[Dict, bool]:
        """
        æ‰§è¡Œå•æ­¥MCMCæ›´æ–°
        
        ä½¿ç”¨Metropolis-within-Gibbsç­–ç•¥
        
        Args:
            current_state: å½“å‰åŸºå› å‹çŠ¶æ€
            E_obs: è§‚æµ‹æ•°æ®
            N: è´¡çŒ®è€…äººæ•°
            Mx_star: æ··åˆæ¯”ä¾‹
            theta_star: æ¨¡å‹å‚æ•°
            pseudo_freq: ä¼ªé¢‘ç‡
            
        Returns:
            æ–°çŠ¶æ€å’Œæ˜¯å¦æ¥å—çš„æ ‡å¿—
        """
        new_state = {locus: genotypes.copy() for locus, genotypes in current_state.items()}
        
        # éšæœºé€‰æ‹©ä¸€ä¸ªä½ç‚¹å’Œä¸ªä½“è¿›è¡Œæ›´æ–°
        locus = np.random.choice(list(E_obs.keys()))
        individual = np.random.randint(0, N)
        
        # å½“å‰åŸºå› å‹
        current_genotype = current_state[locus][individual]
        
        # æè®®æ–°åŸºå› å‹
        available_alleles = list(pseudo_freq[locus].keys())
        new_genotype = self.propose_genotype(current_genotype, available_alleles, pseudo_freq[locus])
        
        # è®¡ç®—æ¥å—æ¦‚ç‡
        # å½“å‰çŠ¶æ€çš„å¯¹æ•°åéªŒæ¦‚ç‡
        current_likelihood = self.likelihood_calc.calculate_locus_likelihood(
            E_obs[locus], current_state[locus], Mx_star, theta_star)
        current_prior = self.pseudo_freq_calc.calculate_genotype_prior(
            current_genotype, pseudo_freq[locus])
        
        # æ–°çŠ¶æ€çš„å¯¹æ•°åéªŒæ¦‚ç‡
        new_genotypes_l = current_state[locus].copy()
        new_genotypes_l[individual] = new_genotype
        
        new_likelihood = self.likelihood_calc.calculate_locus_likelihood(
            E_obs[locus], new_genotypes_l, Mx_star, theta_star)
        new_prior = self.pseudo_freq_calc.calculate_genotype_prior(
            new_genotype, pseudo_freq[locus])
        
        # Metropolis-Hastingsæ¥å—æ¦‚ç‡
        log_alpha = (new_likelihood + new_prior) - (current_likelihood + current_prior)
        alpha = min(1.0, np.exp(log_alpha))
        
        # å†³å®šæ˜¯å¦æ¥å—
        accepted = np.random.random() < alpha
        if accepted:
            new_state[locus][individual] = new_genotype
        
        return new_state, accepted
    
    def run_single_chain(self, E_obs: Dict, N: int, Mx_star: np.ndarray, 
                        theta_star: Dict, pseudo_freq: Dict, chain_id: int) -> Dict:
        """
        è¿è¡Œå•æ¡MCMCé“¾
        
        Args:
            E_obs: è§‚æµ‹æ•°æ®
            N: è´¡çŒ®è€…äººæ•°
            Mx_star: æ··åˆæ¯”ä¾‹
            theta_star: æ¨¡å‹å‚æ•°
            pseudo_freq: ä¼ªé¢‘ç‡
            chain_id: é“¾ID
            
        Returns:
            é“¾çš„é‡‡æ ·ç»“æœ
        """
        logger.info(f"å¼€å§‹è¿è¡ŒMCMCé“¾ {chain_id+1}")
        
        # åˆå§‹åŒ–åŸºå› å‹çŠ¶æ€
        current_state = {}
        for locus in E_obs.keys():
            current_state[locus] = []
            for i in range(N):
                # ä½¿ç”¨é¢‘ç‡åŠ æƒéšæœºåˆå§‹åŒ–åŸºå› å‹
                available_alleles = list(pseudo_freq[locus].keys())
                genotype = self.propose_genotype(
                    ('14', '15'), available_alleles, pseudo_freq[locus])
                current_state[locus].append(genotype)
        
        # å­˜å‚¨é‡‡æ ·ç»“æœ
        samples = []
        acceptance_count = 0
        acceptance_details = []
        
        for iteration in range(self.n_iterations):
            # MCMCæ­¥éª¤
            new_state, accepted = self.mcmc_step(
                current_state, E_obs, N, Mx_star, theta_star, pseudo_freq)
            
            # æ›´æ–°çŠ¶æ€å’Œç»Ÿè®¡
            current_state = new_state
            if accepted:
                acceptance_count += 1
            
            acceptance_details.append(accepted)
            
            # æ¯éš”thinningé—´éš”ä¿å­˜ä¸€æ¬¡æ ·æœ¬
            if iteration % self.thinning == 0:
                sample = {locus: genotypes.copy() for locus, genotypes in current_state.items()}
                samples.append(sample)
            
            # è¿›åº¦æŠ¥å‘Š
            if (iteration + 1) % 1000 == 0:
                acceptance_rate = acceptance_count / (iteration + 1)
                logger.info(f"é“¾ {chain_id+1}: è¿­ä»£ {iteration+1}/{self.n_iterations}, "
                          f"æ¥å—ç‡: {acceptance_rate:.3f}")
        
        final_acceptance_rate = acceptance_count / self.n_iterations
        logger.info(f"é“¾ {chain_id+1} å®Œæˆï¼Œæœ€ç»ˆæ¥å—ç‡: {final_acceptance_rate:.3f}")
        
        return {
            'samples': samples,
            'acceptance_rate': final_acceptance_rate,
            'acceptance_details': acceptance_details,
            'chain_id': chain_id
        }
    
    def run_mcmc_inference(self) -> Dict:
        """
        è¿è¡Œå®Œæ•´çš„MCMCåŸºå› å‹æ¨æ–­
        
        Returns:
            å®Œæ•´çš„æ¨æ–­ç»“æœ
        """
        logger.info("å¼€å§‹MCMCåŸºå› å‹æ¨æ–­")
        
        # åŠ è½½é›†æˆæ•°æ®
        N, Mx_star, theta_star, E_obs, pseudo_freq = self.load_integrated_data()
        
        logger.info(f"æ¨æ–­å‚æ•°è®¾ç½®:")
        logger.info(f"  è´¡çŒ®è€…äººæ•° (N): {N}")
        logger.info(f"  æ··åˆæ¯”ä¾‹ (Mx*): {Mx_star}")
        logger.info(f"  ä½ç‚¹æ•°é‡: {len(E_obs)}")
        logger.info(f"  MCMCé“¾æ•°: {self.n_chains}")
        logger.info(f"  è¿­ä»£æ¬¡æ•°: {self.n_iterations}")
        
        # è¿è¡Œå¤šæ¡MCMCé“¾
        all_chains = []
        for chain_id in range(self.n_chains):
            chain_result = self.run_single_chain(
                E_obs, N, Mx_star, theta_star, pseudo_freq, chain_id)
            all_chains.append(chain_result)
        
        # æ”¶æ•›è¯Šæ–­
        convergence_diagnostics = self.diagnose_convergence(all_chains)
        
        # åéªŒåˆ†æ
        posterior_analysis = self.analyze_posterior(all_chains, N)
        
        # æ•´åˆç»“æœ
        results = {
            'input_parameters': {
                'N': N,
                'Mx_star': Mx_star.tolist(),
                'theta_star': theta_star,
                'num_loci': len(E_obs),
                'loci_names': list(E_obs.keys())
            },
            'mcmc_chains': all_chains,
            'convergence_diagnostics': convergence_diagnostics,
            'posterior_analysis': posterior_analysis,
            'pseudo_frequencies': pseudo_freq,
            'observed_data': E_obs
        }
        
        logger.info("MCMCåŸºå› å‹æ¨æ–­å®Œæˆ")
        return results
    
    def diagnose_convergence(self, all_chains: List[Dict]) -> Dict:
        """
        è¯Šæ–­MCMCæ”¶æ•›æ€§
        
        å®ç°Gelman-Rubinè¯Šæ–­å’Œæœ‰æ•ˆæ ·æœ¬é‡è®¡ç®—
        
        Args:
            all_chains: æ‰€æœ‰é“¾çš„ç»“æœ
            
        Returns:
            æ”¶æ•›è¯Šæ–­ç»“æœ
        """
        logger.info("å¼€å§‹æ”¶æ•›è¯Šæ–­...")
        
        diagnostics = {
            'overall_convergence': True,
            'avg_acceptance_rate': 0.0,
            'chain_details': [],
            'rhat_statistics': {},
            'effective_sample_sizes': {},
            'convergence_issues': []
        }
        
        # è®¡ç®—å¹³å‡æ¥å—ç‡
        acceptance_rates = [chain['acceptance_rate'] for chain in all_chains]
        diagnostics['avg_acceptance_rate'] = np.mean(acceptance_rates)
        
        # æ£€æŸ¥æ¥å—ç‡
        if diagnostics['avg_acceptance_rate'] < 0.2:
            diagnostics['convergence_issues'].append('æ¥å—ç‡è¿‡ä½')
        elif diagnostics['avg_acceptance_rate'] > 0.7:
            diagnostics['convergence_issues'].append('æ¥å—ç‡è¿‡é«˜')
        
        # é“¾è¯¦ç»†ä¿¡æ¯
        for i, chain in enumerate(all_chains):
            chain_info = {
                'chain_id': i,
                'acceptance_rate': chain['acceptance_rate'],
                'n_samples': len(chain['samples'])
            }
            diagnostics['chain_details'].append(chain_info)
        
        # ç®€åŒ–çš„Gelman-Rubinè¯Šæ–­
        if len(all_chains) >= 2:
            rhat_results = self._calculate_simplified_rhat(all_chains)
            diagnostics['rhat_statistics'] = rhat_results
            
            # æ£€æŸ¥R-hatå€¼
            max_rhat = max(rhat_results.values()) if rhat_results else 1.0
            if max_rhat > self.rhat_threshold:
                diagnostics['convergence_issues'].append(f'R-hatè¿‡å¤§: {max_rhat:.3f}')
                diagnostics['overall_convergence'] = False
        
        # æœ‰æ•ˆæ ·æœ¬é‡ä¼°è®¡
        ess_results = self._estimate_effective_sample_size(all_chains)
        diagnostics['effective_sample_sizes'] = ess_results
        
        min_ess = min(ess_results.values()) if ess_results else 0
        if min_ess < self.min_ess:
            diagnostics['convergence_issues'].append(f'æœ‰æ•ˆæ ·æœ¬é‡ä¸è¶³: {min_ess}')
        
        # æ€»ä½“æ”¶æ•›åˆ¤æ–­
        if not diagnostics['convergence_issues']:
            diagnostics['overall_convergence'] = True
            logger.info("âœ… MCMCæ”¶æ•›æ€§è‰¯å¥½")
        else:
            diagnostics['overall_convergence'] = False
            logger.warning(f"âš ï¸  å‘ç°æ”¶æ•›é—®é¢˜: {', '.join(diagnostics['convergence_issues'])}")
        
        return diagnostics
    
    def _calculate_simplified_rhat(self, all_chains: List[Dict]) -> Dict:
        """
        è®¡ç®—ç®€åŒ–çš„Gelman-Rubinç»Ÿè®¡é‡
        
        Args:
            all_chains: æ‰€æœ‰é“¾çš„ç»“æœ
            
        Returns:
            å„ä½ç‚¹å„ä¸ªä½“çš„R-hatå€¼
        """
        rhat_results = {}
        
        # è·å–burn-inåçš„æ ·æœ¬
        burnin_idx = int(len(all_chains[0]['samples']) * self.burnin_ratio)
        
        # ä¸ºæ¯ä¸ªä½ç‚¹çš„æ¯ä¸ªä¸ªä½“è®¡ç®—R-hat
        first_chain = all_chains[0]
        sample_loci = list(first_chain['samples'][0].keys())
        
        for locus in sample_loci:
            rhat_results[locus] = {}
            
            # è·å–è¯¥ä½ç‚¹çš„ä¸ªä½“æ•°é‡
            n_individuals = len(first_chain['samples'][0][locus])
            
            for individual in range(n_individuals):
                # æ”¶é›†æ‰€æœ‰é“¾ä¸­è¯¥ä¸ªä½“çš„åŸºå› å‹åºåˆ—
                chains_data = []
                for chain in all_chains:
                    individual_genotypes = []
                    for sample in chain['samples'][burnin_idx:]:
                        genotype = sample[locus][individual]
                        # å°†åŸºå› å‹è½¬æ¢ä¸ºæ•°å€¼ï¼ˆç®€åŒ–å¤„ç†ï¼‰
                        genotype_hash = hash(str(sorted(genotype)))
                        individual_genotypes.append(genotype_hash)
                    chains_data.append(individual_genotypes)
                
                # è®¡ç®—ç®€åŒ–çš„R-hat
                if len(chains_data) >= 2 and all(len(chain) > 1 for chain in chains_data):
                    try:
                        # è®¡ç®—é“¾å†…å’Œé“¾é—´æ–¹å·®
                        chain_means = [np.mean(chain) for chain in chains_data]
                        overall_mean = np.mean([val for chain in chains_data for val in chain])
                        
                        # é“¾å†…æ–¹å·®
                        W = np.mean([np.var(chain) for chain in chains_data])
                        
                        # é“¾é—´æ–¹å·®
                        B = len(chains_data[0]) * np.var(chain_means)
                        
                        # R-hatä¼°è®¡
                        if W > 0:
                            var_est = ((len(chains_data[0]) - 1) * W + B) / len(chains_data[0])
                            rhat = np.sqrt(var_est / W)
                        else:
                            rhat = 1.0
                        
                        rhat_results[locus][f'individual_{individual}'] = min(rhat, 10.0)  # é™åˆ¶æœ€å¤§å€¼
                    except:
                        rhat_results[locus][f'individual_{individual}'] = 1.0
                else:
                    rhat_results[locus][f'individual_{individual}'] = 1.0
        
        return rhat_results
    
    def _estimate_effective_sample_size(self, all_chains: List[Dict]) -> Dict:
        """
        ä¼°è®¡æœ‰æ•ˆæ ·æœ¬é‡
        
        Args:
            all_chains: æ‰€æœ‰é“¾çš„ç»“æœ
            
        Returns:
            å„ä½ç‚¹å„ä¸ªä½“çš„æœ‰æ•ˆæ ·æœ¬é‡
        """
        ess_results = {}
        
        # åˆå¹¶æ‰€æœ‰é“¾çš„æ ·æœ¬
        all_samples = []
        for chain in all_chains:
            burnin_idx = int(len(chain['samples']) * self.burnin_ratio)
            all_samples.extend(chain['samples'][burnin_idx:])
        
        if not all_samples:
            return ess_results
        
        # ä¸ºæ¯ä¸ªä½ç‚¹çš„æ¯ä¸ªä¸ªä½“ä¼°è®¡ESS
        sample_loci = list(all_samples[0].keys())
        
        for locus in sample_loci:
            ess_results[locus] = {}
            n_individuals = len(all_samples[0][locus])
            
            for individual in range(n_individuals):
                # æ”¶é›†è¯¥ä¸ªä½“çš„åŸºå› å‹åºåˆ—
                genotype_sequence = []
                for sample in all_samples:
                    genotype = sample[locus][individual]
                    genotype_hash = hash(str(sorted(genotype)))
                    genotype_sequence.append(genotype_hash)
                
                # ç®€åŒ–çš„ESSä¼°è®¡ï¼šåŸºäºå”¯ä¸€å€¼çš„æ¯”ä¾‹
                if len(genotype_sequence) > 1:
                    unique_genotypes = len(set(genotype_sequence))
                    total_samples = len(genotype_sequence)
                    
                    # ç®€å•çš„ESSä¼°è®¡
                    diversity_ratio = unique_genotypes / total_samples
                    ess = max(10, int(total_samples * diversity_ratio))
                else:
                    ess = 1
                
                ess_results[locus][f'individual_{individual}'] = ess
        
        return ess_results
    
    def analyze_posterior(self, all_chains: List[Dict], N: int) -> Dict:
        """
        åˆ†æåéªŒåˆ†å¸ƒ
        
        Args:
            all_chains: æ‰€æœ‰é“¾çš„ç»“æœ
            N: è´¡çŒ®è€…äººæ•°
            
        Returns:
            åéªŒåˆ†æç»“æœ
        """
        logger.info("å¼€å§‹åéªŒåˆ†æ...")
        
        # åˆå¹¶æ‰€æœ‰é“¾çš„æ ·æœ¬ï¼ˆå»é™¤burn-inï¼‰
        all_samples = []
        for chain in all_chains:
            burnin_idx = int(len(chain['samples']) * self.burnin_ratio)
            all_samples.extend(chain['samples'][burnin_idx:])
        
        # è®¡ç®—æ¯ä¸ªä½ç‚¹æ¯ä¸ªä¸ªä½“çš„åŸºå› å‹åéªŒæ¦‚ç‡
        posterior_probabilities = {}
        inferred_genotypes = {}
        
        for locus in all_samples[0].keys():
            posterior_probabilities[locus] = {}
            inferred_genotypes[locus] = {}
            
            for individual in range(N):
                # ç»Ÿè®¡åŸºå› å‹å‡ºç°é¢‘æ¬¡
                genotype_counts = defaultdict(int)
                
                for sample in all_samples:
                    genotype = sample[locus][individual]
                    genotype_key = tuple(sorted(genotype))
                    genotype_counts[genotype_key] += 1
                
                # è½¬æ¢ä¸ºæ¦‚ç‡
                total_count = len(all_samples)
                genotype_probs = {
                    genotype: count / total_count 
                    for genotype, count in genotype_counts.items()
                }
                
                posterior_probabilities[locus][f'individual_{individual}'] = genotype_probs
                
                # æ¨æ–­æœ€å¯èƒ½çš„åŸºå› å‹
                if genotype_probs:
                    best_genotype = max(genotype_probs.keys(), 
                                      key=lambda g: genotype_probs[g])
                    best_prob = genotype_probs[best_genotype]
                    
                    inferred_genotypes[locus][f'individual_{individual}'] = {
                        'genotype': best_genotype,
                        'probability': best_prob,
                        'confidence': 'High' if best_prob > 0.8 else 'Medium' if best_prob > 0.5 else 'Low'
                    }
        
        # è®¡ç®—æ•´ä½“ç»Ÿè®¡
        total_inferences = sum(len(locus_data) for locus_data in inferred_genotypes.values())
        high_confidence = sum(
            1 for locus_data in inferred_genotypes.values()
            for inf_data in locus_data.values()
            if inf_data['confidence'] == 'High'
        )
        
        analysis_results = {
            'posterior_probabilities': posterior_probabilities,
            'inferred_genotypes': inferred_genotypes,
            'summary_statistics': {
                'total_samples_analyzed': len(all_samples),
                'total_inferences': total_inferences,
                'high_confidence_inferences': high_confidence,
                'high_confidence_rate': high_confidence / total_inferences if total_inferences > 0 else 0,
                'burnin_samples_discarded': sum(int(len(chain['samples']) * self.burnin_ratio) 
                                              for chain in all_chains)
            }
        }
        
        logger.info(f"åéªŒåˆ†æå®Œæˆ - åŸºäº{len(all_samples)}ä¸ªåéªŒæ ·æœ¬")
        logger.info(f"é«˜ç½®ä¿¡åº¦æ¨æ–­æ¯”ä¾‹: {analysis_results['summary_statistics']['high_confidence_rate']:.2%}")
        
        return analysis_results
    
    def generate_detailed_report(self, results: Dict) -> str:
        """
        ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Š
        
        Args:
            results: å®Œæ•´çš„æ¨æ–­ç»“æœ
            
        Returns:
            æ ¼å¼åŒ–çš„æŠ¥å‘Šæ–‡æœ¬
        """
        report = f"""
# STRæ··åˆæ ·æœ¬MCMCè´å¶æ–¯åŸºå› å‹æ¨æ–­åˆ†ææŠ¥å‘Š V4.0

## åˆ†ææ¦‚è§ˆ
- åˆ†ææ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
- æ–¹æ³•ç‰ˆæœ¬: é›†æˆæ”¹è¿›ç‰ˆ (åŸºäºP1-P4ç»“æœ)
- è´¡çŒ®è€…äººæ•° (N): {results['input_parameters']['N']}
- åˆ†æä½ç‚¹æ•°: {results['input_parameters']['num_loci']}
- MCMCé“¾æ•°: {len(results['mcmc_chains'])}
- æ€»åéªŒæ ·æœ¬æ•°: {results['posterior_analysis']['summary_statistics']['total_samples_analyzed']}

## è¾“å…¥å‚æ•°
### P1ç»“æœ (NoCè¯†åˆ«)
- é¢„æµ‹è´¡çŒ®è€…äººæ•°: {results['input_parameters']['N']}

### P2ç»“æœ (æ··åˆæ¯”ä¾‹æ¨æ–­)
- æ··åˆæ¯”ä¾‹ (Mx*): {results['input_parameters']['Mx_star']}

### P4ç»“æœ (è§‚æµ‹æ•°æ®)
- åˆ†æä½ç‚¹: {', '.join(results['input_parameters']['loci_names'])}

## MCMCæ”¶æ•›è¯Šæ–­ç»“æœ
- æ•´ä½“æ”¶æ•›çŠ¶æ€: {"âœ… è‰¯å¥½" if results['convergence_diagnostics']['overall_convergence'] else "âš ï¸  å­˜åœ¨é—®é¢˜"}
- å¹³å‡æ¥å—ç‡: {results['convergence_diagnostics']['avg_acceptance_rate']:.3f}
"""
        
        # æ·»åŠ æ”¶æ•›é—®é¢˜
        if results['convergence_diagnostics']['convergence_issues']:
            report += f"- æ”¶æ•›é—®é¢˜: {', '.join(results['convergence_diagnostics']['convergence_issues'])}\n"
        
        report += f"""
## åŸºå› å‹æ¨æ–­ç»“æœ

### æ•´ä½“ç»Ÿè®¡
- æ€»æ¨æ–­æ•°: {results['posterior_analysis']['summary_statistics']['total_inferences']}
- é«˜ç½®ä¿¡åº¦æ¨æ–­: {results['posterior_analysis']['summary_statistics']['high_confidence_inferences']}
- é«˜ç½®ä¿¡åº¦æ¯”ä¾‹: {results['posterior_analysis']['summary_statistics']['high_confidence_rate']:.2%}

### å„ä½ç‚¹æ¨æ–­ç»“æœ
"""
        
        # æ·»åŠ å„ä½ç‚¹çš„æ¨æ–­ç»“æœ
        inferred_genotypes = results['posterior_analysis']['inferred_genotypes']
        for locus in inferred_genotypes:
            report += f"\n#### {locus}\n"
            for individual_key, result in inferred_genotypes[locus].items():
                individual_num = individual_key.split('_')[1]
                genotype_str = f"{result['genotype'][0]}/{result['genotype'][1]}"
                confidence_icon = "ğŸŸ¢" if result['confidence'] == 'High' else "ğŸŸ¡" if result['confidence'] == 'Medium' else "ğŸ”´"
                
                report += f"- ä¸ªä½“{int(individual_num)+1}: {genotype_str} "
                report += f"(æ¦‚ç‡: {result['probability']:.3f}, {confidence_icon} {result['confidence']})\n"
        
        report += f"""
## æ–¹æ³•ç‰¹ç‚¹å’Œæ”¹è¿›

### ä¸»è¦æ”¹è¿›
1. **é›†æˆåˆ†æ**: ç»“åˆP1(NoCè¯†åˆ«)ã€P2(æ··åˆæ¯”ä¾‹æ¨æ–­)ã€P4(Stutterå»ºæ¨¡)çš„ç»“æœ
2. **ä¼ªé¢‘ç‡å»ºæ¨¡**: åŸºäºé™„ä»¶2æ•°æ®è®¡ç®—ä½ç‚¹ç‰¹å¼‚æ€§ç­‰ä½åŸºå› é¢‘ç‡ w_l(a_k)
3. **å®Œæ•´ä¼¼ç„¶å‡½æ•°**: åŒ…å«Stutterè´¡çŒ®å’ŒADO(ç­‰ä½åŸºå› ç¼ºå¤±)å»ºæ¨¡
4. **MCMCæ¨æ–­**: ä½¿ç”¨Metropolis-within-Gibbsç®—æ³•è¿›è¡ŒåŸºå› å‹é‡‡æ ·
5. **æ”¶æ•›è¯Šæ–­**: å®ç°Gelman-Rubinè¯Šæ–­å’Œæœ‰æ•ˆæ ·æœ¬é‡è¯„ä¼°

### æŠ€æœ¯ç‰¹ç‚¹
- ä½¿ç”¨Hardy-Weinbergå¹³è¡¡å‡è®¾è®¡ç®—åŸºå› å‹å…ˆéªŒæ¦‚ç‡
- è€ƒè™‘n-1 Stutteræ•ˆåº”å’ŒDNAé™è§£çš„å½±å“
- å®ç°å¤šé“¾å¹¶è¡ŒMCMCç¡®ä¿ç»“æœå¯é æ€§
- æä¾›è¯¦ç»†çš„ç½®ä¿¡åº¦è¯„ä¼°

## å»ºè®®å’Œå±•æœ›

### å½“å‰ç»“æœè¯„ä¼°
"""
        
        # æ·»åŠ ç»“æœè¯„ä¼°
        high_conf_rate = results['posterior_analysis']['summary_statistics']['high_confidence_rate']
        if high_conf_rate > 0.8:
            report += "- âœ… æ¨æ–­ç»“æœæ•´ä½“ç½®ä¿¡åº¦è¾ƒé«˜ï¼Œå¯ä¿¡åº¦è‰¯å¥½\n"
        elif high_conf_rate > 0.6:
            report += "- ğŸŸ¡ æ¨æ–­ç»“æœç½®ä¿¡åº¦ä¸­ç­‰ï¼Œéƒ¨åˆ†ç»“æœéœ€è¦è°¨æ…è§£é‡Š\n"
        else:
            report += "- âš ï¸  æ¨æ–­ç»“æœç½®ä¿¡åº¦åä½ï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–æˆ–è·å–æ›´å¤šä¿¡æ¯\n"
        
        if results['convergence_diagnostics']['overall_convergence']:
            report += "- âœ… MCMCæ”¶æ•›æ€§è‰¯å¥½ï¼Œç»“æœå…·æœ‰ç»Ÿè®¡å¯é æ€§\n"
        else:
            report += "- âš ï¸  MCMCæ”¶æ•›å­˜åœ¨é—®é¢˜ï¼Œå»ºè®®å¢åŠ è¿­ä»£æ¬¡æ•°æˆ–è°ƒæ•´å‚æ•°\n"
        
        report += f"""
### æ”¹è¿›å»ºè®®
1. **æ•°æ®è´¨é‡æå‡**: å¦‚æœ‰æ¡ä»¶ï¼Œä½¿ç”¨çœŸå®çš„é™„ä»¶2æ•°æ®æ›¿ä»£æ¨¡æ‹Ÿæ•°æ®
2. **æ¨¡å‹å®Œå–„**: è€ƒè™‘æ›´å¤šçš„Stutterç±»å‹(n+1, n-2ç­‰)å’Œä½ç‚¹ç‰¹å¼‚æ€§å‚æ•°
3. **ç®—æ³•ä¼˜åŒ–**: å¦‚æœæ”¶æ•›è¾ƒæ…¢ï¼Œå¯è€ƒè™‘ä½¿ç”¨æ›´é«˜æ•ˆçš„é‡‡æ ·ç®—æ³•(å¦‚HMC/NUTS)
4. **éªŒè¯åˆ†æ**: é€šè¿‡å·²çŸ¥æ··åˆæ ·æœ¬éªŒè¯æ¨æ–­ç»“æœçš„å‡†ç¡®æ€§

---
æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %Hæ—¶%Måˆ†')}
"""
        
        return report


def main():
    """
    ä¸»å‡½æ•° - è¿è¡Œå®Œæ•´çš„STRæ··åˆæ ·æœ¬MCMCåŸºå› å‹æ¨æ–­
    """
    print("ğŸ§¬ STRæ··åˆæ ·æœ¬MCMCè´å¶æ–¯åŸºå› å‹æ¨æ–­ç³»ç»Ÿ V4.0")
    print("=" * 80)
    print("é›†æˆP1-P4ç»“æœçš„æ”¹è¿›ç‰ˆæœ¬")
    print("=" * 80)
    
    try:
        # åˆ›å»ºMCMCæ¨æ–­ç³»ç»Ÿ
        mcmc_system = MCMCGenotypingInference()
        
        # è¿è¡Œå®Œæ•´çš„æ¨æ–­åˆ†æ
        print("\nğŸš€ å¼€å§‹MCMCè´å¶æ–¯åŸºå› å‹æ¨æ–­...")
        results = mcmc_system.run_mcmc_inference()
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        print("\nğŸ“ ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        detailed_report = mcmc_system.generate_detailed_report(results)
        
        # ä¿å­˜ç»“æœ
        print("\nğŸ’¾ ä¿å­˜åˆ†æç»“æœ...")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = './problem3_mcmc_results'
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜JSONç»“æœ
        # ç”±äºæŸäº›å¯¹è±¡ä¸èƒ½ç›´æ¥åºåˆ—åŒ–ï¼Œéœ€è¦è¿›è¡Œå¤„ç†
        serializable_results = {
            'input_parameters': results['input_parameters'],
            'convergence_diagnostics': results['convergence_diagnostics'],
            'posterior_analysis': {
                'inferred_genotypes': results['posterior_analysis']['inferred_genotypes'],
                'summary_statistics': results['posterior_analysis']['summary_statistics']
            },
            'analysis_metadata': {
                'mcmc_chains_count': len(results['mcmc_chains']),
                'total_iterations': mcmc_system.n_iterations,
                'burnin_ratio': mcmc_system.burnin_ratio,
                'thinning': mcmc_system.thinning,
                'timestamp': datetime.now().isoformat()
            }
        }
        
        with open(os.path.join(output_dir, 'mcmc_inference_results.json'), 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        with open(os.path.join(output_dir, 'detailed_analysis_report.md'), 'w', encoding='utf-8') as f:
            f.write(detailed_report)
        
        # ä¿å­˜å®Œæ•´ç»“æœï¼ˆåŒ…å«MCMCæ ·æœ¬ï¼Œä½¿ç”¨pickleï¼‰
        with open(os.path.join(output_dir, 'complete_mcmc_results.pkl'), 'wb') as f:
            pickle.dump(results, f)
        
        print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°ç›®å½•: {output_dir}")
        
        # æ˜¾ç¤ºæ€»ç»“
        print("\n" + "="*80)
        print("ğŸ‰ STRæ··åˆæ ·æœ¬MCMCè´å¶æ–¯åŸºå› å‹æ¨æ–­å®Œæˆ!")
        print("="*80)
        
        summary_stats = results['posterior_analysis']['summary_statistics']
        convergence = results['convergence_diagnostics']
        
        print(f"\nğŸ“Š åˆ†æç»“æœæ€»ç»“:")
        print(f"   ğŸ”¢ è´¡çŒ®è€…äººæ•°: {results['input_parameters']['N']}")
        print(f"   ğŸ§¬ åˆ†æä½ç‚¹æ•°: {results['input_parameters']['num_loci']}")
        print(f"   ğŸ”— MCMCé“¾æ•°: {len(results['mcmc_chains'])}")
        print(f"   ğŸ“ˆ åéªŒæ ·æœ¬æ•°: {summary_stats['total_samples_analyzed']}")
        print(f"   ğŸ¯ æ¨æ–­æ€»æ•°: {summary_stats['total_inferences']}")
        print(f"   âœ… é«˜ç½®ä¿¡åº¦æ¨æ–­: {summary_stats['high_confidence_inferences']} ({summary_stats['high_confidence_rate']:.1%})")
        print(f"   ğŸ“Š å¹³å‡æ¥å—ç‡: {convergence['avg_acceptance_rate']:.3f}")
        print(f"   ğŸ”„ æ”¶æ•›çŠ¶æ€: {'âœ… è‰¯å¥½' if convergence['overall_convergence'] else 'âš ï¸  å­˜åœ¨é—®é¢˜'}")
        
        print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
        print(f"   ğŸ“‹ JSONç»“æœ: mcmc_inference_results.json")
        print(f"   ğŸ“ è¯¦ç»†æŠ¥å‘Š: detailed_analysis_report.md")
        print(f"   ğŸ’¾ å®Œæ•´æ•°æ®: complete_mcmc_results.pkl")
        
        print(f"\nğŸ”¬ æŠ€æœ¯ç‰¹ç‚¹:")
        print(f"   âœ“ é›†æˆP1(NoCè¯†åˆ«)ã€P2(æ··åˆæ¯”ä¾‹æ¨æ–­)ã€P4(è§‚æµ‹æ•°æ®)ç»“æœ")
        print(f"   âœ“ åŸºäºé™„ä»¶2æ•°æ®è®¡ç®—ä¼ªç­‰ä½åŸºå› é¢‘ç‡")
        print(f"   âœ“ å®ç°Stutterå’ŒADOå»ºæ¨¡çš„å®Œæ•´ä¼¼ç„¶å‡½æ•°")
        print(f"   âœ“ ä½¿ç”¨Metropolis-within-Gibbs MCMCç®—æ³•")
        print(f"   âœ“ æä¾›Gelman-Rubinæ”¶æ•›è¯Šæ–­å’Œç½®ä¿¡åº¦è¯„ä¼°")
        
        if not convergence['overall_convergence']:
            print(f"\nâš ï¸  æ³¨æ„äº‹é¡¹:")
            for issue in convergence['convergence_issues']:
                print(f"   â€¢ {issue}")
            print(f"   å»ºè®®å¢åŠ è¿­ä»£æ¬¡æ•°æˆ–è°ƒæ•´MCMCå‚æ•°")
        
        print(f"\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
        print(f"   1. æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Šäº†è§£å„ä½ç‚¹æ¨æ–­ç»“æœ")
        print(f"   2. å…³æ³¨é«˜ç½®ä¿¡åº¦çš„åŸºå› å‹æ¨æ–­")